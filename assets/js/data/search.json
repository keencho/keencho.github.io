[ { "title": "JPA Transactional 점검하기", "url": "/posts/transactional-think/", "categories": "Java", "tags": "Java, JPA", "date": "2025-06-27 20:12:00 +0900", "snippet": "JPA Transactional 점검하기JPA를 다루는 개발자라면 Transactional의 동작 원리를 정확히 이해해야 한다. 하지만 솔직히 말하면, 프로젝트 초기에 세운 가이드라인을 처음에는 잘 지켰지만 개발이 진행되면서 점점 일단 @Transactional 붙이고 보자는 식으로 습관적으로 사용했던 적이 많다. 당연히 이런 무분별한 사용은 여러 문제를 야기했고, 이를 계기로 JPA Transactional 사용법을 다시 점검하고 명확한 원칙을 정립해보고자 한다.@Transactional 남용하지 않기@Transactional을 남용하면 어떤 일이 일어날 수 있는지 알아보자. 테스트 환경은 다음과 같다.hikari: maximum-pool-size: 1 connection-timeout: 1000jpa: open-in-view: false @GetMapping(\"/concurrent\")public ResponseEntity&lt;?&gt; concurrentTest() { System.out.println(\"🚀 동시 테스트 시작!\"); var future1 = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(\"🔴 스레드1 시작: \" + Thread.currentThread().getName()); testService.test(); System.out.println(\"🟢 스레드1 완료: \" + Thread.currentThread().getName()); return \"완료1\"; } catch (Exception e) { System.out.println(\"❌ 스레드1 실패: \" + e.getMessage()); return \"실패1: \" + e.getMessage(); } }); var future2 = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(\"🔴 스레드2 시작: \" + Thread.currentThread().getName()); testService.save(); System.out.println(\"🟢 스레드2 완료: \" + Thread.currentThread().getName()); return \"완료2\"; } catch (Exception e) { System.out.println(\"❌ 스레드2 실패: \" + e.getMessage()); return \"실패2: \" + e.getMessage(); } }); return ResponseEntity.ok( Map.of( \"result1\", future1.join(), \"result2\", future2.join() ) );} OpenEntityManagerInView가 HTTP 요청당 하나의 EntityManager를 유지하고, 이는 커넥션을 먼저 점유하기에 open-in-view 를 비활성화 한다. CompletableFutre.supplyAsync()로 2개의 별도 스레드 풀에서 동시에 bookService.save()를 호출한다.DB 작업 없는 메소드에 @Transactional@SneakyThrows@Transactionalpublic void test() { Thread.sleep(2000); var a = 3; var b = 2; var c = a / b; System.out.println(c);}DB 작업이 없다. 그럼에도 @Transactional이 트랜잭션 시작부터 끝까지 커넥션을 홀딩하기에 2개의 호출중 한개는 1초의 커넥션 타임아웃으로 인해 에러가 발생한다. 물론 DB 작업이 없기에 그리 심각하지 않은 문제이긴 할것이다.HikariPool-1 - Connection is not available, request timed out after 1010ms (total=1, active=1, idle=0, waiting=0)결과:{ \"result2\": \"완료2\", \"result1\": \"실패1: Could not open JPA EntityManager for transaction\"}내부 DB 작업이 있는 메소드에 @Transactional@SneakyThrows@Transactionalpublic void test() { var order = apiService.order(); Thread.sleep(2000); var result = new orderResult(); result.setSuccess(order.isSuccess()); result.setMessage(order.getMessage()); result.setOrderNo(order.getOrderNo()); orderResultRepository.save(result);}이번엔 외부 API를 호출하고 결과를 DB에 저장하는 예제이다. 외부 API 호출까진 성공하겠으나, 내부 DB에 결과를 작업하기 위해 대기하는 2초간 에러가 발생할 것이다. DB에는 1개의 결과만 저장된다.이는 외부 서비스에는 주문 정보가 있고 내부 서비스에는 주문 결과가 없는, 상황에 따라 심각한 문제를 야기할 수 있다.또한 외부 API 동작에 의해 트랜잭션 처리가 지연되고 그만큼 내부 서비스 DB 연결시간에 영향을 받기 때문에 매우 중요하게 접근해야할 필요가 있다.HikariPool-1 - Connection is not available, request timed out after 1011ms (total=1, active=1, idle=0, waiting=0){ \"result2\": \"완료2\", \"result1\": \"실패1: Could not open JPA EntityManager for transaction\"}단순 조회만 수행하는 메소드에 @Transactional단순 조회만 수행하는 메소드에 @Transactional을 붙였을때 트랜잭션이 어떻게 동작하는지 확인해보자.@Componentpublic class TransactionChecker { @Autowired DataSource dataSource; /** * 1. 트랜잭션 상태 확인 */ public void checkTransactionStatus(String actionName) { var isTransactionActive = TransactionSynchronizationManager.isActualTransactionActive(); var isSynchronizationActive = TransactionSynchronizationManager.isSynchronizationActive(); var transactionName = TransactionSynchronizationManager.getCurrentTransactionName(); System.out.printf(\"🔍 [%s] 트랜잭션 상태:%n\", actionName); System.out.printf(\" ├─ 활성화: %s%n\", isTransactionActive ? \"✅ YES\" : \"❌ NO\"); System.out.printf(\" ├─ 동기화: %s%n\", isSynchronizationActive ? \"✅ YES\" : \"❌ NO\"); System.out.printf(\" └─ 이름: %s%n\", transactionName != null ? transactionName : \"없음\"); } /** * 2. HikariCP 커넥션 풀 상태 확인 */ public void checkConnectionPoolStatus(String actionName) { if (dataSource instanceof HikariDataSource hikariDataSource) { var poolBean = hikariDataSource.getHikariPoolMXBean(); System.out.printf(\"🏊‍♂️ [%s] HikariCP 상태:%n\", actionName); System.out.printf(\" ├─ 활성 커넥션: %d%n\", poolBean.getActiveConnections()); System.out.printf(\" ├─ 유휴 커넥션: %d%n\", poolBean.getIdleConnections()); System.out.printf(\" ├─ 전체 커넥션: %d%n\", poolBean.getTotalConnections()); System.out.printf(\" ├─ 대기 중인 스레드: %d%n\", poolBean.getThreadsAwaitingConnection()); System.out.printf(\" └─ 최대 풀 크기: %d%n\", hikariDataSource.getMaximumPoolSize()); } else { System.out.printf(\"🏊‍♂️ [%s] HikariCP가 아님: %s%n\", actionName, dataSource.getClass().getSimpleName()); } } /** * 3. 한 번에 모든 상태 확인 */ public void checkAll(String actionName) { var threadName = Thread.currentThread().getName(); System.out.printf(\"📊 === [%s] %s 상태 체크 ===%n\", actionName, threadName); this.checkTransactionStatus(actionName); this.checkConnectionPoolStatus(actionName); System.out.println(\"📊 ========================================\"); }}트랜잭션 상태를 확인할 수 있는 헬퍼 클래스다.1. @Transactional 없이 조회public void test() { transactionChecker.checkAll(\"읽기 전\"); var list = authorRepository.findAll(); transactionChecker.checkAll(\"읽은 후\");}🔴 [13:57:07.976] http-nio-8080-exec-1 - 요청 시작📊 === [읽기 전] http-nio-8080-exec-1 상태 체크 ===🔍 [읽기 전] 트랜잭션 상태: ├─ 활성화: ❌ NO ├─ 동기화: ❌ NO └─ 이름: 없음🏊‍♂️ [읽기 전] HikariCP 상태: ├─ 활성 커넥션: 0 ├─ 유휴 커넥션: 1 ├─ 전체 커넥션: 1 ├─ 대기 중인 스레드: 0 └─ 최대 풀 크기: 1📊 ========================================13:57:07.980 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Creating new transaction with name [org.springframework.data.jpa.repository.support.SimpleJpaRepository.findAll]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT,readOnly13:57:07.980 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Opened new EntityManager [SessionImpl(1191829786&lt;open&gt;)] for JPA transaction13:57:07.983 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Exposing JPA transaction as JDBC [org.springframework.orm.jpa.vendor.HibernateJpaDialect$HibernateConnectionHandle@42c003ac]13:57:08.054 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Initiating transaction commit13:57:08.054 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Committing JPA transaction on EntityManager [SessionImpl(1191829786&lt;open&gt;)]13:57:08.055 DEBUG [http-nio-8080-exec-1] o.s.orm.jpa.JpaTransactionManager - Closing JPA EntityManager [SessionImpl(1191829786&lt;open&gt;)] after transaction📊 === [읽은 후] http-nio-8080-exec-1 상태 체크 ===🔍 [읽은 후] 트랜잭션 상태: ├─ 활성화: ❌ NO ├─ 동기화: ❌ NO └─ 이름: 없음🏊‍♂️ [읽은 후] HikariCP 상태: ├─ 활성 커넥션: 0 ├─ 유휴 커넥션: 1 ├─ 전체 커넥션: 1 ├─ 대기 중인 스레드: 0 └─ 최대 풀 크기: 1📊 ========================================🟢 [13:57:08.056] http-nio-8080-exec-1 - 요청 완료 (소요: 80ms)결과는 위와 같다. authorRepository.findAll() 를 수행할때 트랜잭션이 생성되고 바로 트랜잭션이 커밋된다. 다시말해, 이때 hikari pool에서 커넥션을 획득하고 바로 반납한다는 의미가 된다.2. @Transactional 붙이고 조회@Transactionalpublic void test() { transactionChecker.checkAll(\"읽기 전\"); var list = authorRepository.findAll(); transactionChecker.checkAll(\"읽은 후\");}🔴 [13:59:19.626] http-nio-8080-exec-3 - 요청 시작13:59:19.628 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Creating new transaction with name [com.keencho.spring.service.TestService.test]: PROPAGATION_REQUIRED,ISOLATION_DEFAULT13:59:19.628 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Opened new EntityManager [SessionImpl(2125691159&lt;open&gt;)] for JPA transaction13:59:19.631 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Exposing JPA transaction as JDBC [org.springframework.orm.jpa.vendor.HibernateJpaDialect$HibernateConnectionHandle@27049398]📊 === [읽기 전] http-nio-8080-exec-3 상태 체크 ===🔍 [읽기 전] 트랜잭션 상태: ├─ 활성화: ✅ YES ├─ 동기화: ✅ YES └─ 이름: com.keencho.spring.service.TestService.test🏊‍♂️ [읽기 전] HikariCP 상태: ├─ 활성 커넥션: 1 ├─ 유휴 커넥션: 0 ├─ 전체 커넥션: 1 ├─ 대기 중인 스레드: 0 └─ 최대 풀 크기: 1📊 ========================================13:59:19.634 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Found thread-bound EntityManager [SessionImpl(2125691159&lt;open&gt;)] for JPA transaction13:59:19.634 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Participating in existing transaction📊 === [읽은 후] http-nio-8080-exec-3 상태 체크 ===🔍 [읽은 후] 트랜잭션 상태: ├─ 활성화: ✅ YES ├─ 동기화: ✅ YES └─ 이름: com.keencho.spring.service.TestService.test🏊‍♂️ [읽은 후] HikariCP 상태: ├─ 활성 커넥션: 1 ├─ 유휴 커넥션: 0 ├─ 전체 커넥션: 1 ├─ 대기 중인 스레드: 0 └─ 최대 풀 크기: 1📊 ========================================13:59:19.703 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Initiating transaction commit13:59:19.704 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Committing JPA transaction on EntityManager [SessionImpl(2125691159&lt;open&gt;)]13:59:19.705 DEBUG [http-nio-8080-exec-3] o.s.orm.jpa.JpaTransactionManager - Closing JPA EntityManager [SessionImpl(2125691159&lt;open&gt;)] after transaction🟢 [13:59:19.705] http-nio-8080-exec-3 - 요청 완료 (소요: 79ms)결과는 위와 같다. http 요청이 들어오고 testService.test() 메소드를 탔을때 트랜잭션이 생성된다. 그 후 authRepository.findAll() 수행시 기존 트랜잭션에 참여하고 메소드가 종료될때 트랜잭션을 커밋하고 종료한다.정리하면 다음과 같다.@Transactional 있음:메소드 시작 → [커넥션 획득] → DB작업1 → DB작업2 → [커넥션 반납] ← 메소드 끝 └───────── 쭉 홀딩 ────────────┘@Transactional 없음:메서드 시작 → [획득→반납] → [획득→반납] → 메서드 끝 DB작업1 DB작업2성능 영향:- @Transactional: 커넥션 홀딩 시간 ↑, DB 작업 일관성 ✓- 없음: 커넥션 획득/반납 오버헤드 ↑, 개별 커밋@Transactional rollback롤백 처리에 대한 전략을 잘 세워야 한다. 한 트랜잭션 범위 내에서 Unchecked Exception이 발생하면 자동 롤백되지만, Checked Exception은 기본적으로 커밋된다. 이는 기본적인 개념이고, @Transactional을 어떻게 사용하느냐에 따라 동작이 어떻게 달라지는지 확실히 알아야 한다.TransactionAspectSupport의 동작 원리TransactionAspectSupport는 @Transactional이 붙은 메소드를 실행할 때 트랜잭션 처리를 담당하는 핵심 클래스이다. Spring은 AOP 프록시 패턴을 통해 실제 메서드 호출 전후로 트랜잭션 시작, 커밋, 롤백 로직을 삽입한다.Unchecked Exception 롤백 정책 Unchecked Exception (RuntimeException, Error): 자동 롤백 Checked Exception (IOException, SQLException 등): 롤백 안함이는 Unchecked Exception이 예상치 못한 프로그래밍 오류나 시스템 장애를 나타내는 반면, Checked Exception은 예상 가능한 상황으로 개발자가 의도적인 예외 처리 로직을 작성했을 가능성이 높기 때문이다.try-catch로 감싸도 롤백되는 이유RuntimeException이 발생하는 순간 Spring은 현재 트랜잭션을 “rollback-only” 상태로 마킹한다. 이후 try-catch로 예외를 처리해서 메서드가 정상 종료되어도, 이미 마킹된 트랜잭션은 최종적으로 롤백된다. 트랜잭션 상태는 예외 발생 시점에 결정되며, 예외 처리 여부는 이 결정을 바꾸지 못한다.@Transactionalpublic void save() { authorRepository.save(new Author(\"홍길동\")); try { actionService.doAction(); } catch (Exception ignored) { }}@Transactionalpublic void doAction() throws Exception { throw new Exception(\"Error\"); // Checked Exception: 롤백 안함}@Transactionalpublic void doAction() { throw new RuntimeException(\"Error\"); // Unchecked Exception: 자동 롤백 }REQUIRES_NEW를 활용한 독립 트랜잭션@Transactional(propagation = Propagation.REQUIRES_NEW)를 사용하면 별도의 독립적인 트랜잭션이 생성된다. 이 경우 내부 트랜잭션에서 예외가 발생해도 해당 트랜잭션만 롤백되고, 외부 트랜잭션은 영향받지 않는다. 따라서 로그 저장이나 감사(audit) 기록처럼 메인 비즈니스 로직 실패와 관계없이 반드시 저장되어야 하는 데이터에 활용할 수 있다.@Transactional(propagation = Propagation.REQUIRES_NEW)public void doAction() { throw new RuntimeException(\"Error\"); // Unchecked Exception: 자동 롤백 }그러나 이는 커넥션 풀에서 새로운 커넥션을 얻어온다는 의미이기 때문에, REQUIRES_NEW가 붙은 메소드를 반복문을 통해 호출한다면 로직에 따라 커넥션 풀이 고갈되어 타임아웃이나 대기 상황이 발생할 수 있다. 특히 외부 트랜잭션이 커넥션을 점유한 상태에서 내부적으로 여러 개의 독립 트랜잭션을 동시에 생성하려 할 때 주의해야 한다.동일한 클래스 내부 호출 시 @Transactional 무시@Transactionalpublic void save() { authorRepository.save(new Author(\"홍길동\")); try { this.doAction(); } catch (Exception ignored) { }}@Transactionalpublic void doAction() { throw new RuntimeException(\"Error\");}외부에서 save() 호출 시 프록시를 통해 트랜잭션이 시작되지만, this.doAction() 호출은 직접적인 Java 메서드 호출이므로 Spring 프록시가 개입할 수 없다. 따라서 doAction()의 @Transactional은 무시되고 Unchecked Exception이 발생해도 롤백되지 않는다.롤백이 되어야 한다면 다음 방법중 하나를 선택하면 된다. try-catch 제거 수동 롤백 (TransactionAspectSupport.currentTransactionStatus().setRollbackOnly()) 별도 서비스로 분리 Self Injection 사용 (allow-circular-references: true 필요) AspectJ 위빙 사용 (비추천)@Transactional 유무에 따른 차이점doAction()에 @Transactional이 있든 없든 Repository 호출 시 기존 트랜잭션에 참여하는 건 동일하다. 하지만 예외 처리에서 중요한 차이가 있다.@Transactionalpublic void save() { authorRepository.save(new Author(\"홍길동\")); try { actionService.doAction(); } catch (Exception ignored) { }}public void doAction() { authorRepository.save(new Author(\"거북이\")); // 기존 트랜잭션 참여 throw new RuntimeException(\"Error\");}doAction()에 @Transactional이 없다. 그래도 두 데이터 모두 저장된다. RuntimeException이 발생해도 단순히 상위로 전파될 뿐이다. try-catch로 잡으면 save() 메서드가 정상 종료된 것으로 인식하고 트랜잭션을 커밋한다.@Transactional이 있는 경우 RuntimeException 발생 시 Spring이 현재 트랜잭션을 “rollback-only”로 마킹한다. 이후 try-catch로 예외를 처리해서 save() 메서드가 정상 종료되어도, Spring이 커밋을 시도할 때 “rollback-only” 마킹을 발견하고 UnexpectedRollbackException을 던진다.왜 이런 차이가 발생할까?Spring의 트랜잭션 처리 방식을 살펴보자. @Transactional 없는 경우: doAction()은 단순한 Java 메서드다. Repository 호출할 때만 기존 트랜잭션을 사용한다. RuntimeException이 발생해도 Spring은 이를 “트랜잭션적 오류”로 인식하지 않는다. 단지 일반적인 예외일 뿐이고, try-catch로 처리되면 아무 문제없이 커밋한다. @Transactional 있는 경우: doAction()도 트랜잭션 관리 대상이 된다. Spring AOP가 메서드를 감싸서 예외를 모니터링한다. RuntimeException 발생 시 “이 트랜잭션은 문제가 있다”고 마킹해둔다. 이후 상위에서 try-catch로 예외를 처리해도 Spring은 “어? 문제가 있었는데 커밋하려고 하네?”라고 인식하고 UnexpectedRollbackException을 던진다.핵심은 Spring이 언제 트랜잭션 상태를 추적하느냐다. @Transactional이 붙어야만 Spring이 해당 메서드의 예외를 “트랜잭션 관점에서” 바라본다.@Transactional(readOnly = true)개발을 하다 보면 단순한 조회 메서드에 @Transactional(readOnly = true)를 붙일지 말지 고민하게 된다. 특히 OSIV를 비활성화한 환경에서는 이 선택이 더욱 중요하다.PostgreSQL 환경에서 실제 쿼리 로그를 통해 두 방식의 차이점을 자세히 알아보자.# application.ymlspring: jpa: open-in-view: false # postgresql.conflog_statement = 'all'log_line_prefix = '%t [%p] %u@%d: 'logging_collector = on상황별로 코드를 작성하고 로그를 확인해 보자.public void test() { var list = authorRepository.findAll();}[37380] LOG: statement: BEGIN READ ONLY[37380] LOG: execute &lt;unnamed&gt;: select a1_0.id,a1_0.created_at,a1_0.name from author a1_0[37380] LOG: execute S_1: COMMIT@Transactional(readOnly = true) 가 붙어있는 경우 트랜잭션이 명시적으로 시작되고 종료된다. 트랜잭션을 READ ONLY 로 세팅하는 쿼리와, 조회 후 커밋하는 쿼리가 추가로 날라간다.public void test() { var list = authorRepository.findAllNonTransactional(); // @Query(value = \"SELECT a FROM Author a\") // List&lt;Author&gt; findAllNonTransactional();} [26408] LOG: execute &lt;unnamed&gt;: select a1_0.id,a1_0.created_at,a1_0.name from author a1_0어노테이션이 없는 경우 autocommit 모드로 동작하여 단일 쿼리만 실행된다. CrudRepository 인터페이스에 정의된 메소드를 호출하는 경우 구현체인 SimpleJpaRepository 클래스에 @Transactional(readOnly = true) 가 붙어있기 때문에 기본적으로 readOnly 모드로 동작한다.기능별 상세 비교Hibernate 측면@Transactional(readOnly = true)장점 1차 캐시 활용: 동일한 엔티티를 여러 번 조회해도 캐시에서 가져온다 Lazy Loading 지원: 연관관계 엔티티를 필요할 때 로드할 수 있다 FlushMode.MANUAL: 불필요한 flush 작업을 방지한다단점 세션 장기 보유: 메서드 실행 동안 Hibernate 세션을 계속 유지한다 메모리 사용량 증가: 1차 캐시와 세션 컨텍스트로 인한 메모리 오버헤드어노테이션 제거장점 즉시 세션 해제: 쿼리 실행 후 바로 세션을 정리한다 최소 메모리 사용: 세션 컨텍스트를 오래 보유하지 않는다단점 Lazy Loading 불가: LazyInitializationException 발생 위험 1차 캐시 미활용: 동일한 엔티티를 여러 번 조회하면 매번 DB 쿼리 세션 재생성: 매번 새로운 세션을 생성하는 비용Spring 측면 (프록시)@Transactional(readOnly = true)장점 AOP 기능 활용: 로깅, 모니터링 등 횡단 관심사 처리 트랜잭션 전파 규칙: 다른 트랜잭션과의 상호작용을 명확하게 정의 일관된 예외 처리: Spring의 트랜잭션 예외 변환 기능 활용단점 프록시 생성 비용: Spring AOP 프록시 생성과 관리 비용 인터셉터 체인 실행: 메서드 호출 시마다 인터셉터 체인을 거침 메모리 오버헤드: 프록시 객체와 관련 메타데이터어노테이션 제거장점 프록시 오버헤드 없음: 직접적인 메서드 호출 메모리 효율적: 프록시 관련 메모리 사용 없음 단순한 실행 흐름: 복잡한 AOP 처리 과정이 없음단점 Spring AOP 기능 미사용: 횡단 관심사 처리 불가능 트랜잭션 관리 불가: 명시적인 트랜잭션 제어 없음PostgreSQL 측면BEGIN READ ONLY vs autocommitPostgreSQL에서 BEGIN READ ONLY는 단순한 플래그 설정이 아니라 실제로 다음과 같은 최적화를 제공한다. WAL 생성 방지: Write-Ahead Log를 생성하지 않아 I/O 부하 감소 락 획득 회피: 불필요한 락 요청을 하지 않음 내부 최적화: PostgreSQL이 읽기 전용임을 알고 여러 최적화 수행반면 autocommit 모드는 네트워크 호출 최소화: 단일 쿼리만 전송 커넥션 즉시 반환: 쿼리 실행 후 바로 커텍션 풀에 반환위와같은 특징을 가지고 있다.성능 비교일반적으로 단순 조회에서는 어노테이션 제거가 더 빠른 경우가 많지만, 복잡한 로직에서는 트랜잭션의 이점이 성능 오버헤드를 상쇄한다.결론 및 권장사항@Transactional(readOnly = true) 언제 사용할까? 연관관계(Lazy Loading)가 필요한 모든 경우 여러 쿼리의 일관성이 중요한 경우 복잡한 비즈니스 로직이 있는 경우 Spring의 트랜잭션 관리 기능을 활용하고 싶은 경우이럴때는 제거해도 되지 않을까? 완전히 단순한 단일 조회 (연관관계 없음) DTO 프로젝션만 사용하는 경우 극도의 성능 최적화가 필요한 경우 대량 데이터 처리에서 메모리 효율성이 중요한 경우개인적인 생각 (PostgreSQL 기준) 안전성: Lazy Loading 관련 예외를 방지할 수 있다 확장성: 나중에 연관관계나 복잡한 로직이 추가되어도 안전하다 일관성: 팀 전체가 동일한 패턴을 사용할 수 있다 PostgreSQL 최적화: READ ONLY 모드의 이점을 활용할 수 있다성능이 정말 중요한 특정 API에만 선별적으로 어노테이션을 제거하고, 나머지는 @Transactional(readOnly = true)를 기본으로 사용하는 것이 현실적인것 같다.실제 성능은 애플리케이션의 특성, 데이터 크기, 인프라 환경에 따라 달라질 수 있으니, 중요한 결정을 내리기 전에는 항상 실제 환경에서 테스트해보는 것을 잊지 말자.격리 수준(Isolation Level) 실전 활용격리 수준은 동시 트랜잭션 간의 데이터 일관성을 얼마나 엄격하게 보장할지를 결정한다. PostgreSQL의 기본 격리 수준은 READ_COMMITTED이며, 비즈니스 요구사항에 따라 적절한 수준을 선택해야 한다.@Transactional(isolation = Isolation.READ_COMMITTED)public void readCommittedTest() { // PostgreSQL 기본 격리 수준 - 성능 우선}@Transactional(isolation = Isolation.REPEATABLE_READ)public void repeatableReadTest() { // 일관성 우선 - Snapshot Isolation 제공}격리 수준별 현상과 문제점 Dirty Read: 커밋되지 않은 변경사항을 읽는 현상 Non-repeatable Read: 같은 트랜잭션 내에서 동일한 데이터를 재조회할 때 다른 값이 나오는 현상 Phantom Read: 범위 조건으로 조회 시 새로운 행이 추가로 조회되는 현상READ_COMMITTED (기본값)@Transactional(isolation = Isolation.READ_COMMITTED)public void processOrderWithReadCommitted() { // 첫 번째 조회 var product = productRepository.findById(1L); System.out.println(\"첫 번째 재고: \" + product.getStock()); // 100 // 이 시점에 다른 트랜잭션이 재고를 50으로 변경하고 커밋 // 두 번째 조회 - 변경된 값 읽음 (Non-repeatable Read 발생) product = productRepository.findById(1L); System.out.println(\"두 번째 재고: \" + product.getStock()); // 50 // 문제: 같은 트랜잭션 내에서 데이터가 달라짐 // 첫 번째 조회를 기준으로 비즈니스 로직을 작성했다면 문제가 될 수 있음}READ_COMMITTED 특징: Dirty Read 방지: 커밋된 데이터만 읽음 Non-repeatable Read 허용: 다른 트랜잭션의 커밋된 변경사항이 중간에 보임 Phantom Read 허용: 범위 조건에서 새로운 행이 나타날 수 있음 성능: 가장 높음 (잠금이 적음) 사용 시기: 성능이 중요하고 약간의 데이터 불일치를 허용할 수 있는 경우REPEATABLE_READ@Transactional(isolation = Isolation.REPEATABLE_READ)public void processOrderWithRepeatableRead() { // 첫 번째 조회 - 스냅샷 생성 var product = productRepository.findById(1L); System.out.println(\"첫 번째 재고: \" + product.getStock()); // 100 // 이 시점에 다른 트랜잭션이 재고를 50으로 변경하고 커밋해도 // 두 번째 조회 - 스냅샷에 의해 동일한 값 읽음 product = productRepository.findById(1L); System.out.println(\"두 번째 재고: \" + product.getStock()); // 100 (동일) // 장점: 트랜잭션 내에서 일관된 데이터 보장 // 단점: 다른 트랜잭션의 변경사항을 못 볼 수 있음 (Lost Update 위험)}REPEATABLE_READ 특징: Snapshot Isolation: 트랜잭션 시작 시점의 데이터 스냅샷 유지 Non-repeatable Read 방지: 동일한 데이터 반복 읽기 시 일관성 보장 Phantom Read 일부 방지: PostgreSQL에서는 대부분 방지됨 성능: 중간 (스냅샷 유지 오버헤드) 사용 시기: 복잡한 비즈니스 로직에서 데이터 일관성이 중요한 경우실제 비즈니스 활용 사례1. 재고 차감 로직 (동시성 제어)@Transactional(isolation = Isolation.REPEATABLE_READ)public OrderResult decreaseStock(Long productId, int quantity) { // 스냅샷으로 일관된 데이터 보장 var product = productRepository.findById(productId).get(); // 재고 검증 - 트랜잭션 내에서 일관된 값으로 검증 if (product.getStock() &lt; quantity) { return OrderResult.fail(\"재고 부족: 현재 \" + product.getStock() + \"개\"); } // 재고 차감 product.decreaseStock(quantity); productRepository.save(product); return OrderResult.success(\"주문 완료\");}// 주의: REPEATABLE_READ에서도 Lost Update는 발생할 수 있음// 따라서 낙관적 락(@Version) 또는 비관적 락 추가 고려 필요2. 금융 거래 처리@Transactional(isolation = Isolation.REPEATABLE_READ)public TransferResult transfer(Long fromAccountId, Long toAccountId, BigDecimal amount) { // 일관된 스냅샷으로 계좌 정보 조회 var fromAccount = accountRepository.findById(fromAccountId).get(); var toAccount = accountRepository.findById(toAccountId).get(); // 잔액 검증 - 스냅샷으로 일관성 보장 if (fromAccount.getBalance().compareTo(amount) &lt; 0) { return TransferResult.fail(\"잔액 부족\"); } // 이체 처리 fromAccount.withdraw(amount); toAccount.deposit(amount); accountRepository.saveAll(List.of(fromAccount, toAccount)); return TransferResult.success(\"이체 완료\");}3. 보고서 생성 (데이터 정합성)@Transactional(readOnly = true, isolation = Isolation.REPEATABLE_READ)public DailySalesReport generateSalesReport(LocalDate date) { // 보고서 생성 중 데이터 변경되어도 일관된 스냅샷으로 작업 var orders = orderRepository.findByOrderDate(date); // 주문 총액 계산 var totalAmount = orders.stream() .map(Order::getAmount) .reduce(BigDecimal.ZERO, BigDecimal::add); // 카테고리별 집계 - 모두 동일한 스냅샷 기준 var categoryStats = orders.stream() .collect(Collectors.groupingBy( Order::getCategoryId, Collectors.summingDouble(order -&gt; order.getAmount().doubleValue()) )); return new DailySalesReport(date, totalAmount, orders.size(), categoryStats);}4. READ_COMMITTED 적절한 사용 사례@Transactional(isolation = Isolation.READ_COMMITTED)public void logUserActivity(Long userId, String activity) { // 로깅은 약간의 불일치가 있어도 무관 // 성능을 우선시하는 것이 맞음 var user = userRepository.findById(userId).orElse(null); if (user != null) { activityLogRepository.save(new ActivityLog(userId, activity, LocalDateTime.now())); }}성능 vs 일관성 선택 가이드 READ_COMMITTED: 로깅, 단순 조회, 성능이 중요한 대량 처리 REPEATABLE_READ: 금융 거래, 재고 관리, 복잡한 비즈니스 로직, 보고서 생성 SERIALIZABLE: 극도로 중요한 데이터 (일반적으로 성능상 비추천)주의사항 격리 수준만으로는 모든 동시성 문제가 해결되지 않음 필요시 낙관적 락(@Version) 또는 비관적 락(SELECT FOR UPDATE) 추가 고려 격리 수준이 높을수록 데드락 발생 가능성 증가@Transactional과 LazyInitializationExceptionLazyInitializationException은 Hibernate를 사용할 때 자주 마주치는 문제다. 특히 spring.jpa.open-in-view: false 설정을 사용하는 환경에서 트랜잭션이 끝난 후 지연 로딩을 시도할 때 발생한다.문제 발생 시나리오@Entitypublic class Author { @Id private Long id; private String name; @OneToMany(mappedBy = \"author\", fetch = FetchType.LAZY) // 기본값: LAZY private List&lt;Book&gt; books = new ArrayList&lt;&gt;(); // getter, setter...}@Transactional(readOnly = true)public Author findAuthor(Long id) { return authorRepository.findById(id).orElse(null); // 트랜잭션이 여기서 종료됨}@GetMapping(\"/authors/{id}\")public ResponseEntity&lt;AuthorResponse&gt; getAuthor(@PathVariable Long id) { var author = authorService.findAuthor(id); // 트랜잭션 종료 // 여기서 LazyInitializationException 발생! // could not initialize proxy - no Session var bookTitles = author.getBooks().stream() .map(Book::getTitle) .collect(Collectors.toList()); return ResponseEntity.ok(new AuthorResponse(author.getName(), bookTitles));}왜 발생하는가? JPA Session (EntityManager) 종료: 트랜잭션이 끝나면 Session도 함께 닫힘 프록시 객체의 한계: Lazy 컬렉션은 프록시로 래핑되어 있어 Session이 필요 open-in-view: false: HTTP 요청 범위에서 Session을 유지하지 않음// Author 객체 내부의 books 필드는 실제로 이런 프록시books = new PersistentBag(); // Hibernate의 지연 로딩 컬렉션// Session이 없으면 실제 데이터를 가져올 수 없음해결 방법1. 트랜잭션 범위 확장 (Controller에 @Transactional)@GetMapping(\"/authors/{id}\")@Transactional(readOnly = true) // 트랜잭션을 Controller까지 확장public ResponseEntity&lt;AuthorResponse&gt; getAuthor(@PathVariable Long id) { var author = authorService.findAuthor(id); // 트랜잭션 내에서 Lazy Loading 수행 - 정상 동작 var bookTitles = author.getBooks().stream() .map(Book::getTitle) .collect(Collectors.toList()); return ResponseEntity.ok(new AuthorResponse(author.getName(), bookTitles));}장점: 간단한 수정단점: Controller에 트랜잭션 로직이 들어감, 레이어 경계가 모호해짐2. Eager Fetch Join 사용@Repositorypublic interface AuthorRepository extends JpaRepository&lt;Author, Long&gt; { @Query(\"SELECT a FROM Author a JOIN FETCH a.books WHERE a.id = :id\") Optional&lt;Author&gt; findByIdWithBooks(@Param(\"id\") Long id); // 또는 여러 연관관계를 한 번에 @Query(\"SELECT DISTINCT a FROM Author a \" + \"JOIN FETCH a.books b \" + \"JOIN FETCH b.category \" + \"WHERE a.id = :id\") Optional&lt;Author&gt; findByIdWithBooksAndCategories(@Param(\"id\") Long id);}@Servicepublic class AuthorService { @Transactional(readOnly = true) public Author findAuthorWithBooks(Long id) { return authorRepository.findByIdWithBooks(id).orElse(null); // books 데이터가 이미 로드되어 있어서 Session 종료 후에도 접근 가능 }}장점: 명확한 데이터 로딩, N+1 문제 해결단점: 항상 연관 데이터를 로드함 (불필요한 경우에도)3. @EntityGraph 활용@Repositorypublic interface AuthorRepository extends JpaRepository&lt;Author, Long&gt; { @EntityGraph(attributePaths = {\"books\", \"books.category\"}) @Query(\"SELECT a FROM Author a WHERE a.id = :id\") Optional&lt;Author&gt; findByIdWithBooks(@Param(\"id\") Long id); // 여러 시나리오에 대응 @EntityGraph(attributePaths = {\"books\"}) Optional&lt;Author&gt; findById(Long id); // 기본 메서드 오버라이드}장점: 선언적 방식, 유연함단점: 복잡한 그래프에서는 예측이 어려울 수 있음4. DTO 변환을 트랜잭션 내에서 (권장)@Transactional(readOnly = true)public AuthorResponse findAuthorResponse(Long id) { var author = authorRepository.findById(id).orElse(null); if (author == null) { return null; } // 트랜잭션 내에서 필요한 데이터를 모두 접근하여 DTO로 변환 var bookTitles = author.getBooks().stream() .map(Book::getTitle) .collect(Collectors.toList()); var authorStats = AuthorStats.builder() .totalBooks(author.getBooks().size()) .avgRating(calculateAverageRating(author.getBooks())) .build(); return AuthorResponse.builder() .name(author.getName()) .email(author.getEmail()) .bookTitles(bookTitles) .stats(authorStats) .build();}private double calculateAverageRating(List&lt;Book&gt; books) { return books.stream() .mapToDouble(Book::getRating) .average() .orElse(0.0);}@GetMapping(\"/authors/{id}\")public ResponseEntity&lt;AuthorResponse&gt; getAuthor(@PathVariable Long id) { var response = authorService.findAuthorResponse(id); return response != null ? ResponseEntity.ok(response) : ResponseEntity.notFound().build();}장점: 명확한 레이어 분리, 필요한 데이터만 노출, 성능 최적화단점: DTO 클래스 추가 필요5. Batch Fetch Size 설정으로 N+1 문제 완화spring: jpa: properties: hibernate: default_batch_fetch_size: 100 # 한 번에 100개씩 배치로 로딩 jdbc: batch_size: 50 # INSERT/UPDATE 배치 크기@Transactional(readOnly = true)public List&lt;AuthorResponse&gt; findAllAuthorsWithBooks() { var authors = authorRepository.findAll(); // 1번 쿼리 return authors.stream() .map(author -&gt; { // Batch Fetch Size 덕분에 100개씩 묶어서 쿼리 실행 // N+1이 아니라 N/100+1 쿼리로 최적화 var bookTitles = author.getBooks().stream() .map(Book::getTitle) .collect(Collectors.toList()); return new AuthorResponse(author.getName(), bookTitles); }) .collect(Collectors.toList());}6. 프로젝션(Projection) 활용public interface AuthorBookProjection { String getName(); String getEmail(); List&lt;BookInfo&gt; getBooks(); interface BookInfo { String getTitle(); String getIsbn(); BigDecimal getPrice(); }}@Repositorypublic interface AuthorRepository extends JpaRepository&lt;Author, Long&gt; { @Query(\"SELECT a.name as name, a.email as email, \" + \"b.title as books_title, b.isbn as books_isbn, b.price as books_price \" + \"FROM Author a LEFT JOIN a.books b WHERE a.id = :id\") AuthorBookProjection findAuthorBookProjection(@Param(\"id\") Long id);}성능 최적화 실전 팁1. 연관관계 설계 시 고려사항@Entitypublic class Author { // 필수: 양방향 관계에서 연관관계 주인 설정 @OneToMany(mappedBy = \"author\", fetch = FetchType.LAZY) private List&lt;Book&gt; books = new ArrayList&lt;&gt;(); // 성능 최적화: 컬렉션 초기화 public List&lt;Book&gt; getBooks() { return books != null ? books : Collections.emptyList(); } // 편의 메서드: Lazy Loading을 위한 크기 체크 public boolean hasBooks() { return books != null &amp;&amp; !books.isEmpty(); } public int getBooksCount() { return books != null ? books.size() : 0; // size() 호출 시 실제 로딩됨 }}2. 조건부 Lazy Loading@Servicepublic class AuthorService { @Transactional(readOnly = true) public AuthorResponse findAuthorResponse(Long id, boolean includeBooks) { var author = authorRepository.findById(id).orElse(null); if (author == null) return null; var builder = AuthorResponse.builder() .name(author.getName()) .email(author.getEmail()); // 필요한 경우에만 Lazy Loading 수행 if (includeBooks &amp;&amp; author.hasBooks()) { var bookTitles = author.getBooks().stream() .map(Book::getTitle) .collect(Collectors.toList()); builder.bookTitles(bookTitles); } return builder.build(); }}핵심 원칙 데이터 접근은 트랜잭션 내에서: Session이 열려있을 때 모든 필요 데이터에 접근 명시적 데이터 로딩: Fetch Join, EntityGraph 등으로 의도를 명확히 표현 적절한 DTO 활용: 필요한 데이터만 추출하여 레이어 간 결합도 감소 성능 측정 기반 최적화: N+1 문제와 메모리 사용량을 모니터링하며 최적화Dirty Checking 이해하기Hibernate의 핵심 기능 중 하나인 Dirty Checking에 대해 알아보자.@Transactionalpublic void updateAuthor(Long id) { var author = authorRepository.findById(id).get(); // 엔티티 수정 - 명시적인 save() 호출 없이도 UPDATE 쿼리 실행됨 author.setName(\"변경된 이름\"); author.setEmail(\"new@email.com\"); // 트랜잭션 커밋 시점에 자동으로 UPDATE 쿼리 생성}Dirty Checking 동작 원리 1차 캐시 저장 시점: 엔티티 조회 시 현재 상태의 스냅샷을 생성 트랜잭션 커밋 시점: 현재 엔티티와 스냅샷을 비교 변경 감지: 달라진 필드가 있으면 UPDATE 쿼리 자동 생성// 내부 동작 개념public class EntityEntry { private Object entity; // 현재 엔티티 private Object[] snapshot; // 조회 시점의 스냅샷 public boolean requiresUpdate() { // 현재 엔티티와 스냅샷 비교 return !Arrays.equals(getCurrentState(), snapshot); }}성능 고려사항1. 불필요한 스냅샷 생성 방지@Transactional(readOnly = true)public List&lt;AuthorDto&gt; findAllAuthorsForReport() { return authorRepository.findAll().stream() .map(author -&gt; new AuthorDto(author.getName(), author.getEmail())) .collect(Collectors.toList());}읽기 전용이므로 스냅샷 생성하지 않음2. 대용량 조회 시 주의@Transactionalpublic void processAllAuthors() { var authors = authorRepository.findAll(); // 1만건 조회 // 1만개의 스냅샷 생성 → 메모리 사용량 증가 // 실제로는 수정하지 않아도 스냅샷은 생성됨 authors.forEach(author -&gt; { // 단순 읽기 작업만 수행 generateReport(author); });}3. 선택적 업데이트 vs 전체 필드 업데이트// @DynamicUpdate 없는 경우 - 모든 필드 UPDATE@Entitypublic class Author { // UPDATE author SET name=?, email=?, created_at=? WHERE id=?}// @DynamicUpdate 있는 경우 - 변경된 필드만 UPDATE@Entity@DynamicUpdatepublic class Author { // UPDATE author SET name=? WHERE id=?}최적화 팁 단순 조회만 하는 경우: @Transactional(readOnly = true) 또는 트랜잭션 없이 대용량 데이터 처리: 청크 단위로 분할하여 메모리 사용량 제어 부분 업데이트: @DynamicUpdate 활용" }, { "title": "Application Load Balancer에 고정 IP 사용하기 (feat. VPC IPAM 통합)", "url": "/posts/aws-alb-static-ip/", "categories": "AWS", "tags": "AWS", "date": "2025-04-12 20:12:00 +0900", "snippet": "Application Load Balancer에 고정 IP 사용하기 (feat. VPC IPAM 통합)aws 콘솔 보다가 이런걸 발견했다.궁금해서 찾아보니까 이런 글을 확인할 수 있었다. (마참내!)ALB에 고정IP를 사용하려고 앞에 NLB 붙이기, 프록시 서버 사용하기, Global Accelerator과 통합하기, Lambda 사용하기 등 다양한 방법들을 사용해왔다. 그런데 이제 ALB에 직접적으로 고정 IP를 사용할 수 있게 된 것이다.VPC IPAM이란?Amazon VPC IP Address Manager(IPAM)는 AWS 워크로드의 IP 주소를 보다 쉽게 계획, 추적 및 모니터링할 수 있게 해주는 VPC 기능이다. IPAM을 사용하면 조직, 계정, VPC 및 AWS 리전 전체에서 IP 주소 배정을 중앙에서 관리하고 감사할 수 있다. 범위(Scope): IPAM에서 가장 상위 컨테이너로, 단일 네트워크의 IP 공간을 나타낸다. IPAM 생성 시 퍼블릭 범위와 프라이빗 범위가 자동으로 생성된다. 풀(Pool): 연속된 IP 주소 범위(CIDR)의 모음으로, 라우팅 및 보안 요구사항에 따라 IP 주소를 구성할 수 있다. 최상위 풀 내에 여러 개의 풀이 존재할 수 있다. 할당(Allocation): IPAM 풀에서 AWS 리소스나 다른 IPAM 풀로 CIDR을 할당하는 것을 의미한다.Application Load Balancer와 VPC IPAM의 통합2025년 3월에 AWS는 Application Load Balancer(ALB)와 Amazon VPC IPAM의 통합을 발표했다. 이 통합을 통해 고객은 로드 밸런서 노드에 IP 주소를 할당할 때 사용할 퍼블릭 IPv4 주소 풀을 직접 제공할 수 있게 되었다.주요특징 IP 주소 풀 지정: 고객은 퍼블릭 IPAM 풀을 구성할 수 있으며, 이 풀은 다음과 같은 주소로 구성될 수 있다. 고객 소유 Bring Your Own IP Address(BYOIP) Amazon이 제공하는 연속 IPv4 주소 블록 비용 최적화: BYOIP를 사용하여 퍼블릭 IPv4 비용을 최적화할 수 있다. 2024년 2월 1일부터 AWS에서는 모든 서비스 관리형 퍼블릭 IPv4 주소에 대해 요금을 부과하고 있다. 그러나 BYOIP 주소는 이러한 요금이 부과되지 않는다. 간편한 허용 목록 관리: Amazon이 제공하는 연속 IPv4 블록을 사용하여 사내 허용 목록 작성 및 운영 작업을 간소화할 수 있다. 자동 전환 메커니즘: IPAM 풀에서 공급되는 ALB의 IP 주소는 퍼블릭 IPAM 풀의 주소가 소진되면 AWS 관리형 IP 주소로 자동 전환된다. 이를 통해 규모 조정 이벤트 중에도 서비스 가용성을 최대화할 수 있다.ALB와 IPAM 통합하기1. IPAM, 풀 생성AWS 콘솔에서 Amazon VPC IP Address Manager 를 검색해서 이동한 뒤, IPAM 과 풀을 생성한다.CIDR 넷마스크 길이는 ALB의 가용 영역 갯수에 따라 선택하면 될 것 같다.2. ALB IP 풀 편집사용중인 ALB -&gt; 네트워크 매핑 -&gt; IP 풀 편집을 클릭하여 IP 풀 편집 화면으로 이동한다.생성한 IPv4 IPAM 풀을 선택하고 저장하면 통합이 완료된다.3. 확인다시 IPAM 콘솔로 돌아와 가용 영역의 갯수에 맞춰 IP가 리소스에 배정되었는지 확인해 보자.정확한 확인을 위해 nslookup 명령어로 할당된 IP가 리턴되는지 확인해 보자.결론ALB와 IPAM의 통합을 통해 ALB에 고정 IP를 사용할 수 있게 되었다. 당연한 이야기겠지만 이런 아키텍처를 사용한다면 CloudFront에 고정 IP를 사용한것이 아니기 때문에 무용지물이다. (물론 이 경우 Anycast 고정 IP 를 사용하면 된다.)API 서비스 제공자라면 고객의 방화벽 설정을 위해 IPv4 주소를 요구하는 경우가 있다. 저희가 API를 사용하려는데, 방화벽 시스템 때문에 IP 등록이 필요합니다. DNS가 아닌 IPv4 주소를 알려주실 수 있을까요? 와 같은 요청이 빈번하게 발생하기 때문이다. 이때 위 기능이 도움이 될 것이다." }, { "title": "야간 및 주말에 AWS Fargate 동적으로 운영하기", "url": "/posts/aws-fargate-dynamic-operation/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2024-12-14 08:12:00 +0900", "snippet": "야간 및 주말에 AWS Fargate 동적으로 운영하기사내에서만 사용하는 관리자(관제) 서비스가 있다. 아주 가끔 업무 시간외에 쓸일이 있긴 하지만 대부분의 경우 평일 21시 ~ 08시, 주말에는 전혀 사용하지 않는다.클라우드의 장점중 필요한 만큼만 사용, 필요할 때만 사용을 중요한 장점으로 생각한다. 그래서 이 기회에 AWS Fargate 를 동적으로 운영해보기로 했다. :exclamation: 본 포스팅에서 소개하는 로직은, 제 개인적인 접근 방식이므로 실제 상용 시스템에 적용하기 전 철저한 테스트 과정이 필요합니다.현재 아키텍처현재 아키텍처는 다음과 같다.CloudFront에서 1차적인 요청을 처리하고 경로가 /api 로 시작한다면 api endpoints로 간주하여 로드밸런서 - Fargate로 요청을 보내고 그 외는 frontend api로 간주하여 s3로 요청을 보낸다.프론트는 React, 백엔드는 Spring Boot로 구성되어 있다.업무 외 시간 아키텍처업무 외 시간 아키텍처는 다음과 같다.프론트엔드로 가는 요청은 건들지 않았다. 아예 처음부터 CloudFront 도 업무 외 시간 트래픽을 핸들링할수 있게 별도의 배포를 구성할까도 고민해봤지만, CloudFront 배포에서는 동일한 대체 도메인 이름을 여러 배포에 중복해서 사용할 수 없기 때문에 포기했다.예를들어 업무시간에는 a.b.c -&gt; a 배포, 그 외 시간에는 a.b.c -&gt; b 배포 이게 안된다. 물론 업무 외 시간에 a 배포의 대체 도메인을 바꿔버리면 되지만 CloudFront 수정 후 배포 시간이 꽤 소요되기 때문에 포기했다.백엔드 아키텍처를 변경했다. 기존 Fargate로 가는 요청을 Lambda로 보낸다. 그 외 EventBridge 가 수행하는 동작에 대해서는 아래에서 설명한다.Lambda 함수 작성하기람다가 처리하는 일이 많다. 처리하는일은 다음과 같다. EventBridge 스케줄러 실행 요청 처리 EventBridge 이벤트 규칙을 통해 전달된 이벤트 처리 Application Load Balancer 로부터 전달된 요청 처리다음은 람다의 index 코드이다. 여기서 분기를 통해 각 이벤트를 처리한다.export const handler = async (event, context) =&gt; { if (event) { // 로드밸런서 요청 if (event.requestContext &amp;&amp; event.requestContext.elb) { return handleAlb(event, context); } if (event.sourceLocation &amp;&amp; event.sourceLocation === 'EventBridge') { // 서비스가 SERVICE_STEADY_STATE 가 되었을때 if (event.eventName === 'SERVICE_STEADY_STATE') { return handleService(event, context); } // EventBridge 스케줄러 실행 요청 처리 if (event.eventName === 'FARGATE_SCALING_TRIGGER') { return handleScaling(event, context); } } } return null;};다음은 시스템을 핸들링할때 사용될 서비스 데이터이다. 사실 호스트나 기타 데이터로 필요한 arn들을 역으로 찾아갈수도 있겠으나 그렇게 되면 불필요한 호출이나 로직이 들어가기 때문에 필요한 정보들을 변수에 담아두기로 했다.const SERVICE_DATA = [ { key: 'admin', host: 'admin.keencho.com', cluster: 'keencho-cluster', service: 'admin', albArn: '로드 밸런서 arn', albRuleArns: ['실제 admin.keencho.com 을 HTTP 호스트 헤더 조건으로 가지고 있는 리스너 규칙 arn 리스트'], lambdaTargetGroupArn: '람다가 포함되어 있는 대상그룹 arn', ecsFargateTargetGroupArn: ['실제 fargate 태스크가 포함되어 있는 대상그룹 arn 리스트'] }]EventBridge 스케줄러 실행 요청 처리첫번째는 EventBridge 스케줄러 실행 요청 처리이다. Amazon EventBridge -&gt; Scheduler -&gt; 일정 에서 설정하였으며 어떻게 설정하는지까지 설명하진 않는다. 나의 경우 30분마다 작동하게 설정하였으며, 다음과 같은 페이로드를 전달하도록 하였다.{ \"sourceLocation\": \"EventBridge\", \"eventName\": \"FARGATE_SCALING_TRIGGER\" }위 페이로드를 전달받은 람다는 handleScaling 메소드를 호출한다. 이 메소드는 1차로 시간을 확인한다. 현재 시간이 주말이거나 평일 21시 ~ 08시 사이면 stop 메소드를, 그 외에는 start 메소드를 호출한다.const handleScaling = async(event, context) =&gt; { const now = new Date(); const utcHour = now.getUTCHours(); const kstHour = (utcHour + 9) % 24 const isNightTime = (kstHour &gt;= 21) || (kstHour &lt; 8); const kstTime = new Date(now.getTime() + (9 * 60 * 60 * 1000)); const kstDay = kstTime.getUTCDay(); const isWeekend = kstDay === 0 || kstDay === 6; for (const data of SERVICE_DATA) { if (isNightTime || isWeekend) { await stop(data) } else { await start(data, context) } }}1. stop멈춰야 하는 시간대이면 멈춘다. 다음은 사용되는 전체 코드이다.// 서비스 가져오기const getService = async(ecsClient, clusterName, serviceName) =&gt; { const response = await ecsClient.send(new DescribeServicesCommand({ cluster: clusterName, services: [serviceName] })) if (!response || !response.services || response.services.length !== 1) { return undefined } return response.services[0];}// 트래픽 검증const checkTraffic = async(data) =&gt; { const now = new Date(); const end = new Date(Date.UTC( now.getUTCFullYear(), now.getUTCMonth(), now.getUTCDate(), now.getUTCHours(), now.getUTCMinutes(), now.getUTCSeconds() )); // 20분 const start = new Date(end.getTime() - 20 * 60 * 1000); // 간격: 20분 const period = 1200; const loadBalancerArn = data.albArn const loadBalancerName = loadBalancerArn.split(':').pop().split('/').slice(1).join('/'); const sum = await data.ecsFargateTargetGroupArn.reduce(async (acc, arn) =&gt; { const accumulated = await acc; const tgName = arn.split(':').pop() const params = { StartTime: start, EndTime: end, MetricDataQueries: [ { Id: \"count\", MetricStat: { Metric: { Namespace: \"AWS/ApplicationELB\", MetricName: \"HTTPCode_Target_2XX_Count\", // 200번대 상태 코드만 필터링 Dimensions: [ { Name: \"LoadBalancer\", Value: loadBalancerName }, { Name: \"TargetGroup\", Value: tgName } ] }, Period: period, Stat: \"Sum\" // 요청 개수의 합계 }, Label: \"200 Status Code Requests\" } ] }; const cloudwatch = new CloudWatchClient({ region: 'ap-northeast-2' }); const command = new GetMetricDataCommand(params); const response = await cloudwatch.send(command); const results = response.MetricDataResults[0] const trafficSum = (!results.Values || results.Values.length === 0) ? 0 : results.Values.reduce((sum, number) =&gt; sum + number, 0) return accumulated + trafficSum; }, Promise.resolve(0)) return sum === 0}const stop = async(data) =&gt; { ////////// 1. 멈춰야 하는 서비스인지 검증 const ecsClient = new ECSClient({region: 'ap-northeast-2'}); const service = await getService(ecsClient, data.cluster, data.service); if (service === undefined || service.desiredCount === 0) { return } // 새벽에 도는 스케쥴의 경우 현재 desiredCount가 1 이상인 경우 지난 20분간 2xx 상태코드를 가진 트래픽이 있는지 검증한다. // 사용중인데 꺼지면 안되기 때문이다. if (!await checkTraffic(data)) { return; } ////////// 2. 멈춰야 하는 서비스라면 해당 서비스로 라우팅중인 로드밸런서 룰 변경 const albClient = new ElasticLoadBalancingV2Client({region: 'ap-northeast-2'}) const describeResponse = await albClient.send( new DescribeRulesCommand({ RuleArns: data.albRuleArns }) ); for (const rule of (describeResponse.Rules || [])) { if (rule.Actions &amp;&amp; rule.Actions.length === 1 &amp;&amp; rule.Actions[0].Type === 'forward') { const action = rule.Actions[0] const updatedAction = { ...action, TargetGroupArn: data.lambdaTargetGroupArn, ForwardConfig: { ...action.ForwardConfig, TargetGroups: [ { TargetGroupArn: data.lambdaTargetGroupArn, Weight: 1 } ] } }; await albClient.send( new ModifyRuleCommand({ RuleArn: rule.RuleArn, Actions: [updatedAction] }) ) } } ////////// 3. 서비스 desired count 0 처리 const params = { cluster: data.cluster, service: data.service, desiredCount: 0 } await ecsClient.send(new UpdateServiceCommand(params));}순서는 다음과 같다. 멈춰야 하는 서비스인지 검증한다. 이미 ecs 서비스의 desired count가 0 이면 수행하지 않는다. 지난 20분간 2xx 상태코드를 리턴한 요청이 있으면 수행하지 않는다. (이 스케줄러는 30분 간격으로 도는데 도는 시점에 사용하고 있을수도 있기 때문에) 실제 admin.keencho.com 을 HTTP 호스트 헤더 조건으로 가지고 있는 리스너 규칙 arn 리스트 리스너 규칙의 작업을 람다 대상그룹으로 전달 로 변경한다. ecs 서비스의 desired count를 0으로 변경한다.이렇게 하면 api 트래픽은 람다로 가고 서비스의 태스크 수는 0이 된다. 태스크가 동작하지 않으므로 당연히 요금이 부과되지 않는다.2. start업무시간이 다가오면 시작한다. 다음은 사용되는 전체 코드이다.const start = async(data, context) =&gt; { ////////// 1. 시작해야 하는 서비스인지 검증 const ecsClient = new ECSClient({ region: 'ap-northeast-2' }); const service = await getService(ecsClient, data.cluster, data.service); if (service === undefined || service.desiredCount !== 0) { return } ////////// 2. desired count 1 처리 const params = { cluster: data.cluster, service: data.service, desiredCount: 1 }; await ecsClient.send(new UpdateServiceCommand(params)); ////////// 3. EventBridge에 규칙 등록 (이미 등록되어 있는 경우 업데이트됨) const ruleName = `ecs-monitor-${data.cluster}_${data.service}` const ruleParams = { Name: ruleName, EventPattern: JSON.stringify({ source: [\"aws.ecs\"], \"detail-type\": ['ECS Service Action'], resources: [service.serviceArn], detail: { eventName: ['SERVICE_STEADY_STATE'] } }), }; const eventBridgeClient = new EventBridgeClient({ region: \"ap-northeast-2\" }); const result = await eventBridgeClient.send(new PutRuleCommand(ruleParams)); ////////// 4. 규칙에 대한 타겟 설정 const targetParams = { Rule: ruleName, Targets: [ { Id: `${ruleName}-target`, Arn: context.invokedFunctionArn, Input: JSON.stringify({ sourceLocation: 'EventBridge', eventName: 'SERVICE_STEADY_STATE', clusterArn: service.clusterArn, serviceArn: service.serviceArn, albArn: data.albArn, key: data.key }) } ] } await eventBridgeClient.send(new PutTargetsCommand(targetParams)); ////////// 5. lambda에 리소스 액세스 권한 부여 try { const lambdaClient = new LambdaClient({ region: \"ap-northeast-2\" }); const addPermissionParams = { Action: \"lambda:InvokeFunction\", FunctionName: context.functionName, Principal: \"events.amazonaws.com\", SourceArn: result.RuleArn, StatementId: `${ruleName}-permission` }; await lambdaClient.send(new AddPermissionCommand(addPermissionParams)); } catch (error) { // 권한이 이미 존재하는 경우 에러 발생 (StatementId 중복) if (error.name === 'ResourceConflictException') { // ... } }}순서는 다음과 같다. 멈춰야 하는 서비스인지 검증한다. 이미 ecs 서비스의 desired count가 1 이면 수행하지 않는다. ecs 서비스의 desired count를 1으로 변경한다. EventBridge에 규칙을 등록한다. detail type: ECS Service Action event type: SERVICE_STEADY_STATE 참조 등록한 규칙에 대한 타겟을 설정한다. 람다와 Input 데이터를 지정한다. EventBridge 규칙이 람다에 접근할 수 있도록 액세스 권한을 부여한다. 이 작업은 최초 1회만 하면 되며 그 이후 동일한 작업 수행시 ResourceConflictException 에러가 발생한다. 이렇게 하면 ecs 서비스 태스크 수가 1이 된다. 그러나 실제 로드밸런서 리스너 규칙을 변경하지는 않는다. 물론 이 시점에서 폴링을 통해 태스크 상태를 검증하는 방법도 있지만 람다는 실행시간 만큼 요금이 부과되기 때문에 ecs 서비스가 안정적인 상태에 도달하면 람다를 다시 호출하는 이벤트를 EventBridge에 등록하는 방식을 선택했다.EventBridge 이벤트 규칙을 통해 전달된 이벤트 처리ecs 서비스의 태스크 수를 1로 변경하고 서비스가 안정적인 상태에 도달한다면, EventBridge에 등록해둔 SERVICE_STEADY_STATE 이벤트가 트리거되어 람다를 호출한다. 다음은 이를 핸들링하는 코드이다.const handleService = async(event) =&gt; { const data = SERVICE_DATA.find(item =&gt; item.key === event.key) if (!data) { return; } const input = { services: [event.serviceArn], cluster: event.clusterArn } const ecsClient = new ECSClient({ region: 'ap-northeast-2' }); const command = new DescribeServicesCommand(input); const response = await ecsClient.send(command); if (!response.services || response.services.length === 0) { return; } const service = response.services[0]; if (service.desiredCount === 0) { return; } if (!service.loadBalancers || service.loadBalancers.length === 0) { return; } const targetChangeArns = data.albRuleArns const targetGroupArn = service.loadBalancers[0].targetGroupArn; const albClient = new ElasticLoadBalancingV2Client({ region: 'ap-northeast-2' }) const describeResponse = await albClient.send( new DescribeRulesCommand({ RuleArns: targetChangeArns }) ); if (!describeResponse.Rules || describeResponse.Rules.length === 0) { return; } const rule = describeResponse.Rules[0]; // 이미 라우팅이 정상인 경우 -&gt; 배포의 경우 if (rule.Actions.some(item =&gt; item.TargetGroupArn === targetGroupArn)) { return; } const updatedActions = rule.Actions.map(action =&gt; { return { ...action, TargetGroupArn: targetGroupArn, ForwardConfig: { ...action.ForwardConfig, TargetGroups: [ { TargetGroupArn: targetGroupArn, Weight: 1 } ] } }; }); for (const ruleArn of targetChangeArns) { await albClient.send( new ModifyRuleCommand({ RuleArn: ruleArn, Actions: updatedActions }) ) }}순서는 다음과 같다. 넘어온 페이로드 의 key로 SERVICE_DATA 에서 필요한 데이터를 찾는다. ecs 서비스의 desired count가 0 이면 수행하지 않는다. 이미 리스너 규칙이 올바른 대상 그룹으로 전달되고 있을 경우 수행하지 않는다. 서비스가 배포되는 경우도 이 람다가 실행되기 때문에 추가된 조건이다. 람다 대상그룹을 바라보고 있는 리스너 규칙을 ecs 서비스 태스크가 포함된 대상그룹으로 변경한다 나의 경우 CodeDeploy 기반 배포를 진행하고 있고, 이는 2개의 대상그룹을 필요로 한다. 따라서 변경할 태스크 대상그룹은 하드코딩된 대상그룹이 아닌 ecs 서비스와 엮여있는 대상그룹으로 해야 한다. Rolling Update 기반 배포의 경우 대상그룹이 1개일 것이기 때문에 하드코딩된 대상그룹을 사용해도 무방하다. Application Load Balancer 로부터 전달된 요청 처리지금까지의 작업만 진행하면 업무 외 시간에는 서비스의 필요 태스크가 0이 되고 업무 시간이 다가오면 서비스의 필요 태스크가 1이 된다. 하지만 만약에 긴급하게 서비스를 이용해야할 경우는 어떻게 해야 할까? 이에 대해 내가 생각한 방법을 작성해보도록 한다.1. 요청 검증하기앞서 서비스를 stop 할때, 리스너 규칙이 바라볼 대상을 람다로 변경했다. 업무 외 시간 긴급하게 서비스를 이용해야할 경우 이 람다로 요청이 전달될 것이다. 이 경우 서비스를 다시 시작해야 한다. 다만 2가지 고려사항이 있다. 프로그램(봇) 에 의해 찔러본 요청인가? 실제 서비스 (Spring Boot) 에 존재하는 경로인가?1번의 경우 CloudFront 앞단에 WAF 를 배치하여 봇을 컨트롤 할 수 있다. 람다에서 직접 컨트롤 할 수도 있겠지만 내 경우 그냥 WAF 로 컨트롤하는 쪽을 선택했다.2번의 경우 생각을 좀 해봤는데 내가 생각한 방법은 다음과 같다. @PostConstruct를 이용하여 di가 끝난 후 앱 내에 있는 모든 경로를 검색해서 s3에 배열로 저장한다. 실제 람다가 호출되었을때 배열에 path와 일치하는 요소가 존재하는 경우만 서비스를 시작한다.다음은 모든 경로를 검색해서 s3에 업로드하는 코드이다. s3util 부분만 실제 올리는 로직으로 변경하면 될 것이다.@Profile(\"prod\")@Componentpublic class EndpointGenerator { @Autowired RequestMappingHandlerMapping requestMappingHandlerMapping; @Autowired S3Util s3Util; @Autowired ObjectMapper objectMapper; @Autowired WebApplicationContext webApplicationContext; @PostConstruct public void init() throws JsonProcessingException { var patterns = new ArrayList&lt;String&gt;(); var contextPath = webApplicationContext.getServletContext().getContextPath(); for (var info : requestMappingHandlerMapping.getHandlerMethods().keySet()) { var patternWithContext = info.getPatternValues().stream() .map(pattern -&gt; contextPath + pattern) .collect(Collectors.toList()); patterns.addAll(patternWithContext); } s3Util.upload( \"keencho-bucket\", \"admin.json\", objectMapper.writeValueAsString(patterns) ); }}2. 시작하기이제 실제 시작에 필요한 코드를 작성해보자. 로드밸런서의 요청을 처리해야 하기 때문에 그에 맞는 형식을 리턴할 필요가 있다.const buildResponse = (success, data) =&gt; { return { headers: { \"Content-Type\": \"application/json\", }, statusCode: success ? 200 : 500, body: JSON.stringify(data) }}const errorResponse = () =&gt; buildResponse(false, { success: false, message: 'Invalid access' })const successResponse = () =&gt; buildResponse(true, { success: true })const handleAlb = async(event, context) =&gt; { /////////////////////////////////////// 헬스 체크 if (event.path === '/health-check') { return { statusCode: 200, statusDescription: \"OK\", isBase64Encoded: false, headers: { \"Content-Type\": \"application/json\" } }; } try { return handle(event, context) } catch (error) { return errorResponse() }}const isValidRoute = async(type, url) =&gt; { const patternToRegex = (pattern) =&gt; { const regexPattern = pattern .replace(/\\//g, '\\\\/') // 슬래시 이스케이프 .replace(/\\*\\*/g, '.*') // ** 패턴을 .* (모든 문자, 여러 개)로 변환 .replace(/{([^}]+)}/g, '([^/]+)'); // {변수} 패턴을 캡처 그룹으로 변환 return new RegExp(`^${regexPattern}$`); } const client = new S3Client({}); const command = new GetObjectCommand({ Bucket: 'keencho-bucket', Key: type + '.json' }) const responseData = await client.send(command); const str = await responseData.Body.transformToString(); const routes = JSON.parse(str); for (const route of routes) { const regex = patternToRegex(route); if (regex.test(url)) { return true; } } return false;}const handle = async(event, context) =&gt; { const path = event.path; const host = event.headers.host; const data = SERVICE_DATA.find(item =&gt; item.host === host); if (!data) { return errorResponse() } const type = data.key ////////// 1. 검증 if (!type || !await isValidRoute(type, path) || !event?.headers?.referer?.includes(host)) { return errorResponse() } ////////// 2. desired count 이미 1 이상인지 확인 const ecsClient = new ECSClient({region: 'ap-northeast-2'}); const service = await getService(ecsClient, data.cluster, data.service); if (service === undefined) { return errorResponse() } // 이미 시작된 경우라고 간주함. if (service.desiredCount &gt; 0) { return successResponse(type) } ////////// 3. desire task 1 처리 await start(data, context) return successResponse(type)}순서는 다음과 같다. 데이터를 검증한다. isValidRoute() 메소드를 통해 유효한 경로인지 검증한다. @PathVariable를 사용한 경로일 수 있으므로 그에대한 정규식이 필요하다. 동일 출처인지 검증한다. 어짜피 따로 허용하는 코드를 작성하지 않았으므로 에러가 발생할 테지만 명시적으로 검증한다. ecs 서비스의 desired count가 0보다 크면 수행하지 않는다. 앞서 작성한 start() 함수를 호출한다. 성공 응답을 리턴한다.이미 검증을 했으므로 start() 메소드에서 또 검증할 필요는 없다. 불필요한 호출을 막는 로직을 추가하면 좋을것 같다. (여기선 하지 않는다.)이후 태스크가 시작되면 EventBridge에 등록해둔 SERVICE_STEADY_STATE가 트리거되고 앞서 살펴본 이벤트 처리 로직 에 의해 처리된다.결론현재 사내 시스템(관리자)과 테스트서버 에만 위 아키텍처를 적용하고 있다. 테스트서버의 경우 새벽시간대에 시작하는 로직이 빠져있다. 그냥 무조건 업무시간에만 쓰도록 한 것이다.코드나 로직은 조금 보완해야 겠지만 예상대로 돌아감에 만족한다. 추가로 이 포스팅에 작성하진 않았지만 중지 시간대에 다음과 같이 UI가 구성되어 있으면 좋을것이다.내 경우 서비스 유지기간인 경우 아예 별도의 페이지로 redirect 시키고 시작해야할 서비스면 서비스를 시작시키고 상태를 START, PROCESSING, COMPLETED 3단계로 간단하게 나누어 확인할 수 있도록 하고 시작하지 않아야 할 서비스면 위와 같은 화면이 보이도록 UI를 구성했다.Instance Scheduler 이라는 솔루션이 존재하긴 하는데 이는 ECS Fargate 대상으로는 자동화 못하는거 같아 EventBridge + Lambda 조합으로 직접 구현해 봤다. 덕분에 리소스 비용이 조금이나마 절감되어 좋은듯 하다." }, { "title": "Arm 아키텍처에서 AAPT2 빌드 도구 사용하기", "url": "/posts/aapt2-arm/", "categories": "Android", "tags": "Android, Arm", "date": "2024-09-22 08:12:00 +0900", "snippet": "Arm 아키텍처에서 AAPT2 빌드 도구 사용하기최근 개발서버의 아키텍처를 x86 에서 Arm 으로 변경했다. (더 싸서)해당 서버에는 gitea, Jenkins, Nexus Repository 가 굴러가고 있었다. 마이그레이션은 그럭저럭 순조롭게 진행했는데 문제는 Jenkins job을 마이그레이션 하던중, 특정 job에서 안드로이드를 빌드하는 과정에서 나타났다.stage('android build') { steps { script { dir('android') { sh 'chmod +x gradlew' sh './gradlew assembleRelease' } } }}그냥 간단한 apk 빌드 명령어다. 근데 아래와 같은 에러 메시지가 발견됐다.AAPT2 aapt2-3.6.1-6040484-linux Daemon #0: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #26: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #23: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #36: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #40: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #38: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #47: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #55: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #85: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary fileAAPT2 aapt2-3.6.1-6040484-linux Daemon #84: Unexpected error output: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: /var/lib/jenkins/.gradle/caches/transforms-2/files-2.1/52f8b6df1762ac7006c8d9f8d6d1e0ad/aapt2-3.6.1-6040484-linux/aapt2: cannot execute binary file서버 아키텍처는 x86인데 바이너리 파일의 아키텍처는 arm 이라서 발생한 문제였다.그래서 당연히 구글에서 arm용 aapt2 파일을 제공할줄 알고 문서를 뒤져보았는데… 아니 글쎄 아무리 찾아봐도 없는거 아닌가?심지어 이런 이슈 트래커도 찾아볼 수 있었다.궁시렁대면서 찾아보니 build-tool 을 arm용으로 컴파일할 수 있는 저장소 를 찾을 수 있었다. 문서에는 직접 빌드하는 방법에 대해 설명하고 있지만 난 그냥 릴리즈 되어있는 결과물을 다운받아 사용했다.다음과 같은 순서로 적용했다 build-tool 다운받아 aapt2 파일을 원하는 곳에 복사 Jenkins 환경변수에 ANDROID_AAPT2_ROOT:/a/b/c/aapt2 등록 gradle.properties에 aapt2FromMavenOverride 속성 적용파일 자체에 프로퍼티를 추가할 수 있지만 어짜피 빌드시에만 사용될 것이므로 다음과 같은 stage를 추가했다.stage ('replace aapt2') { steps { script { dir('android') { def gradlePropsPath = \"${WORKSPACE}/gradle.properties\" sh \"\"\" # 기존 AAPT2 설정이 있으면 제거 sed -i '/android.aapt2FromMavenOverride/d' ${gradlePropsPath} # 새로운 AAPT2 설정을 개행과 함께 추가 echo -e \"\\\\n# Custom AAPT2 configuration for ARM\\\\nandroid.aapt2FromMavenOverride=${env.ANDROID_AAPT2_ROOT}\" &gt;&gt; ${gradlePropsPath} \"\"\" } } }}구글이 공식적으로 arm용 aapt2를 출시했으면 좋겠다." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 7. 마무리", "url": "/posts/terraform-aws-infra-7/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-06-22 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 7. 마무리이 시리즈를 작성하면서 작성한 스크립트들이 완벽하다고 생각하진 않는다. 모듈화를 더 하거나 조금더 아름답게 스크립트들을 작성할 수 있었다고 생각한다.또한 구성적으로도 미흡한 부분들이 존재한다. 엄격한 리소스 네이밍 규칙 Bastion Host 분리 RDS 프라이빗 서브넷으로 이전 ECR 수명 주기 정책 규칙 설정 상세한 Autoscaling 규칙 정의 배포, Scaling 이벤트 발생시 AWS SNS 트리거 컨테이너 환경변수 안전하게 관리 Github Actions 환경변수 안전하게 관리 그 외…Terraform 사용 측면에서는 직접 콘솔로 하나하나 생성했을때나 AWS CLI, SDK 등으로 리소스를 생성할때보다 오류가 덜 발생하고 생산성 측면에서 많이 발전했다고 생각한다. 또한 비교적 읽기 쉽기 때문에 다른 사람과도 쉽게 공유할 수 있을것 같다.무엇보다 Terraform의 가이드가 매우 잘되어 있는것 같다. 문제가 발생해도 가이드 문서를 참고하여 문제를 쉽게 해결할 수 있었다.실제 서비스에 사용하려면 세세하게 옵션을 설정해야 하겠지만 이 글을 읽으신 분들은 모두 잘 적용할 수 있을것이라 생각한다. 모든 코드는 이곳 에서 확인할 수 있다." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 6. 운영환경 (백엔드)", "url": "/posts/terraform-aws-infra-6/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-05-18 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 6. 운영환경 (백엔드)백엔드 운영환경을 구축한다.리소스1. RDSRDS를 생성한다.resource \"aws_security_group\" \"app-rds-sg\" { name = \"app-rds-sg\" description = \"security group for rds\" vpc_id = aws_vpc.vpc.id ingress { description = \"postgresSQL\" from_port = 5432 to_port = 5432 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"app-rds-sg\" }}resource \"aws_db_subnet_group\" \"app-rds-subnet-group\" { name = \"app-rds-subnet-group\" subnet_ids = aws_subnet.public[*].id tags = { name = \"app-rds-subnet-group\" }}resource \"aws_db_instance\" \"app-rds\" { db_name = \"app\" identifier = \"app-rds\" engine = \"postgres\" engine_version = \"15.8\" storage_type = \"gp2\" allocated_storage = 20 instance_class = \"db.t3.micro\" username = var.db-username # secret password = var.db-password # secret parameter_group_name = \"default.postgres15\" skip_final_snapshot = true publicly_accessible = true vpc_security_group_ids = [aws_security_group.app-rds-sg.id] db_subnet_group_name = aws_db_subnet_group.app-rds-subnet-group.name}rds가 생성되었다면 db 접속하여 유저, database 등 앱에 필요한 리소스들을 설정한다. :warning: 편의상 publicly_accessible, subnet group, security group을 퍼블릭으로 설정했다. 당연히 특정 bastion 어플리케이션환경 혹은 bastion host 에서만 접근 가능하게 해야한다.2. EFS로그를 저장할 EFS 저장소를 생성한다. 2049 포트를 열어야 하며 네트워크상 프라이빗 서브넷에 위치하도록 하였다.resource \"aws_security_group\" \"app-efs-sg\" { name = \"app-efs-sg\" description = \"security group for ecs service\" vpc_id = aws_vpc.vpc.id ingress { description = \"allow 2049\" from_port = 2049 to_port = 2049 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"app-efs-sg\" }}resource \"aws_efs_file_system\" \"app-efs\" { encrypted = true performance_mode = \"generalPurpose\" throughput_mode = \"bursting\" lifecycle_policy { transition_to_ia = \"AFTER_90_DAYS\" }}resource \"aws_efs_mount_target\" \"app-efs-target\" { count = \"${length(aws_subnet.private.*.id)}\" file_system_id = \"${aws_efs_file_system.app-efs.id}\" subnet_id = \"${element(aws_subnet.private.*.id, count.index)}\" security_groups = [\"${aws_security_group.app-efs-sg.id}\"]}3. ECSECS와 관련된 리소스들을 설정한다.1. ECRECR을 생성한다.resource \"aws_ecr_repository\" \"app-ecr\" { name = \"app-ecr\" image_tag_mutability = \"MUTABLE\" image_scanning_configuration { scan_on_push = false }}원할한 테스트를 위해 이 단계에서 ECR 리포지토리에 이미지를 푸쉬해두자. 나의 경우 nginx-latet, admin-latest, user-latest 3개의 이미지를 푸쉬해 두었다.2. Task Definitionresource \"aws_iam_role\" \"app-ecs-task-execution-role\" { name = \"ecsTaskExecutionRole\" assume_role_policy = jsonencode({ Version = \"2012-10-17\", Statement = [ { Action = \"sts:AssumeRole\", Effect = \"Allow\", Principal = { Service = \"ecs-tasks.amazonaws.com\" } } ] })}resource \"aws_iam_role_policy_attachment\" \"app-ecs-task-execution-role-policy-attachment\" { role = aws_iam_role.app-ecs-task-execution-role.name policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\"}태스크를 정의하기전 필요한 리소스들이다. AmazonECSTaskExecutionRolePolicy 정책이 필요하기 떄문에 app-ecs-task-execution-role를 정의하여 붙여 두었다.태스크 정의 설명전 잠시 서버 구성에 대해 이야기 하자면, 내가 구상한 구성은 각 SpringBoot 어플리케이션이 독립적인 컨테이너 형태로 존재하는것이 아닌 한 태스크 안에 nginx, admin, user 컨테이너가 존재하는 형태이다.트래픽 흐름으로 설명하자면 사용자 -&gt; CloudFront -&gt; ALB -&gt; (태스크) -&gt; Admin or User (N개의 앱, N개의 ECS 서비스) 가 아닌 사용자 -&gt; CloudFront -&gt; ALB -&gt; (태스크) -&gt; Nginx -&gt; Admin or User (N개의 앱, 1개의 ECS 서비스) 형태가 되는 것이다. 다른 시스템에는 (예를들어 회사에서 운영중인 서비스 등.) 독립적인 컨테이너 형태로 존재하는것이 더욱 안전하다고 생각한다.물론 그에따른 리소스 비용 증가나 관리의 복잡성이 따라올 수 있으니 각 서비스별로 적당하게 서버 구성을 하면 될 것 같다. 일단 이 포스팅에선 사용자 -&gt; CloudFront -&gt; ALB -&gt; (태스크) -&gt; Nginx -&gt; Admin or User 형태로 태스크를 정의한다.user nginx;worker_processes auto;events { worker_connections 1024;}http { sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; server { listen 80; server_name app-admin.keencho.com client_max_body_size 1G; access_log off; location /health-check { return 200; } location /api/ { proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; proxy_headers_hash_max_size 512; proxy_headers_hash_bucket_size 128; proxy_pass http://127.0.0.1:10000/api/; } } server { listen 80; server_name app-user.keencho.com client_max_body_size 1G; access_log off; location /api/ { proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; proxy_headers_hash_max_size 512; proxy_headers_hash_bucket_size 128; proxy_pass http://127.0.0.1:10010/api/; } }}위와같은 nginx 설정파일을 사용한다.variable \"nginx-container-name\" { default = \"nginx\"}resource \"aws_ecs_task_definition\" \"app-definition\" { family = \"app-definition\" requires_compatibilities = [\"FARGATE\"] cpu = 1024 memory = \"2048\" network_mode = \"awsvpc\" task_role_arn = aws_iam_role.app-ecs-task-execution-role.arn execution_role_arn = aws_iam_role.app-ecs-task-execution-role.arn runtime_platform { operating_system_family = \"LINUX\" cpu_architecture = \"ARM64\" } container_definitions = jsonencode([ { name = var.nginx-container-name image = \"${aws_ecr_repository.app-ecr.repository_url}:nginx-latest\" essential = true portMappings = [ { containerPort = 80 hostPort = 80 } ], healthCheck = { command = [\"CMD-SHELL\", \"curl -f http://localhost:80/health-check || exit 1\"] } }, { name = \"admin\" image = \"${aws_ecr_repository.app-ecr.repository_url}:admin-latest\" essential = true portMappings = [ { containerPort = 10000 hostPort = 10000 } ], environment = [ { name = \"spring.profiles.active\", value = \"prod\" }, { \"name\": \"logging.file.name\", \"value\": \"/app-efs/logs/prod/admin/$(curl -s $ECS_CONTAINER_METADATA_URI_V4/task | jq -r .TaskARN | cut -d '/' -f 3).log\" }, { \"name\": \"server.port\", \"value\": \"10000\" }, { \"name\": \"db_url\", \"value\": \"jdbc:postgresql://${aws_db_instance.app-rds.endpoint}/${aws_db_instance.app-rds.db_name}\" }, { \"name\": \"db_username\", \"value\": var.db-username }, { \"name\": \"db_password\", \"value\": var.db-password }, ], mountPoints: [ { sourceVolume: \"app-efs\", containerPath: \"/app-efs\", readOnly: false } ], healthCheck = { command = [\"CMD-SHELL\", \"curl -f http://localhost:10000/api/health-check || exit 1\"] } }, { name = \"user\" image = \"${aws_ecr_repository.app-ecr.repository_url}:user-latest\" essential = true portMappings = [ { containerPort = 10010 hostPort = 10010 } ], environment = [ { name = \"spring.profiles.active\", value = \"prod\" }, { \"name\": \"logging.file.name\", \"value\": \"/app-efs/logs/prod/user/$(curl -s $ECS_CONTAINER_METADATA_URI_V4/task | jq -r .TaskARN | cut -d '/' -f 3).log\" }, { \"name\": \"server.port\", \"value\": \"10010\" }, { \"name\": \"db_url\", \"value\": \"jdbc:postgresql://${aws_db_instance.app-rds.endpoint}/${aws_db_instance.app-rds.db_name}\" }, { \"name\": \"db_username\", \"value\": var.db-username }, { \"name\": \"db_password\", \"value\": var.db-password }, ], mountPoints: [ { sourceVolume: \"app-efs\", containerPath: \"/app-efs\", readOnly: false } ], healthCheck = { command = [\"CMD-SHELL\", \"curl -f http://localhost:10010/api/health-check || exit 1\"] } } ]) volume { name = \"app-efs\" efs_volume_configuration { file_system_id = aws_efs_file_system.app-efs.id root_directory = \"/\" } } lifecycle { ignore_changes = [container_definitions] }} 시작유형: Fargate OS, 아키텍처: Linux/ARM64 CPU: 2vCPU 메모리: 2 GB 볼륨: 앞에서 생성한 EFS 유형의 스토리지를 마운트, 루트 디렉토리는 / 컨테이너 정의 nginx 포트: 80 상태확인: /health-check admin / user 포트: 10000 / 10010 상태확인: /api/health-check EFS 볼륨 마운트 환경변수 SpringBoot 프로필 로깅파일 경로 지정: EFS 경로 + … + 현재 컨테이너가 속한 태스크의 전체 Amazon 리소스 이름 (ARN) db 설정값들 (여기서는 그냥 rds의 endpoint와 db_name을 끌어다 썼다. 실제 운영환경 이라면 S3에 환경 파일을 저장해두고 사용하는게 안전할것 같다.) :warning: ignore_changes 에 container_definitions 를 추가한다. 아무 수정 없어도 항상 terraform이 ‘force replacement’ 를 시도하기 때문이다. 아마 ‘container_definitions’가 JSON 형식으로 정의되기 때문에 필드 순서나 미세한 차이로 인해 변경이 감지될 수 있어서 그런것 같다.2. Cluster &amp; Service클러스터를 생성한다.resource \"aws_ecs_cluster\" \"app-cluster\" { name = \"app-cluster\"}서비스 생성전 로드밸런서와 연결할 타겟 그룹을 정의한다. 추후 CodeDeploy로 Blue / Green 배포를 진행할 것이므로 타겟 그룹을 2개 정의한다. 로드 밸런서에는 1번 대상그룹만 연결한다.resource \"aws_lb_target_group\" \"app-ecs-service-tg1\" { name = \"app-ecs-service-tg1\" port = 80 protocol = \"HTTP\" target_type = \"ip\" vpc_id = aws_vpc.vpc.id health_check { path = var.alb-health-check-path port = \"traffic-port\" }}resource \"aws_lb_target_group\" \"app-ecs-service-tg2\" { name = \"app-ecs-service-tg2\" port = 80 protocol = \"HTTP\" target_type = \"ip\" vpc_id = aws_vpc.vpc.id health_check { path = var.alb-health-check-path port = \"traffic-port\" }}resource \"aws_lb_listener_rule\" \"app-alb-ecs-service-rule\" { listener_arn = aws_lb_listener.app-alb-listener-https.arn priority = 2 action { type = \"forward\" target_group_arn = aws_lb_target_group.app-ecs-service-tg1.arn } condition { host_header { values = [\"app-admin.keencho.com\", \"app-user.keencho.com\"] } }}다음으로는 서비스를 정의한다.resource \"aws_security_group\" \"app-ecs-service-sg\" { name = \"app-ecs-service-sg\" description = \"security group for ecs service\" vpc_id = aws_vpc.vpc.id ingress { description = \"alb traffic\" from_port = 0 to_port = 65535 protocol = \"tcp\" security_groups = [aws_security_group.app-alb-sg.id] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"app-ecs-service-sg\" }}resource \"aws_ecs_service\" \"app-ecs-service\" { name = \"app-ecs-service\" cluster = aws_ecs_cluster.app-cluster.id task_definition = aws_ecs_task_definition.app-definition.arn desired_count = 1 launch_type = \"FARGATE\" propagate_tags = \"SERVICE\" health_check_grace_period_seconds = 60 network_configuration { subnets = aws_subnet.private[*].id security_groups = [aws_security_group.app-ecs-service-sg.id] assign_public_ip = false } deployment_controller { type = \"CODE_DEPLOY\" } load_balancer { target_group_arn = aws_lb_target_group.app-ecs-service-tg1.arn container_name = var.nginx-container-name container_port = 80 } lifecycle { ignore_changes = [desired_count] }} 시작유형: Fargate 태그전파: 서비스 기준 상태확인 유휴기간: 60초 배포 옵션: CodeDeploy 로드밸런서: 타겟그룹1에 등록, 80포트로 트래픽 라우팅 네트워크 구성: 프라이빗 서브넷에 존재, 로드밸런서 로부터 오는 트래픽만 허용, 나머지 금지여기까지 오면 서비스와 태스크가 생성된다. 콘솔에서 상태를 확인해보자. :warning: 배포 옵션을 CodeDeploy로 지정했기 때문에 문제가 발생해도 서비스 업데이트를 할 수 없다. (현 시점에는 CodeDeploy와 연결되어 있지 않기 때문) 문제를 해결하려면 서비스를 지우고 다시 생성하는 수 밖에 없다.3. AutoScalingresource \"aws_iam_role\" \"app-ecs-autoscale\" { name = \"app-ecs-autoscale-iam-role\" assume_role_policy = jsonencode({ Version = \"2012-10-17\", Statement = [ { Sid = \"Autoscaling\" Action = \"sts:AssumeRole\", Effect = \"Allow\", Principal = { Service = \"application-autoscaling.amazonaws.com\" } } ] })}resource \"aws_iam_role_policy_attachment\" \"app-ecs-autoscale\" { role = aws_iam_role.app-ecs-autoscale.name policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceAutoscaleRole\"}resource \"aws_appautoscaling_target\" \"app-ecs-target\" { min_capacity = 1 max_capacity = 4 resource_id = \"service/${aws_ecs_cluster.app-cluster.name}/${aws_ecs_service.app-ecs-service.name}\" role_arn = aws_iam_role.app-ecs-task-execution-role.arn scalable_dimension = \"ecs:service:DesiredCount\" service_namespace = \"ecs\" depends_on = [ aws_ecs_service.app-ecs-service ]}resource \"aws_appautoscaling_policy\" \"app-ecs-policy-scale-out\" { name = \"scale-out\" policy_type = \"StepScaling\" resource_id = aws_appautoscaling_target.app-ecs-target.resource_id scalable_dimension = aws_appautoscaling_target.app-ecs-target.scalable_dimension service_namespace = aws_appautoscaling_target.app-ecs-target.service_namespace step_scaling_policy_configuration { adjustment_type = \"PercentChangeInCapacity\" cooldown = 1 metric_aggregation_type = \"Average\" step_adjustment { metric_interval_lower_bound = 0 scaling_adjustment = 100 } }}resource \"aws_cloudwatch_metric_alarm\" \"app-ecs-cpu-high\" { alarm_name = \"app-ecs-cpu-high\" comparison_operator = \"GreaterThanOrEqualToThreshold\" evaluation_periods = \"3\" metric_name = \"CPUUtilization\" namespace = \"AWS/ECS\" period = \"60\" statistic = \"Average\" threshold = \"70\" dimensions = { ClusterName = aws_ecs_cluster.app-cluster.name ServiceName = aws_ecs_service.app-ecs-service.name } alarm_actions = [aws_appautoscaling_policy.app-ecs-policy-scale-out.arn]}최소용량을 1, 최대용량을 4 로 지정하고 CPU에 따라 태스크를 scale out 하도록 구성하였다. 콘솔에서 확인해보자.4. CloudFront 수정앞서 운영환경 - 프론트를 구성할때 CloudFront로 들어오는 모든 트래픽은 S3로 전달되게 구성하였다. 맨 처음 개요에서 설명했듯 /api/** 경로로 시작하는 요청은 Application Load Balancer로 전달되도록 수정해야 한다.resource \"aws_cloudfront_distribution\" \"admin-distribution\" { origin { domain_name = aws_s3_bucket.app-prod-react.bucket_regional_domain_name origin_id = aws_s3_bucket.app-prod-react.id origin_access_control_id = aws_cloudfront_origin_access_control.admin-front.id origin_path = \"/admin\" } # 추가 origin { domain_name = aws_lb.app-alb.dns_name origin_id = aws_lb.app-alb.id custom_origin_config { http_port = 80 https_port = 443 origin_protocol_policy = \"https-only\" origin_ssl_protocols = [\"TLSv1.2\"] origin_keepalive_timeout = 5 origin_read_timeout = 30 } } enabled = true default_root_object = \"index.html\" comment = \"admin distribution\" aliases = [\"app-admin.keencho.com\"] default_cache_behavior { allowed_methods = [\"GET\", \"HEAD\"] cached_methods = [\"GET\", \"HEAD\"] target_origin_id = aws_s3_bucket.app-prod-react.id viewer_protocol_policy = \"redirect-to-https\" min_ttl = 0 default_ttl = 3600 max_ttl = 86400 forwarded_values { query_string = false cookies { forward = \"none\" } } } # 추가 ordered_cache_behavior { path_pattern = \"/api/*\" allowed_methods = [\"GET\", \"HEAD\", \"OPTIONS\", \"PUT\", \"POST\", \"PATCH\", \"DELETE\"] cached_methods = [\"GET\", \"HEAD\"] target_origin_id = aws_lb.app-alb.id viewer_protocol_policy = \"redirect-to-https\" default_ttl = 0 max_ttl = 0 min_ttl = 0 forwarded_values { query_string = true headers = [\"*\"] cookies { forward = \"all\" } } } price_class = \"PriceClass_100\" restrictions { geo_restriction { restriction_type = \"whitelist\" locations = [\"KR\"] } } viewer_certificate { acm_certificate_arn = aws_acm_certificate.ssl-certificate-virginia.arn ssl_support_method = \"sni-only\" minimum_protocol_version = \"TLSv1.2_2021\" } custom_error_response { error_code = 403 error_caching_min_ttl = 10 response_page_path = \"/index.html\" response_code = 200 }}글에는 관리자 수정본만 작성한다. Application Load Balancer 원본을 추가하였고 /api/* 경로 패턴은 로드밸런서로 라우팅 될 수 있도록 동작을 수정하였다.5. CodeDeployBlue / Green 배포를 위한 CodeDeploy 관련 리소스를 생성한다.resource \"aws_codedeploy_app\" \"app-deploy-app\" { compute_platform = \"ECS\" name = \"app-deploy\"}data \"aws_iam_policy_document\" \"app-deploy-assume-role\" { statement { effect = \"Allow\" principals { type = \"Service\" identifiers = [\"codedeploy.amazonaws.com\"] } actions = [\"sts:AssumeRole\"] }}resource \"aws_iam_role\" \"app-deploy-role\" { name = \"app-deploy-role\" assume_role_policy = data.aws_iam_policy_document.app-deploy-assume-role.json}resource \"aws_iam_role_policy_attachment\" \"app-AWSCodeDeployRole\" { policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole\" role = aws_iam_role.app-deploy-role.name}resource \"aws_iam_role_policy_attachment\" \"app-AWSCodeDeployRoleForECS\" { policy_arn = \"arn:aws:iam::aws:policy/AWSCodeDeployRoleForECS\" role = aws_iam_role.app-deploy-role.name}resource \"aws_codedeploy_deployment_group\" \"app-deploy-group\" { app_name = aws_codedeploy_app.app-deploy-app.name deployment_config_name = \"CodeDeployDefault.ECSAllAtOnce\" deployment_group_name = \"app-deploy-group\" service_role_arn = aws_iam_role.app-deploy-role.arn auto_rollback_configuration { enabled = false } blue_green_deployment_config { deployment_ready_option { action_on_timeout = \"CONTINUE_DEPLOYMENT\" } terminate_blue_instances_on_deployment_success { action = \"TERMINATE\" termination_wait_time_in_minutes = 1 } } deployment_style { deployment_option = \"WITH_TRAFFIC_CONTROL\" deployment_type = \"BLUE_GREEN\" } ecs_service { cluster_name = aws_ecs_cluster.app-cluster.name service_name = aws_ecs_service.app-ecs-service.name } load_balancer_info { target_group_pair_info { prod_traffic_route { listener_arns = [aws_lb_listener.app-alb-listener-https.arn] } target_group { name = aws_lb_target_group.app-ecs-service-tg1.name } target_group { name = aws_lb_target_group.app-ecs-service-tg2.name } } }} 어플리케이션 생성 CodeDeploy 관련 IAM Role 생성 배포그룹 생성 로드밸런서 지정 (Blue / Green 배포를 위해 앞에서 생성한 타겟그룹 1, 2 지정)6. Github Actions 배포 스크립트 작성내가 사용한 Dockerfile이다.FROM amazoncorretto:21# 타임존 세팅ENV TZ=\"Asia/Seoul\"ARG JAR_PATH# 워크 디렉토리 세팅WORKDIR /app-admin/# 빌드된 jar 파일 복사COPY $JAR_PATH /app-admin/app.jarENTRYPOINT [\"java\",\"-jar\",\"app.jar\"]FROM nginx:latestCOPY nginx.conf /etc/nginx/nginx.confEXPOSE 80다음은 배포 스크립트다.name: Deploy Spring Boot Application to ECS using CodeDeploy Blue / Green Deploymenton: workflow_dispatch:jobs: deploy: name: Deploy runs-on: ubuntu-latest steps: - name: Check out the repository uses: actions/checkout@v4 - name: Setup JDK 21 uses: actions/setup-java@v4 with: java-version: '21' distribution: 'corretto' - name: Setup Gradle uses: gradle/actions/setup-gradle@v3 - name: Build Gradle working-directory: spring-boot run: | chmod +x ./gradlew ./gradlew bootjar --project-dir ./app-admin ./gradlew bootjar --project-dir ./app-user - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ${{ secrets.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v2 - name: Build, tag and push image to Amazon ECR working-directory: spring-boot env: ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }} run: | docker build --build-arg JAR_PATH=app-admin/build/libs/*.jar --platform=linux/arm64 -t $ECR_REGISTRY/app-ecr:admin-latest -f app-admin/Dockerfile . docker build --build-arg JAR_PATH=app-user/build/libs/*.jar --platform=linux/arm64 -t $ECR_REGISTRY/app-ecr:user-latest -f app-user/Dockerfile . docker build --platform=linux/arm64 -t $ECR_REGISTRY/app-ecr:nginx-latest -f nginx.Dockerfile . docker push $ECR_REGISTRY/app-ecr:admin-latest docker push $ECR_REGISTRY/app-ecr:user-latest docker push $ECR_REGISTRY/app-ecr:nginx-latest - name: CodeDeploy Blue / Green Deployment working-directory: spring-boot run: | APPLICATION_NAME=\"${{ secrets.AWS_CODEDEPLOY_APPLICATION_NAME }}\" DEPLOYMENT_GROUP=\"${{ secrets.AWS_CODEDEPLOY_DEPLOYMENT_GROUP_NAME }}\" REGION=\"${{ secrets.AWS_REGION }}\" REVISION_JSON='{ \"version\": 1, \"Resources\": [ { \"TargetService\": { \"Type\": \"AWS::ECS::Service\", \"Properties\": { \"TaskDefinition\": \"${{ secrets.AWS_CODEDEPLOY_TASK_DEFINITION }}\", \"LoadBalancerInfo\": { \"ContainerName\": \"nginx\", \"ContainerPort\": 80 }, \"PlatformVersion\": \"LATEST\" } } } ] }' # Create a new deployment aws deploy create-deployment \\ --cli-input-json \"{\\\"applicationName\\\":\\\"$APPLICATION_NAME\\\",\\\"deploymentGroupName\\\":\\\"$DEPLOYMENT_GROUP\\\",\\\"revision\\\":{\\\"revisionType\\\":\\\"AppSpecContent\\\",\\\"appSpecContent\\\":{\\\"content\\\":\\\"$(echo $REVISION_JSON | sed 's/\"/\\\\\"/g')\\\"}}}\" \\ --region $REGION repository checkout setup jdk21 setup gradle build gradle (bootjar) configure aws credentials login ecr build, tag and push image to ecr OS, 아키텍처를 linux/arm64로 지정 CodeDeploy 배포 생성 어플리케이션 지정 배포 그룹 지정 리전 지정 태스크 정의 지정 로드밸런서가 트래픽을 nginx(80 포트) 로 라우팅하도록 지정 위와같은 순서로 진행된다. 두근거리는 마음으로 Run workflow 버튼을 눌러보자." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 5. 운영환경 (프론트)", "url": "/posts/terraform-aws-infra-5/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-04-13 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 5. 운영환경 (프론트)프론트 운영환경을 구축한다.리소스1. S3 버킷먼저 S3 버킷을 생성한다.resource \"aws_s3_bucket\" \"app-prod-react\" { bucket = \"app-prod-react\" tags = { Name = \"app-prod-react\" }}resource \"aws_s3_bucket_ownership_controls\" \"app-prod-react-ownership\" { bucket = aws_s3_bucket.app-prod-react.id rule { object_ownership = \"BucketOwnerPreferred\" }}resource \"aws_s3_bucket_acl\" \"app-prod-react-acl\" { depends_on = [aws_s3_bucket_ownership_controls.app-prod-react-ownership] bucket = aws_s3_bucket.app-prod-react.id acl = \"private\"}resource \"aws_s3_object\" \"app-prod-react-admin\" { bucket = aws_s3_bucket.app-prod-react.id content_type = \"application/x-directory\" key = \"admin/\"}resource \"aws_s3_object\" \"app-prod-react-user\" { bucket = aws_s3_bucket.app-prod-react.id content_type = \"application/x-directory\" key = \"user/\"}버킷과 엑세스 제어 목록, 관리자와 유저 어플리케이션이 배포될 폴더를 정의했다.관리자와 유저 어플리케이션을 업로드해 둘 것이다. 이 또한 terraform으로 진행이 가능하지만, 어짜피 앱 배포는 추후 github action을 통해 빌드하고 s3에 업로드할 것이므로 지금은 콘솔 환경에서 직접 업로드 한다.이 상태에서 버킷을 퍼블릭 액세스로 열어버리고 주소로 접근하면 웹 페이지가 뜬다. 그러나 앞단에 CloudFront를 두어 그곳에서 s3로 라우팅할 것이기 때문에 넘어간다.2. CloudFrontresource \"aws_cloudfront_origin_access_control\" \"admin-front\" { name = \"admin-front\" description = \"admin front\" origin_access_control_origin_type = \"s3\" signing_behavior = \"always\" signing_protocol = \"sigv4\"}resource \"aws_cloudfront_distribution\" \"admin-distribution\" { origin { domain_name = aws_s3_bucket.app-prod-react.bucket_regional_domain_name origin_id = aws_s3_bucket.app-prod-react.id origin_access_control_id = aws_cloudfront_origin_access_control.admin-front.id origin_path = \"/admin\" } enabled = true default_root_object = \"index.html\" comment = \"admin distribution\" aliases = [\"app-admin.keencho.com\"] default_cache_behavior { allowed_methods = [\"GET\", \"POST\", \"PUT\", \"OPTIONS\", \"DELETE\", \"PATCH\", \"HEAD\"] cached_methods = [\"GET\", \"HEAD\"] target_origin_id = aws_s3_bucket.app-prod-react.id forwarded_values { query_string = false cookies { forward = \"none\" } } viewer_protocol_policy = \"redirect-to-https\" min_ttl = 0 default_ttl = 3600 max_ttl = 86400 } price_class = \"PriceClass_100\" restrictions { geo_restriction { restriction_type = \"whitelist\" locations = [\"KR\"] } } viewer_certificate { acm_certificate_arn = aws_acm_certificate.ssl-certificate-virginia.arn ssl_support_method = \"sni-only\" minimum_protocol_version = \"TLSv1.2_2021\" } custom_error_response { error_code = 403 error_caching_min_ttl = 10 response_page_path = \"/index.html\" response_code = 200 }}관리자 배포를 생성했다. 대체 도메인을 지정할 경우 SSL 인증서는 필수인데, 앞서 말했던 것처럼 여기엔 us-east-1 리전에 존재하는 인증서만 지정할 수 있다.현 시점엔 라우팅되는 모든 트래픽이 s3 버킷의 /admin 폴더로 전송된다.resource \"aws_route53_record\" \"app-admin\" { zone_id = aws_route53_zone.keencho.id name = \"app-admin.keencho.com\" type = \"A\" alias { name = aws_cloudfront_distribution.admin-distribution.domain_name zone_id = aws_cloudfront_distribution.admin-distribution.hosted_zone_id evaluate_target_health = true }}Route 53 레코드를 생성하여 app-admin.keencho.com 으로 들어오는 요청이 CloudFront로 라우팅 되도록 하였다. 사용자(user) 배포도 이름만 바꾸어 생성한다.S3 버킷 정책 변경현재 S3 버킷 정책은 아래 이미지와 같이 모든 퍼블릭 엑세스가 차단되어 있을 것이다.CloudFront 에서 온 요청은 허용하는 정책을 적용한다.resource \"aws_s3_bucket_policy\" \"allow-from-cloudfront-policy\" { bucket = aws_s3_bucket.app-prod-react.id policy = jsonencode({ Version = \"2012-10-17\", Statement = [ { Sid = \"AllowCloudFrontServicePrincipal\", Action = \"s3:GetObject\", Effect = \"Allow\", Resource = \"${aws_s3_bucket.app-prod-react.arn}/*\", Principal = { \"Service\": \"cloudfront.amazonaws.com\" }, Condition = { StringEquals = { \"AWS:SourceArn\": [ aws_cloudfront_distribution.admin-distribution.arn, aws_cloudfront_distribution.user-distribution.arn, ] } } } ] })}도메인으로 접속했을 때 의도한대로 페이지가 동작하는지 확인하자.3. Github Actions 배포 스크립트 작성react 프로젝트 구조이다. root 폴더에서 npm install 명령어를 수행한 후 각 admin, user 폴더에서 빌드를 수행해야 한다.다음은 Github Actions 배포 스크립트이다. 앞서 설명한대로 install 과 build를 수행한 후 S3에 업로드한다. 이때 기존 파일은 모두 삭제한다. 물론 실제 운영환경에선 따로 백업해두는게 안전하다.그 후 CloudFront Invalidate 를 통해 캐싱된 파일을 무효화 하여 사용자가 배포된 파일을 확인할 수 있게 한다.name: Deploy Admin and User to S3on: workflow_dispatch:jobs: deploy: name: Deploy runs-on: ubuntu-latest steps: - name: Check out the repository uses: actions/checkout@v4 - name: Set up Node.js uses: actions/setup-node@v3 with: node-version: '20' - name: Install dependencies for Root Project working-directory: react run: npm install - name: Build Admin working-directory: react/app/admin run: npm run build:production - name: Build User working-directory: react/app/user run: npm run build:production - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ${{ secrets.AWS_REGION }} - name: Remove Exist &amp; Upload New to S3 working-directory: react run: | aws s3 rm s3://${{ secrets.S3_BUCKET_NAME }}/admin --recursive aws s3 rm s3://${{ secrets.S3_BUCKET_NAME }}/user --recursive aws s3 cp app/admin/dist s3://${{ secrets.S3_BUCKET_NAME }}/admin --recursive aws s3 cp app/user/dist s3://${{ secrets.S3_BUCKET_NAME }}/user --recursive - name: Invalidate CloudFront run: | aws cloudfront create-invalidation --distribution-id ${{ secrets.AWS_CLOUDFRONT_DISTRIBUTION_ID_ADMIN }} --paths \"/*\" aws cloudfront create-invalidation --distribution-id ${{ secrets.AWS_CLOUDFRONT_DISTRIBUTION_ID_USER }} --paths \"/*\"" }, { "title": "Athena로 ALB Access Log 분석하기", "url": "/posts/aws-alb-log-athena/", "categories": "AWS", "tags": "AWS", "date": "2024-03-23 08:12:00 +0900", "snippet": "Athena로 ALB Access Log 분석하기Appliation Load Balancer에는 액세스 로그를 저장할 수 있는 기능이 있다. 이를 활성화 하면 S3에 모든 요청 로그를 저장하게 되는데, 동일한 날짜의 요청이라도 한 파일에 저장되는 것이 아니라 굉장히 많은 .gz 형식의 압축파일로 쪼개져 저장된다.아름답다/order 라는 경로로 들어오는 요청을 검색해 본다고 가정해보자. 일단 파일을 찾기 위해 마지막 수정 필드를 확인해야 한다. 어찌저찌 파일을 찾았다면 파일을 다운받고 압축풀고 잘 읽어지지도 않는 파일을 찾아야 한다. 시간대가 좀 차이나는 요청을 비교해야 한다면 수많은 파일을 띄워놓고 하나하나 비교해야 한다.AWS 에서 제공하는 Amazone Athena 서비스를 이용하여 SQL문으로 원하는 데이터를 쉽게 추출할 수 있다.Athena는 표준 SQL을 사용해 S3에 있는 데이터를 직접 간편하게 분석할 수 있는 대화형 쿼리 서비스이다. Athena는 서버리스이므로 인프라 설정이나 관리가 불필요하며, 실행하는 쿼리 또는 쿼리에 필요한 컴퓨팅을 기준으로 요금을 지불할 수 있다. 스캔한 데이터 1TB 당 $5의 요금이 부과된다. 예를들어 Athena를 사용해 100MB 의 데이터를 스캔하는 쿼리를 매일 100개씩 실행한다면 월별 $1.45의 요금이 부과된다.엄청난 크기의 데이터를 다룬다면 비용이 부담되겠지만, 적당한 트래픽이 발생하는 서비스의 ALB 로그 분석용으로만 사용한다면 비용 압박은 거의 없다고 생각된다. (이런거 아끼는 회사는…ㅎ) 파티셔닝 관리를 위한 CREATE, ALTER 혹은 DROP TABLE 문과 같은 DDL문 또는 실패한 쿼리에는 요금이 부과되지 않습니다. 취소한 쿼리는 쿼리를 취소할 시점에 스캔된 총 데이터 양에 따라 요금이 청구됩니다.Athena 설정쿼리를 날리기 앞서 필요한 설정들을 진행해 보자.일단 쿼리 결과가 저장될 별도의 S3 버킷이 필요하다. 버킷을 생성하고 Amazon Athena &gt; 쿼리 편집기 &gt; 설정 &gt; 관리로 이동한다.Location of query result 항목에 방금 생성한 버킷을 지정한다.데이터베이스 &amp; 테이블 생성설정이 완료되었다면 편집기로 돌아와 데이터베이스를 생성한다.CREATE DATABASE alb_log_db;데이터베이스를 생성하였다면 좌측 데이터 - 데이터베이스 항목에서 방금 생성한 데이터베이스를 선택한다.옵션1. 일반 테이블 생성CREATE EXTERNAL TABLE IF NOT EXISTS alb_access_logs ( type string, time string, elb string, client_ip string, client_port int, target_ip string, target_port int, request_processing_time double, target_processing_time double, response_processing_time double, elb_status_code int, target_status_code string, received_bytes bigint, sent_bytes bigint, request_verb string, request_url string, request_proto string, user_agent string, ssl_cipher string, ssl_protocol string, target_group_arn string, trace_id string, domain_name string, chosen_cert_arn string, matched_rule_priority string, request_creation_time string, actions_executed string, redirect_url string, lambda_error_reason string, target_port_list string, target_status_code_list string, classification string, classification_reason string, conn_trace_id string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = '1', 'input.regex' = '([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\\"([^ ]*) (.*) (- |[^ ]*)\\\" \\\"([^\\\"]*)\\\" ([A-Z0-9-_]+) ([A-Za-z0-9.-]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" ([-.0-9]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^ ]*)\\\" \\\"([^\\s]+?)\\\" \\\"([^\\s]+)\\\" \\\"([^ ]*)\\\" \\\"([^ ]*)\\\" ?([^ ]*)?( .*)?') LOCATION 's3://DOC-EXAMPLE-BUCKET/access-log-folder-path/'위 SQL 문을 사용해 테이블을 생성한다. LOCATION 항목의 경로를 ALB 로그가 저장되어 있는 경로로 변경하면 된다.옵션2. partition projection 기능을 활용해 테이블 생성 (추천)ALB 로그는 정해진 체계로 생성이 되기 때문에 Athena partition projection 기능을 활용해 쿼리 런타임을 줄이고 파티션 관리를 자동화 할수 있다. 파티션 프로젝션은 새로운 데이터를 추가할 때 자동으로 새로운 파티션을 추가하기 때문에 별도의 ALTER TABLE ADD PARTITION문을 사용하여 파티션을 수동으로 추가할 필요가 없다. 이를 통해 Athena를 최적화 (쿼리 실행시간 단축, 파티션 관리 자동화, 스캔 데이터 절약, 비용 절감) 할 수 있다.partition projection 에 대해서는 공식문서 를 확인하면 상세하게 알 수 있다.ALB 로그 파일은 설명 에 따르면 다음과 같은 형식으로 생성된다.bucket[/prefix]/AWSLogs/aws-account-id/elasticloadbalancing/region/yyyy/mm/dd/aws-account-id_elasticloadbalancing_region_app.load-balancer-id_end-time_ip-address_random-string.log.gzyyyy/mm/dd 형식으로 폴더가 생성됨을 확인할 수 있다. 따라서 year, month, day를 파티션 컬럼으로 지정하도록 하겠다.CREATE EXTERNAL TABLE IF NOT EXISTS sys_log ( type string, time string, elb string, client_ip string, client_port int, target_ip string, target_port int, request_processing_time double, target_processing_time double, response_processing_time double, elb_status_code int, target_status_code string, received_bytes bigint, sent_bytes bigint, request_verb string, request_url string, request_proto string, user_agent string, ssl_cipher string, ssl_protocol string, target_group_arn string, trace_id string, domain_name string, chosen_cert_arn string, matched_rule_priority string, request_creation_time string, actions_executed string, redirect_url string, lambda_error_reason string, target_port_list string, target_status_code_list string, classification string, classification_reason string, conn_trace_id string ) PARTITIONED BY ( year integer, month integer, day integer ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = '1', 'input.regex' = '([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\\"([^ ]*) (.*) (- |[^ ]*)\\\" \\\"([^\\\"]*)\\\" ([A-Z0-9-_]+) ([A-Za-z0-9.-]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" ([-.0-9]*) ([^ ]*) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\" \\\"([^ ]*)\\\" \\\"([^\\s]+?)\\\" \\\"([^\\s]+)\\\" \\\"([^ ]*)\\\" \\\"([^ ]*)\\\" ?([^ ]*)?( .*)?') LOCATION 's3://DOC-EXAMPLE-BUCKET/AWSLogs/&lt;ACCOUNT-NUMBER&gt;/elasticloadbalancing/&lt;REGION&gt;/' TBLPROPERTIES ( \"projection.enabled\" = \"true\", \"projection.year.type\" = \"integer\", \"projection.year.range\" = \"2024,2024\", \"projection.year.digits\" = \"4\", \"projection.month.type\" = \"integer\", \"projection.month.range\" = \"01,12\", \"projection.month.digits\" = \"2\", \"projection.day.type\" = \"integer\", \"projection.day.range\" = \"01,31\", \"projection.day.digits\" = \"2\", \"storage.location.template\" = \"s3://DOC-EXAMPLE-BUCKET/AWSLogs/&lt;ACCOUNT-NUMBER&gt;/elasticloadbalancing/&lt;REGION&gt;/${year}/${month}/${day}\" )LOCATION 항목과 storage.locationon.template 항목의 경로를 ALB 로그 버킷 경로로 변경하면 된다. 테이블을 생성하였다면 우측 테이블 섹션에서 year, month, day 컬럼을 찾아보자. 아래 캡쳐된 이미지와 같이 파티션된 컬럼이라고 표기되어 있을 것이다. 테이블은 생성되었는데 데이터가 조회되지 않는다면 이 문서의 Syntax 문단을 확인해서 누락된 필드가 있는지 확인해 보세요. 예제의 경우 이 포스팅을 작성하는 시점의 필드들만을 포함하고 있으므로 추후 필드가 추가된다면 정상동작하지 않을 수 있습니다.쿼리 실행테이블이 오류없이 생성되었다면 필요한 데이터를 분석하는 일만 남았다. 쿼리 작성후 ctrl + enter 로 쿼리를 실행할 수 있으며 하단 쿼리 결과 에서 결과를 확인할 수 있다.예제날짜별 전체 요청 갯수 countSELECT cast(year as VARCHAR) || cast(month as VARCHAR) || cast(day as VARCHAR) as date, count(*) as countFROM sys_logGROUP BY cast(year as VARCHAR) || cast(month as VARCHAR) || cast(day as VARCHAR)2024년 3월 23일 11시 ~ 18시 30분 사이 요청중 처리 시간이 10초를 초과하는 요청을 처리시간 내림차순으로 정렬SELECT *FROM sys_logWHERE ( parse_datetime(time, 'yyyy-MM-dd''T''HH:mm:ss.SSSSSS''Z') BETWEEN parse_datetime('2024-03-23 02:00:00', 'yyyy-MM-dd HH:mm:ss') AND parse_datetime('2024-03-23 09:30:00', 'yyyy-MM-dd HH:mm:ss') )AND target_processing_time &gt; 10ORDER BY target_processing_time DESC3월 10일 요청중 네이버 웨일 브라우저에서 요청한 요청 갯수 countSELECT count(*)FROM sys_logWHERE month = 03 AND day = 10 AND user_agent like '%Whale%'" }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 4. 테스트 환경", "url": "/posts/terraform-aws-infra-4/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-02-24 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 4. 테스트 환경이번 포스팅에서는 테스트 환경을 구성한다. 테스트 환경은 운영환경과는 다르게 백엔드, 프론트엔드, db가 모두 1개의 인스턴스에서 돌아가게 구성한다.당연히 실제 비즈니스 서비스의 경우 실제 운영환경과 동일하게 환경을 구성해야 겠지만 이 시리즈에서는 그렇게까지 하진 않는다. ec2, key-pair등과 같은 리소스도 Terraform으로 생성해보기 위함이다.테스트 환경의 트래픽은 Route53 -&gt; Application Load Balancer -&gt; EC2 로 전송된다.테스트 환경은 무중단 배포를 고려하지 않는다.리소스1. Application Load Balancer먼저 로드밸런서를 생성한다. 로드밸런서는 퍼블릭 서브넷을 가용영역으로 두어야 한다. 또한 80 포트와 443 포트를 개방하도록 하겠다.resource \"aws_lb\" \"app-alb\" { name = \"app-alb\" internal = false load_balancer_type = \"application\" security_groups = [aws_security_group.app-alb-sg.id] subnets = [for subnet in aws_subnet.public : subnet.id] enable_deletion_protection = true}resource \"aws_security_group\" \"app-alb-sg\" { name = \"app-alb-sg\" description = \"security group for application load balancer\" vpc_id = \"${aws_vpc.vpc.id}\" ingress { description = \"http\" from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } ingress { description = \"https\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] ipv6_cidr_blocks = [\"::/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"app-alb-sg\" }}2. EC2다음으로 EC2를 생성한다. 나는 가난한 개발자 이므로 프리티어를 사용할 것이다.2.1 Key Pair키페어를 생성한다. 이 글 을 참고해 생성하였다. 다 만들었다면 output을 확인하여 파일을 생성해 추후 ssh 접속시 사용하면 된다.resource \"tls_private_key\" \"pk\" { algorithm = \"RSA\" rsa_bits = 4096}resource \"aws_key_pair\" \"app-test-key-pair\" { key_name = \"app-test-key-pair\" public_key = tls_private_key.pk.public_key_openssh}output \"private_key\" { value = tls_private_key.pk.private_key_pem sensitive = true}2.2 Security Group &amp; Elastic IP다음으론 보안그룹과 고정 IP를 생성한다. 원래대로라면 개발자 pc의 ip만 ssh 접속 허용해야겠으나 편의상 모든 트래픽을 허용하도록 하겠다. 이 인스턴스로 들어오는 http 포트는 80 포트를 사용할 것이며 로드밸런서 에서 라우팅되는 요청만 허용할 것이다. 따라서 ingress 부분의 security groups 값을 로드밸런서의 보안그룹 id로 지정한다.resource \"aws_security_group\" \"app-test-sg\" { name = \"app-test-sg\" description = \"security group for app test instance\" vpc_id = \"${aws_vpc.vpc.id}\" ingress { description = \"ssh\" from_port = 22 to_port = 22 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } ingress { description = \"http\" from_port = 80 to_port = 80 protocol = \"tcp\" security_groups = [aws_security_group.app-alb-sg.id] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } tags = { Name = \"app-test-sg\" }}resource \"aws_eip\" \"app-test-ec2-eip\" { vpc = true tags = { Name = \"app-test-eip\" }}2.3 EC2준비가 되었으니 EC2를 생성한다. a 존에 있는 퍼블릭 서브넷에 생성될 것이며 프리티어로 사용이 가능한 t2.micro 타입을 선택했다. 기타 위 생성한 리소스들과 연결해준다.resource \"aws_instance\" \"app-test-ec2\" { ami = \"ami-0b8414ae0d8d8b4cc\" instance_type = \"t2.micro\" availability_zone = \"${var.aws_region}${element(var.availability_zones, 0)}\" subnet_id = aws_subnet.public[0].id key_name = aws_key_pair.app-test-key-pair.key_name vpc_security_group_ids = [aws_security_group.app-test-sg.id] tags = { Name = \"app-test-ec2\" }}인스턴스를 생성하였으니 위에서 생성한 eip와 연결해준다.resource \"aws_eip\" \"app-test-ec2-eip\" { vpc = true instance = \"${aws_instance.app-test-ec2.id}\" tags = { Name = \"app-test-eip\" }}3. Application Load Balancer - Target Group &amp; Routing테스트 EC2가 존재하는 대상 그룹을 생성하고 테스트 도메인을 해당 대상그룹으로 라우팅한다.3.1 Target Group타겟 그룹을 생성한다. 80포트로 트래픽을 전송받을 것이며 /alb/health-check 경로로 상태검사를 진행할 것이다.resource \"aws_lb_target_group\" \"app-test\" { name = \"app-test\" port = 80 protocol = \"HTTP\" target_type = \"instance\" vpc_id = aws_vpc.vpc.id health_check { path = var.alb-health-check-path port = \"traffic-port\" }}3.2 Listener로드밸런서 리스너를 생성한다. 80포트의 경우 443 포트로 리다이렉트 시킨다. 443 포트의 경우 앞서 AWS Certificate Manager 에서 생성한 ssl 인증서를 적용하고 대상그룹으로 트래픽을 포워딩한다.resource \"aws_lb_listener\" \"app-alb-listener-http\" { load_balancer_arn = aws_lb.app-alb.arn port = \"80\" protocol = \"HTTP\" default_action { type = \"redirect\" redirect { port = \"443\" protocol = \"HTTPS\" status_code = \"HTTP_301\" } }}resource \"aws_lb_listener\" \"app-alb-listener-https\" { load_balancer_arn = aws_lb.app-alb.arn port = \"443\" protocol = \"HTTPS\" certificate_arn = aws_acm_certificate.ssl-certificate.arn default_action { type = \"forward\" target_group_arn = aws_lb_target_group.app-test.arn }}3.3 Target Register테스트 인스턴스를 대상 인스턴스로 등록한다.resource \"aws_lb_target_group_attachment\" \"app-test-attachment\" { target_group_arn = aws_lb_target_group.app-test.id target_id = aws_instance.app-test-ec2.id port = 80}3.4 Listener Rule로드밸런서 리스너 룰을 지정한다. 도메인이 app-admin-test.keencho.com, app-user-test.keencho.com 인 경우 테스트 인스턴스로 라우팅할 것이다.resource \"aws_lb_listener_rule\" \"app-alb-test-rule\" { listener_arn = aws_lb_listener.app-alb-listener-https.arn priority = 1 action { type = \"forward\" target_group_arn = aws_lb_target_group.app-test.arn } condition { host_header { values = [\"app-admin-test.keencho.com\", \"app-user-test.keencho.com\"] } }} :warning: 이 단계에서는 대상 그룹에 등록된 대상의 상태가 Unhealthy로 표시될 것입니다. 이는 인스턴스 내부에서 세팅해주지 않았기 때문이며 다음에 바로 설정해 보도록 하겠습니다.4. Route 53 RecordRoute 53 에 레코드를 추가한다. 각 테스트 도메인으로 들어오는 트래픽이 alb로 향할수 있도록 설정할 것이다.resource \"aws_route53_record\" \"app-admin-test\" { zone_id = aws_route53_zone.keencho.id name = \"app-admin-test.keencho.com\" type = \"A\" alias { name = aws_lb.app-alb.dns_name zone_id = aws_lb.app-alb.zone_id evaluate_target_health = true }}resource \"aws_route53_record\" \"app-user-test\" { zone_id = aws_route53_zone.keencho.id name = \"app-user-test.keencho.com\" type = \"A\" alias { name = aws_lb.app-alb.dns_name zone_id = aws_lb.app-alb.zone_id evaluate_target_health = true }}테스트 환경을 위해 AWS에서 설정해 줘야할 것은 끝났다. 나머지는 테스트 인스턴스에 직접 접속해서 진행해야 한다.5. EC2 설정이제 인스턴스에 직접 들어와 여러가지 설정을 해줘야 한다. nginx 설치 java 설치 PostgreSQL 설치 백엔드 앱 배포 프론트 앱 배포필자는 이러한 순서로 진행하였으며 이 포스팅은 인프라 구성에 대한 내용만 담을것이기 때문에 하나하나 어떻게 설치하고 배포하는지까지 작성하진 않는다.사용한 스크립트는 이곳 에서 확인할 수 있으며 꼭 필수로 해야할 작업 몇가지만 소개하도록 하겠다. target group 상태검사 경로 지정 - nginx 설정 파일을 통해 /alb/health-check 경로로 접근한 경우 200 상태값을 리턴할 수 있도록 첫번째 서버 블록에 정의하였다. /api 경로가 존재하는 요청과 아닌 경우 구분 - /api 경로가 없는 경우 프론트 리액트 앱으로, /api 경로가 있는 경우 백엔드 서버로 라우팅 될수 있도록 하였다.이 단계까지 진행하였다면 대상그룹이 Healthy 상태로 변경되었는지 확인해보자. 변경되었다면 실제 도메인으로 접속해 기능이 정상적으로 동작하는지 확인해보면 된다." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 3. 네트워크", "url": "/posts/terraform-aws-infra-3/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-02-03 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 3. 네트워크이번 포스팅부터 본격적으로 AWS 리소스를 생성한다. 그 첫번째로 인프라 구성에 가장 기본이 되는 네트워크 관련 리소스(vpc, subnet 등…)부터 생성한다.이 글에선 vpc, subnet등이 무엇인지 설명하진 않는다. 이 시리즈 를 따라가다 보면 개념들을 확인할 수 있으니 모르시는 분들은 읽어보시기 바란다.시작하기전 기초 코드를 작성한다.# main.tfterraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~&gt; 4.16\" } } required_version = \"&gt;= 1.2.0\"}provider \"aws\" { region = var.aws_region default_tags { tags = { Project = var.app_project_name } }}variable \"app_name\" { description = \"The name of application\" type = string default = \"app\"}variable \"app_project_name\" { description = \"The name of Project\" type = string default = \"appProject\"}전역적으로 Project 태그를 세팅하였다. 한 계정에 프로젝트가 1개이면 상관없지만, 프로젝트가 N개라면 프로젝트를 분리하여 지정했을 시 프로젝트 별로 비용을 확인할 수 있기 때문에 편리하다.변수변수를 활용한다.variable \"aws_region\" { description = \"The AWS region to deploy the VPC in\" type = string default = \"ap-northeast-2\"}variable \"availability_zones\" { description = \"List of availability zones to use\" type = list(string) default = [\"a\", \"b\"]}variable \"vpc_cidr\" { description = \"The CIDR block for the VPC\" type = string default = \"10.0.0.0/16\"}variable \"public_subnet_cidrs\" { description = \"The CIDR blocks for the public subnets\" type = list(string) default = [\"10.0.1.0/24\", \"10.0.2.0/24\"]}variable \"private_subnet_cidrs\" { description = \"The CIDR blocks for the private subnets\" type = list(string) default = [\"10.0.3.0/24\", \"10.0.4.0/24\"]}variable \"db_subnet_cidrs\" { description = \"The CIDR blocks for the DB subnets\" type = list(string) default = [\"10.0.5.0/24\", \"10.0.6.0/24\"]}위 변수들은 네트워크 리소스 생성에 필요한 변수들이다. 이 예제에서는 a, b 2개의 가용존을 사용한다. 만약 a, b, c, d 모두 사용하고 싶다면variable \"availability_zones\" { description = \"List of availability zones to use\" type = list(string) default = [\"a\", \"b\", \"c\", \"d\"]}위와 깉이 기본 값만 수정해주면 쉽게 가용존을 확장할 수 있다. (물론 서브넷 cidr 도 수정해 줘야 한다.) 웹 콘솔 환경에서 하나하나 클릭하면서 생성할때와 비교해 봤을때 생산성이 눈에 띄게 늘어남을 확인할 수 있을 것이다.리소스1. VPCresource \"aws_vpc\" \"vpc\" { cidr_block = var.vpc_cidr enable_dns_support = true enable_dns_hostnames = true tags = { Name = \"${var.app_name}-vpc\" }}첫번째로 VPC를 생성한다.2. Internet Gatewayresource \"aws_internet_gateway\" \"igw\" { vpc_id = aws_vpc.vpc.id tags = { Name = \"${var.app_name}-igw\" }}다음으로 퍼블릭 서브넷에 붙일 용도로 Internet Gateway를 생성한다.3. NAT Gatewayresource \"aws_eip\" \"nat\" { count = length(var.public_subnet_cidrs) vpc = true tags = { Name = \"${var.app_name}-nat-eip-${element(var.availability_zones, count.index)}\" }}resource \"aws_nat_gateway\" \"nat\" { count = length(var.public_subnet_cidrs) allocation_id = aws_eip.nat[count.index].id subnet_id = element(aws_subnet.public[*].id, count.index) tags = { Name = \"${var.app_name}-nat-${element(var.availability_zones, count.index)}\" } depends_on = [aws_internet_gateway.igw]}다음으로 프라이빗 서브넷에 붙일 용도로 NAT Gateway를 생성한다. NAT Gateway는 탄력적 ip를 필요로 하기 때문에 이 또한 생성한다.4. Routing Tableresource \"aws_route_table\" \"rt-public\" { vpc_id = aws_vpc.vpc.id tags = { Name = \"${var.app_name}-rt-public\" }}resource \"aws_main_route_table_association\" \"main\" { vpc_id = aws_vpc.vpc.id route_table_id = aws_route_table.rt-public.id}resource \"aws_route\" \"route-public\" { route_table_id = aws_route_table.rt-public.id destination_cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.igw.id}resource \"aws_route_table\" \"rt-private\" { count = length(var.private_subnet_cidrs) vpc_id = aws_vpc.vpc.id tags = { Name = \"${var.app_name}-rt-private-${element(var.availability_zones, count.index)}\" }}resource \"aws_route\" \"route-private\" { count = length(var.private_subnet_cidrs) route_table_id = element(aws_route_table.rt-private[*].id, count.index) destination_cidr_block = \"0.0.0.0/0\" nat_gateway_id = element(aws_nat_gateway.nat[*].id, count.index)}다음으로 라우팅 테이블을 생성한다. 1개의 퍼블릭 라우팅 테이블과 각각의 존에 존재하는 2개의 프라이빗 라우팅 테이블을 생성하였으며 퍼블릭 라우팅 테이블을 기본 라우팅 테이블로 지정한다.퍼블릭 서브넷의 아웃바운드 트래픽은 Internet Gateway로, 프라이빗 서브넷의 아웃바운드 트래픽은 NAT Gateway로 전송한다. :warning: VPC 생성시 AWS는 기본적으로 라우팅 테이블을 하나 생성한다. 위 구성에서 생성한 라우팅 테이블과는 전혀 관련없는 라우팅 테이블이다. 뭔가 깔끔(?) 하게 구성하려면 콘솔에서 혹은 CLI로 직접 해당 라우팅 테이블을 삭제하면 된다. (Terraform 차원에서는 살짝 어려운듯 하다.) 여기서는 진행하지 않는다.5. Subnetresource \"aws_subnet\" \"public\" { count = length(var.public_subnet_cidrs) vpc_id = aws_vpc.vpc.id cidr_block = element(var.public_subnet_cidrs, count.index) map_public_ip_on_launch = true availability_zone = \"${var.aws_region}${element(var.availability_zones, count.index)}\" tags = { Name = \"${var.app_name}-subnet-public-${element(var.availability_zones, count.index)}\" }}resource \"aws_subnet\" \"private\" { count = length(var.private_subnet_cidrs) vpc_id = aws_vpc.vpc.id cidr_block = element(var.private_subnet_cidrs, count.index) availability_zone = \"${var.aws_region}${element(var.availability_zones, count.index)}\" tags = { Name = \"${var.app_name}-subnet-private-${element(var.availability_zones, count.index)}\" }}resource \"aws_subnet\" \"private-db\" { count = length(var.db_subnet_cidrs) vpc_id = aws_vpc.vpc.id cidr_block = element(var.db_subnet_cidrs, count.index) availability_zone = \"${var.aws_region}${element(var.availability_zones, count.index)}\" tags = { Name = \"${var.app_name}-subnet-db-private-${element(var.availability_zones, count.index)}\" }}한개의 퍼블릭 서브넷과 4개의 프라이빗 서브넷 (2 for app, 2 for db)을 생성한다.6. Subnet Associationresource \"aws_route_table_association\" \"rt-public-association\" { count = length(var.public_subnet_cidrs) subnet_id = element(aws_subnet.public[*].id, count.index) route_table_id = aws_route_table.rt-public.id}resource \"aws_route_table_association\" \"rt-private-association\" { count = length(var.private_subnet_cidrs) subnet_id = element(aws_subnet.private[*].id, count.index) route_table_id = element(aws_route_table.rt-private[*].id, count.index)}resource \"aws_route_table_association\" \"rt-private-db-association\" { count = length(var.db_subnet_cidrs) subnet_id = element(aws_subnet.private-db[*].id, count.index) route_table_id = element(aws_route_table.rt-private[*].id, count.index)}마지막으로 서브넷과 라우팅 테이블들을 명시적으로 연결한다.리소스 맵VPC 콘솔로 들어가 리소스 맵을 확인해 보자. 위와같이 리소스가 구성되어 있으면 된다. 더욱더 확실하게 확인하려면 각각의 리소스 콘솔로 이동하여 확인해보자.Route 53 &amp; SSL 인증서원할한 진행을 위해 Route 53 호스팅 영억을 생성하고 AWS Certificate Manager를 통해 SSL 인증서를 발급받아 두도록 하겠다. (추후 CloudFront 구성시 대체 도메인을 연결해야 하는데, 이때 SSL 인증서가 필수이다.)각각의 record는 그때그때 추가하도록 한다.resource \"aws_route53_zone\" \"keencho\" { name = \"keencho.com\" lifecycle { prevent_destroy = true } tags = { Name = \"app-keencho-route53\" }}resource \"aws_route53_record\" \"app-certificate-validation\" { for_each = { for dvo in aws_acm_certificate.ssl-certificate.domain_validation_options : dvo.domain_name =&gt; { name = dvo.resource_record_name record = dvo.resource_record_value type = dvo.resource_record_type } } allow_overwrite = true name = each.value.name records = [each.value.record] ttl = 60 type = each.value.type zone_id = aws_route53_zone.keencho.id}resource \"aws_acm_certificate\" \"ssl-certificate\" { domain_name = \"*.keencho.com\" validation_method = \"DNS\" lifecycle { create_before_destroy = true }}resource \"aws_acm_certificate_validation\" \"ssl-certificate-validation\" { certificate_arn = aws_acm_certificate.ssl-certificate.arn validation_record_fqdns = [for record in aws_route53_record.app-certificate-validation : record.fqdn]}AWS Certificate Manager에 SSL 인증서를 추가하고 Route 53 CNAME 레코드를 통해 손쉽게 검증할 수 있다.주의할 점이 있다. 현 시점 생성된 SSL 인증서는 서울 리전 (ap-northeast-2)에 존재한다. 추후 CloudFront 생성시에는 버지니아 북부 (us-east-1) 에 존재하는 SSL 인증서만 사용할 수 있다. 따라서 us-ease-1 리전에도 인증서를 추가해 주도록 하겠다.provider \"aws\" { alias = \"us-east-1\" region = \"us-east-1\"}resource \"aws_acm_certificate\" \"ssl-certificate-virginia\" { domain_name = \"*.keencho.com\" validation_method = \"DNS\" provider = aws.us-east-1 lifecycle { create_before_destroy = true }}어떤 로직이 숨겨져 있는지 모르겠지만 위 인증서도 앞서 진행한 ssl-certificate-validation 검증을 통해 검증받을 수 있다. (별도의 검증 리소스 블락이 필요하지 않음.) 따라서 생성후 조금만 기다리면 상태가 변경됨을 확인할 수 있다.만약 도메인을 외부 도메인 업체 (가비아, 후이즈 등) 에서 구매하였다면 해당 업체 관리화면으로 이동해 네임서버를 AWS가 생성한 네임서버로 변경하도록 하자. 변경하지 않으면 무용지물이다." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 2. 기초", "url": "/posts/terraform-aws-infra-2/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2024-01-13 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS ECS 무중단 배포 인프라 구성하기 - 2. 기초앞서 설명했든 프론트단은 React, 백단은 Spring-Boot를 사용한다.앱이 포스팅에서 앱을 구성하는 방법까지 포함하지는 않는다. 이 리포지토리 에 필요한 코드는 모두 존재하니 필요한 분들은 참고하시기 바란다.전체적인 구성 은 다음과 같다. 프론트 (React) 모노레포 프로젝트 구성 백 (SpringBoot) 멀티모듈 프로젝트 구성 프론트 경로와 겹치지 않는 contextPath 필수 (이 예제에선 /api 사용) 프론트 경로와 겹치지 않는 contextPath가 필수인 이유는 CloudFront에서 경로에 따라 라우팅 정책을 다르게 가져가야 때문이다. (또한 CORS 문제 피하기) CloudFront는 운영 트래픽을 각각 s3와 Application Load Balancer 로 전송해야 하는데, 만약 contextPath가 /app 으로 모두 동일하다면 CloudFront가 경로에 따라 요청 분기를 하지 못하기 때문에 둘중 한곳에 유니크한 contextPath는 필수이다.따라서 백단에 /api contextPath를 적용한다. (/api 경로가 존재하는 요청 -&gt; ALB로 전송, 그외 -&gt; S3로 전송)만약 프론트 경로에 /api/account/list와 같은 경로가 존재하면 CloudFront는 요청을 ALB로 전송할 것이므로 의도하지 않은 동작이 될 것이다. 이를 염두하고 코드를 작성했다.Terraform &amp; AWS CLI 이곳 을 참고하여 terraform을 설치한다. 이곳 을 참고하여 AWS CLI를 설치한다. 이곳 을 참고하여 AWS CLI를 설정한다.모든 설정이 완료되었다면 아래 명령어를 통해 terraform 과 aws cli가 잘 구성되었는지 확인한다.terraformaws sts get-caller-identity본격적인 AWS 리소스 구성은 다음 포스팅부터 진행한다." }, { "title": "Terraform으로 AWS 무중단 배포 인프라 구성하기 - 1. 개요", "url": "/posts/terraform-aws-infra-1/", "categories": "AWS, Terraform", "tags": "AWS, ECS", "date": "2023-12-09 08:12:00 +0900", "snippet": "Terraform으로 AWS 무중단 배포 인프라 구성하기 개요 기초 네트워크 테스트 환경 운영환경 (프론트) 운영환경 (백엔드) 마무리Terraform으로 AWS 무중단 배포 인프라 구성하기 - 1. 개요최근 오픈하는 시스템을 ECS 기반 인프라로 변경하였다. 도입시 겪었던 시행착오나 기타 문제를 다시한번 정리하며 무중단 배포 인프라를 구성해본다.AWS Web Console로도 인프라 구성이 가능히지만 코드를 사용해 인프라를 관리하기 위해 IaC중 하나인 Terraform 을 사용한다. Terraform이 무엇인지는 자세하게 설명하진 않겠으나 필요시 간단하게 짚고 넘어가도록 한다.이 시리즈 와는 다르게 테스트 환경 구축, 운영 환경의 경우 ECS Fargate 사용, Terraform 사용, CloudFront 사용 등 실제 서비스에 적용할 수 있도록 구성해볼 것이다.인프라 구성구성할 인프라를 간단하게 설명하면 다음과 같다. 도메인은 내가 보유중인 keencho.com 도메인을 사용한다. Route 53은 다음 도메인의 트래픽을 각각 라우팅한다. app-admin.keencho.com (관리자 - CloudFront) app-user.keencho.com (사용자 - CloudFront) app-admin-test.keencho.com (관리자 테스트 - Application Load Balancer) app-user-test.keencho.com (사용자 테스트 - Application Load Balancer) app-resources.keencho.com (리소스 - CloudFront) CloudFront 운영 트래픽, 리스소 트래픽을 담당 운영 트래픽의 /api/** 경로로 시작하는 요청의 경우 Application Load Balancer로 전달한다. /api/** 경로로 시작하지 않는 운영 요청은 react 앱이 배포되어 있는 s3 버킷으로 전송한다. 리소스 트래픽은 리소스 s3 버킷으로 전송한다. Application Load Balancer 테스트 트래픽, 운영 api 트래픽 담당 (도메인에 따라 분기) 테스트 트래픽은 2a 퍼블릭 서브넷에 위치하는 test + bastion 인스턴스로 전달 운영 api 트래픽은 ECS Fargate 로 전송한다. RDS - 운영 db는 RDS를 사용한다. EFS - 어플리케이션 로그를 쌓기위한 용도로 EFS 를 사용한다. ECR - 빌드한 도커 위이미지를 저장하기해 ECR을 사용한다. CodeDeploy - 운영 앱 배포를 위해 CodeDeploy를 사용한다.프론트단은 React, 백단은 SpringBoot를 사용하며 DB는 PostgreSQL을 사용한다.인프라 다이어그램위 구성을 다이어그램으로 표현해보면 다음과 같다.기타이 시리즈에서는 Terraform, AWS CLI 가 무엇인지 등은 설명하지 않는다.terraform applyterraform planterraform destroy위와 같은 명령어들도 마찬가지로 무엇인지는 설명하지 않으며 IaC, CLI 등에 기초 지식이 있다는 가정 하에 진행한다. :warning: Application Load Balancer에서 S3로 트래픽을 라우팅할수 있는 방법은 없다. 따라서 CloudFront에서 요청을 분기해야 하는데 이렇게 인프라를 구성하려면 Route 53 은 필수다. 아예 모든 리소스를 aws에서 구매하거나 본인이 소유한 도메인의 네임서버를 aws의 것으로 변경할 수 있어야 한다. (혹은 도메인을 구매한 곳에서 CNAME 레코드를 설정할 수 있어야 한다.)" }, { "title": "AWS Abuse malware website report", "url": "/posts/aws-abuse-malware-website-report/", "categories": "AWS", "tags": "AWS", "date": "2023-10-31 08:12:00 +0900", "snippet": "AWS Abuse malware website report평화로운 목요일 AWS에서 메일 한통이 날라왔다. 다음날 연차고 여행 계획이 잡혀있기 때문에 대수롭지 않게 넘겼다. 그런데 다음날 금요일 밤 11시에 두번째 메일이 날라왔다.구글링 해보니 메일 끝부분에 자세한 내용이 적혀 있다고 하여 마지막 부분까지 쭉 읽어보니 멀웨어 공격과 관련된 사이트를 호스팅하고 있다는 내용임을 확인할 수 있었다.해당 링크는 apk파일을 다운받을 수 있는 링크로서, 회사 내부 직원들이 쉽게 앱을 다운받을 수 있도록 해둔 링크였다.메일을 뒤져보니 Netcraft라는 곳에서 링크를 피싱 링크로 지정하고 해당 서버와 관련된 곳에 알림을 보낸것으로 확인됐다.링크 비활성화문제는 AWS가 마지막 메일을 보낸 이후 24시간 안에 조치를 취하고 해당 메일에 답신하지 않으면 관련 리소스들을 임의로 종료시켜버릴수 있다는 것이었다.서버를 자세히 뒤져볼수 없는 상황(여행중) 이었지만 시말서를 쓰기 싫었던 나는 부랴부랴 서버로 접속하여 nginx로 해당 링크로 접속시 404 상태값을 리턴하도록 임시로 조치하고 AWS에 조치를 취했다고 답신하였다.이벤트 종료다행히 AWS 에서 우리가 필요한 모든 조치를 했다고 판단하고 이벤트를 종료시켰다.원인 파악언제까지고 링크를 비활성화한 상태로 둘순 없어서 출근 하자마자 원인 파악에 들어갔다. 내가 확인해 봤던건 다음과 같다. 서버 로그인 기록 탐색 비정상 프로세스 확인 JADX로 apk파일 디컴파일 clamav로 멀웨어 탐색 AWS GuardDuty로 인스턴스 볼륨(EBS) 멀웨어 탐색이중 문제가 발견된곳은 아무곳도 없었다.그래서 다시 구글링을 해보니 Netcraft가 잘못 탐지하여 링크를 피싱 링크로 등록한 사례를 어렵지 않게 찾아볼 수 있었다.그래서 Netcraft에 더블 체크를 요청하는 메일을 전송하였고 그쪽에서 잘못된 인식 이었음을 동의하고 후처리했다는 메일을 받아볼수 있었다.의문점보아하니 AWS가 Netcraft 에서 보낸 알림을 받고 이벤트를 생성하여 우리에게 알림을 보낸것 같다. 그런데 문제를 해결한 시점에 다시 보니 AWS는 따로 더블체크하지 않은듯 하다.원래 이런가 하여 좀 찾아보니 GCP, OCI 등에서도 이런 사례가 있는것 같았다. 그렇다면 웹 호스팅 업체에서 지정한 보안업체(Netcraft 같은) 에서 알림을 받으면 진위 여부를 확인하지 않고 이벤트를 생성한다는 말이 되는데 뭔가 좀 이상했다.보안업체에서 잘못한 경우 잘못된 문제를 해결하는 동안 실생활에서 발생한 또다른 문제는 누구에게 책임을 돌려야 하는가?결론이 케이스의 경우 해당 링크를 사용하는 사람이 많지 않고 사람이 직접 파일을 전달하면 되기 때문에 큰 문제없이 넘어갔지만 만약 사용자가 많아 링크를 비활성화 할 수 없는 상황이라면 끔찍했을거 같은 생각이 든다." }, { "title": "React custom hook return array vs object", "url": "/posts/react-hook-return-array-object/", "categories": "React", "tags": "React", "date": "2023-08-29 08:12:00 +0900", "snippet": "React custom hook return array vs object1. 개요리액트 자체의 훅, 또는 리액트 관련 라이브러리들의 훅을 사용하다 보면 드는 의문점이 하나 있다.const [state, setState] = useState&lt;string&gt;('');const { table, selection } = useTable();어떤 훅은 리턴타입이 배열이고, 어떤 훅은 리턴타입이 객체이다. (엄밀히 따지면 배열도 객체이긴 하지만 편의상 객체라고 하겠다.)이 둘의 차이점을 살펴보자.2. 차이점?어떤것을 선택하던 훅이 수행하는 행위의 차이점은 없다.리턴타입으로 배열을 사용하는 경우, 동일 훅을 같은 컴포넌트 함수 내에서 N번 사용하는 경우 배열은 순서에 기반하기 때문에 객체 형태에 비해 코드량이 줄어드는 장점(?) 이 있다.만약 useState() 훅의 리턴타입이 객체였다면 어땠을까? 아래 예시를 보자const { state: state1, setState: setState1 } = useState&lt;string&gt;('');const { state: state2, setState: setState2 } = useState&lt;string&gt;('');배열 방식에 비해 덜 깔끔하지 않은가? 훅이 N번 사용될 경우 객체 비구조화 할당시 위와같이 별칭을 사용해야 하기 때문이다. 이것이 리액트 저자들이 useState() 훅의 리턴 타입을 배열로 지정한 이유이다.물론 이것이 항상 훅이 리턴타입으로 배열을 사용해야 함을 의미하는 것은 아니다. 훅이 리턴하는 값의 갯수가 많을수록 배열은 적합하지 않다.어떤 훅의 리턴타입이 배열이고 그 배열이 포함하고 있는 값이 5개라고 가정하고 개발자는 배열의 첫번째 속성과 마지막 속성만 사용한다고 가정하자. 우리는 훅의 모든 속성을 사용하는 것이 아님에도 배열의 구조적 특성상 변수를 할당해야 한다.const [v1, _1, _2, _3, v2] = useCustomHook();위와같이 실제 사용하는 속성은 v1, v2 임에도 순서를 유지하기 위해 불필요한 변수를 할당해야 하는 것이다.3. 그래서?공식 문서 에서는 다음과 같이 설명하고 있다. Unlike a React component, a custom Hook doesn’t need to have a specific signature. We can decide what it takes as arguments, and what, if anything, it should return. In other words, it’s just like a normal function. Its name should always start with use so that you can tell at a glance that the rules of Hooks apply to it.그냥 사용하는 용도에 따라 선택하라고 설명되어 있다.따라서 본인은 다음과 같은 룰을 정하여 개발하고 있다. 같은 훅이 동일한 컴포넌트 내에서 N번 사용된다면 리턴타입으로 배열을 사용한다. 훅이 리턴하는 값의 갯수가 3개를 넘어가면 리턴타입으로 객체를 사용한다.어떤 글을 봐도 위 주제에는 딱히 정답이랄게 없어 보인다. 개발자가 확실한 규칙을 갖고 훅을 개발한다면, 곧 그것이 정답이 아닐까?" }, { "title": "Sticky Session - AWS 에서 설정하기", "url": "/posts/aws-application-load-balancer-sticky-session/", "categories": "AWS", "tags": "AWS, Application Load Balancer, SAML", "date": "2023-07-03 08:12:00 +0900", "snippet": "Sticky Session1. 개요최근 외부 시스템과 로그인 처리를 연동해야할 일이 있었는데, 해당 시스템은 SAML기반 인증 방식 을 사용하고 있었다.카카오, 네이버, 구글등은 로그인을 Oauth 2.0 기반의 사용자 인증 기능을 제공하는데, 개인적으로는 SAML 인증 방식은 처음 접해봤다.가이드 문서가 잘 되어 있어서 개발환경 / 테스트 환경에서는 문제없이 개발 및 테스트를 진행했다. 그러나 라이브 서버에 배포했을때 문제가 발생했다.짧지 않은 시간동안 끙끙 앓다가 어느순간 머리를 스치듯 한 생각이 지나갔다.세션 인증 방식을 사용하고 있어 요청 / 응답하는 서버가 다르면 에러가 발생한다? 라는 생각이다. 실제로 확인해보니, A 인스턴스가 요청한 인증을 B 인스턴스가 받아서 처리하면 에러가 발생한다.그렇다면 만약 최초 접속을 A 인스턴스로 하였다면 그 이후의 모든 요청은 A 인스턴스로 라우팅 될 수 있도록 조치가 필요하다. 이때 필요한 것이 Sticky Session 기능이다. nginx로 치면 ip_hash와 비슷하다고 볼 수 있겠다.라이브 서버는 Application Load Balancer를 사용하고 있는데 다행히 설정 하나로 Sticky Session 설정이 가능하다.2. 왜 발생하지? :exclamation: 개인적으로 기억을 정리하기 위한 문단입니다. 궁금하지 않으신 분들은 다음 문단으로 넘어가 주세요.근데 뭔가 이상하다. SAML 토큰에는 일반적으로 서명(signature)이 포함되어 있어 토큰의 무결성을 보장한다. 따라서 요청 서버와 응답 서버가 다르더라도 서명 검증을 통해 SAML 토큰의 유효성을 확인할 수 있다. (확실하지 않음…)물론 이를 위해선 요청 서버와 인증 서버간 적절한 키 or 인증서 공유가 이루어져야 한다. 나의 경우 이러한 키를 전달받아 적용한 상태였다.그런데도 에러가 발생해 라이브러리 코드를 까봤다. 까보니 이유를 찾을수 있었다. 인증 요청시 message 생성후 Map 형태의 LocalStorage class에 데이터 저장후 인증 요청 인증 성공후 인증 정보 파싱시 LocalStorage class에서 데이터 검색 인스턴스가 다르니 당연히 각각의 인스턴스에 LocalStorage가 존재함 A 인스턴스에 저장된 정보를 B 인스턴스에서 찾고자 하니 에러 발생이 라이브러리는 2010년대 초반에 만들어진듯 한데, 그때는 서버를 이중화로 구성하지 않았나? 잘 모르겠다… 어쨌거나 명확한 문제를 찾아서 속이 시원했다.3. AWS 에서 Sticky Session 설정하기위에서 말했다시피 현재 서비스는 Application Load Balancer을 사용하고 있었고, 다행히 간단한 설정으로 Sticky Session을 설정할 수 있었다.Sticky Session은 로브밸런서가 아닌 대상 그룹 에서 확인할 수 있다.대상 그룹 -&gt; 타겟 -&gt; 속성 -&gt; 편집 순으로 이동하자.편집 페이지에 들어왔으면 대상 선택 구성 항목을 찾을 수 있다.옵션을 활성화 하고 고정 유형을 로드 밸런서 생성 쿠키로 선택한다. 애플리케이션 기반 쿠키를 선택한다면 애플리케이션 레벨에서 처리를 해줘야 하기 때문에 귀찮다.적당한 고정 지속 시간을 선택한다. 예를들어 1일로 지정했다면 오늘 10시에 사용자의 요청이 로드밸런서를 통해 A 인스턴스로 흘러 들어갔다면 내일 10시까지는 해당 사용자의 모든 요청은 A 인스턴스로 흘러 들어가게 된다.저장하면 완료다. 정말 간단하게 설정할 수 있다.위와같이 요청에 AWSALB, AWSALBCORS 라는 이름의 쿠키가 있다면 성공이다. 가장 확실한 방법은 어떤 인스턴스인지 확인할 수 있는 방법을 만들고 배포하여 확인해보는 것이다.4. 결론Sticky Session은 특정 클라이언트의 요청을 항상 동일한 서버로 보내기 때문에 일부 서버에 부하가 집중될 수 밖에 없다.이를 사용하는 도중 서버에 장애가 발생하거나 다운되면 해당 클라이언트의 요청은 다른 서버로 전달되지 못하고 서비스 중단 현상이 발생할 수 있어 전체 시스템의 성능 저하를 야기할 수 있다.그러나 나로서는 급한 상황이라 선택지가 없었고, 어쩔수 없이 적용하게 되었다.아예 별도의 도메인과 별도의 인스턴스를 띄워 장애가 발생하더라도 기존 시스템에 영향을 주지 않도록 하고 싶었지만 문제는 돈이지…서버 비용을 아예 신경 안쓸순 없겠지만 시스템의 안정성을 위해서라도 분리 할껀 하고 돈 낼껀 내주면 좋겠다. (이직하자)" }, { "title": "FFI", "url": "/posts/ffi/", "categories": "FFI", "tags": "FFI, Java", "date": "2023-06-12 08:12:00 +0900", "snippet": "FFI1. 개요FFI는 Foreign Function Interface의 약자입니다. 이는 어떤 프로그래밍 언어세서 다른 프로그래밍 언어나 라이브러리의 함수를 호출하고 사용하기 위한 인터페이스를 의미합니다.FFI는 일반적으로 서로 다른 언어 간에 상호 운용성(interoperability)을 제공하기 위해 사용됩니다. 프로그래밍 언어 간에는 다양한 규칙과 데이터 형식이 존재하기 때문에, 다른 언어나 라이브러리의 함수를 직접 호출하는 것은 어려울 수 있습니다. 이런 경우에 FFI를 사용하여 함수 호출 및 데이터 전달을 처리할 수 있습니다.FFI는 일반적으로 프로그래밍 언어의 컴파일러나 인터프리터에서 제공하는 기능이며, 언어에 따라 구현 방식이 다를 수 있습니다. 예를 들어, C 언어에서는 FFI를 통해 다른 언어의 함수를 호출하거나 C 함수를 다른 언어에서 호출할 수 있습니다. Java에서는 JNI(Java Native Interface)라는 FFI를 제공하여 C나 C++로 작성된 함수를 호출할 수 있습니다.FFI는 다른 언어나 플랫폼에 구애받지 않고 다양한 기능을 사용할 수 있도록 도와주는 중요한 도구입니다. 이를 통해 다른 언어나 라이브러리의 장점을 활용하면서도 프로그래밍 언어의 생산성과 효율성을 유지할 수 있습니다.2. 자바에서의 FFI자바에서 FFI를 구현하려면 JNI(Java Native Interface)혹은 JNA(Java Native Access) 를 사용하면 됩니다. 두가지 방법은 각각 장단점이 있으며 다음과 같은 차이점이 있습니다.1. JNI (Java Native Interface)JNI는 네이티브 코드와 직접적으로 상호 작용하기 때문에 매우 뛰어난 성능을 제공 합니다. 네이티브 코드에 접근하기 위해 자바와 C/C++ 사이의 직접적인 매핑을 사용합니다.JNI를 사용하면 자바에서 네이티브 코드에 직접 메모리를 할당하고 조작할 수 있습니다. 이는 성능이 중요하거나 복잡한 데이터 구조를 다룰 때 유용합니다.JNI는 복잡한 구현이 필요하며, 네이티브 코드와 자바 사이의 인터페이스를 명시적으로 작성해야 합니다. 이로 인해 개발 과정이 번거로울 수 있습니다.JNI는 플랫폼에 종속적입니다. 네이티브 코드의 컴파일이 필요하며, 특정 플랫폼에 맞춰야 합니다.JNI는 직접 C 코드를 작성해야 하기 때문에 따로 예제코드를 작성하지 않습니다.2. JNA (Java Native Access)JNA는 네이티브 코드와 자바 간의 인터페이스를 자동으로 처리해주는 라이브러리입니다. 별도의 네이티브 헤더 파일을 작성할 필요 없이, 자바 인터페이스를 선언하고 JNA가 자동으로 매핑을 처리합니다.JNA는 플랫폼에 독립적입니다. 네이티브 라이브러리를 로드하고 사용할 수 있는 기능을 제공하며, 플랫폼에 종속되지 않습니다.JNI에 비해 JNA는 약간의 성능 손실이 있을 수 있습니다. JNA는 동적 라이브러리 매핑과 추가적인 오버헤드를 포함하기 때문입니다.public class HelloController { public interface User32 extends Library { User32 INSTANCE = (User32) Native.load(\"user32\", User32.class); int MessageBoxA(int hWnd, String lpText, String lpCaption, int uType); } @FXML protected void onHelloButtonClick() { User32.INSTANCE.MessageBoxA(0, \"Hello from Java!\", \"MessageBox Example\", 0); }}간단한 예제입니다. 몇줄되지 않는 코드로 버튼 클릭시 Win32 API - MessageBoxA 함수를 호출하는 것을 확인할 수 있습니다.그래서?JNI와 JNA는 각각의 상황과 용도에 따라 선택해야 합니다. 성능과 직접 메모리 접근이 중요한 경우에는 JNI가 적합하며, 플랫폼 독립성과 개발 편의성이 중요한 경우에는 JNA를 고려할 수 있습니다." }, { "title": "DB 뷰(View)", "url": "/posts/database-view/", "categories": "Database", "tags": "Database, PostgreSQL", "date": "2023-05-23 08:12:00 +0900", "snippet": "1. 개요데이터베이스 뷰는 현대의 데이터 중심 시스템에서 필수적인 요소로 자리잡았습니다. 데이터베이스 뷰는 데이터베이스 시스템에서 사용자들에게 편리한 인터페이스를 제공하고, 데이터를 효율적으로 조작하고 검색할 수 있는 기능을 제공합니다. 특히, PostgreSQL과 같은 강력한 관계형 데이터베이스에서 데이터베이스 뷰는 매우 유용한 도구입니다.데이터베이스 뷰란 무엇일까요? 간단하게 말하면, 뷰는 하나 이상의 테이블에서 파생된 가상의 테이블입니다. 즉, 실제로 데이터를 저장하지 않고, 기존의 테이블에서 필요한 데이터를 조회하거나 가공하여 보여주는 역할을 합니다. 뷰는 쿼리의 결과를 미리 정의된 구조로 표시하고 필요한 조건에 따라 데이터를 제한하는 등의 기능을 수행할 수 있습니다.2. 뷰의 장점1. 복잡한 쿼리 단순화복잡한 조인이나 계산이 필요한 쿼리를 미리 정의된 뷰로 대체함으로써, 사용자는 간단한 쿼리만 작성하여 데이터를 추출하고 조작할 수 있습니다. 이는 개발자와 데이터 분석가가 작업을 더욱 효율적으로 수행할 수 있게 해줍니다.2. 데이터의 보안과 접근 제어 강화뷰를 사용하여 사용자에게 필요한 데이터만 노출하고, 민감한 정보를 숨길 수 있습니다. 데이터베이스의 보안 정책을 더욱 강력하게 구현할 수 있으며, 접근 권한을 통해 특정 사용자 또는 역할에게만 뷰를 제공할 수 있습니다.3. 데이터의 일관성과 중복성을 관리하는데 도움이 됨여러 테이블에서 동일한 데이터를 사용해야 할 때, 뷰를 사용하여 중복 데이터를 피하고 일관된 데이터를 제공할 수 있습니다. 이는 데이터의 일관성을 유지하고, 데이터의 변경이 필요한 경우 뷰만 수정하면 되므로 유지 보수와 관리의 편의성을 높입니다.3. 뷰의 단점1. 성능뷰는 가상의 테이블로써, 실제 데이터를 포함하고 있지 않으며 쿼리 실행 시 실제 데이터를 추출하여 결과를 반환합니다. 따라서 복잡한 뷰의 경우 성능 문제가 발생할 수 있습니다. 특히, 뷰에 대한 쿼리가 복잡하거나 데이터 양이 많은 경우에는 성능 저하가 발생할 수 있습니다.2. 복잡성뷰는 데이터베이스 시스템의 구조를 복잡하게 만들 수 있습니다. 여러 테이블을 조합하고 조인하여 뷰를 생성하는 경우, 데이터베이스 스키마가 복잡해지고 이해하기 어려워질 수 있습니다. 또한 뷰 자체가 다른 뷰에 의존하는 경우, 변경 사항을 추적하고 유지 관리하기가 어려울 수 있습니다.3. 업데이트 제한일부 데이터베이스 시스템에서는 뷰를 통해 데이터를 업데이트하는 것을 제한합니다. 뷰가 여러 테이블의 조합이거나 특정 조건에 따라 데이터를 필터링하는 경우, 뷰를 통해 데이터를 직접 업데이트하는 것이 어려울 수 있습니다. 이 경우에는 대신 뷰를 구성하는 원본 테이블을 직접 업데이트해야 합니다.4. PostgreSQL에서 뷰 생성하기PostgreSQL에서 뷰를 생성하는 방법은 간단합니다.1. CREATE VIEW 문을 사용하여 뷰 생성PostgreSQL에서는 CREATE VIEW 문을 사용하여 뷰를 생성합니다. 아래의 형식을 따라 뷰를 생성할 수 있습니다.CREATE VIEW view_name ASSELECT column1, column2, ...FROM table1WHERE condition; view_name: 생성할 뷰의 이름을 지정합니다. column1, column2, ...: 뷰에서 표시할 열의 이름을 지정합니다. 필요에 따라 테이블의 열이나 계산된 열을 선택할 수 있습니다. table1: 뷰를 생성할 기반이 되는 테이블의 이름을 지정합니다. WHERE condition: 뷰에 적용할 선택적인 조건을 지정합니다. 이를 통해 특정 데이터만 뷰에 표시할 수 있습니다.예를 들어, employees 테이블로부터 이름과 직급 정보만을 포함하는 employee_view라는 뷰를 생성하려면 다음과 같이 작성할 수 있습니다.CREATE VIEW employee_view ASSELECT name, titleFROM employees;2. 뷰 사용 권한 설정뷰를 생성한 후, 해당 뷰에 대한 액세스 권한을 설정해야 합니다. 기본적으로 뷰를 생성한 사용자가 해당 뷰에 대한 액세스 권한을 가지게 됩니다. 그러나 다른 사용자나 역할에게도 뷰를 사용할 수 있는 권한을 주고 싶다면 GRANT 문을 사용하여 액세스 권한을 명시적으로 할당할 수 있습니다.employee_view에 대한 읽기 액세스 권한을 user1 사용자에게 주려면 다음과 같이 작성합니다.GRANT SELECT ON employee_view TO user1;3. 뷰 사용뷰가 생성되고 권한이 설정되었다면, 해당 뷰를 쿼리에서 사용할 수 있습니다.SELECT * FROM employee_view;뷰를 사용할때는 몇가지 사항을 고려해야 합니다. 뷰는 기존의 테이블을 기반으로 생성되므로, 뷰에 대한 변경은 실제 테이블에 영향을 주지 않습니다. 뷰는 데이터를 저장하지 않고 실시간으로 데이터를 조회하므로, 뷰를 통해 변경된 데이터는 실제 테이블에 반영되지 않습니다. 뷰는 복잡한 쿼리의 단순화나 데이터의 가공을 위해 사용되지만, 성능 저하의 가능성도 있습니다. 큰 데이터셋이나 복잡한 조인이 있는 경우 성능에 영향을 줄 수 있으므로 주의해야 합니다.5. 뷰의 사용 사례데이터베이스에서 뷰는 실제로 유용하게 활용됩니다. 몇가지 대표적인 사례를 소개하고 각각에 대한 설명과 실제 쿼리를 살펴보겠습니다.1. 복잡한 쿼리 단순화뷰를 사용하면 복잡한 쿼리를 단순화할 수 있습니다. 여러 개의 테이블 간 조인이 필요한 쿼리를 미리 정의된 뷰로 대체하여, 사용자는 간단한 쿼리만 작성하여 결과를 얻을 수 있습니다.예를 들어, orders와 customers 테이블이 있을 때, 주문 정보와 고객 정보를 함께 조회하는 복잡한 쿼리를 뷰로 단순화할 수 있습니다.CREATE VIEW order_details ASSELECT o.order_id, o.order_date, c.customer_nameFROM orders oINNER JOIN customers c ON o.customer_id = c.customer_id;이제 order_details 뷰를 사용하여 간단하게 주문 정보와 고객 정보를 조회할 수 있습니다.SELECT * FROM order_details;2. 데이터 보안 강화뷰를 사용하여 데이터의 보안을 강화할 수 있습니다. 민감한 정보가 포함된 테이블을 직접 노출하지 않고, 필요한 열만 포함하여 사용자에게 제공할 수 있습니다.예를 들어, employees 테이블에는 연봉 정보가 포함되어 있습니다. 그러나 일반 사용자에게는 연봉 정보를 숨기고 이름과 직급 정보만을 제공하고 싶다면 다음과 같이 뷰를 생성할 수 있습니다.CREATE VIEW employee_info ASSELECT name, titleFROM employees;이제 employee_info 뷰를 통해 직원의 이름과 직급 정보만을 조회할 수 있으며, 연봉 정보는 숨겨져 있습니다.3. 데이터 가공 및 형식 변환뷰를 사용하여 데이터를 가공하거나 형식을 변환할 수 있습니다. 특정 형식으로 변환된 데이터를 필요로 하는 경우, 뷰를 사용하여 원본 데이터를 변환된 형식으로 제공할 수 있습니다.예를 들어, sales 테이블에는 날짜와 판매량이 포함되어 있습니다. 특정 형식으로 날짜를 표시하고자 할 때, 뷰를 사용하여 변환된 날짜 형식으로 데이터를 제공할 수 있습니다.CREATE VIEW sales_summary ASSELECT TO_CHAR(sale_date, 'YYYY-MM-DD') AS formatted_date, quantityFROM sales;이제 sales_summary 뷰를 통해 형식이 변환된 날짜와 판매량을 조회할 수 있습니다.6. 성능 고려 사항데이터베이스 뷰는 유용한 도구지만, 잘못 사용하거나 설계하지 않으면 성능 저하의 가능성이 있습니다. 이제 데이터베이스 뷰를 사용할 때 고려해야 할 성능 측면에 대해 다루고, 성능을 향상시킬 수 있는 방법에 대해 알아보겠습니다.1. 조인과 쿼리 복잡성뷰를 생성할 때 조인 연산이 포함된 복잡한 쿼리를 사용한다면 성능 저하가 발생할 수 있습니다. 조인이 많거나 복잡한 쿼리는 처리 시간을 증가시킬 수 있으므로, 필요한 경우 쿼리를 최적화하고 인덱스를 활용하는 것이 중요합니다. 인덱스 활용: 뷰의 성능을 향상시키기 위해 뷰에 사용된 열에 인덱스를 생성할 수 있습니다. 이는 조인 조건이나 필터링 조건에 사용되는 열에 인덱스를 생성하는 것과 동일한 원리입니다. 당연히 뷰 자체에 인덱스를 생성할 순 없습니다.2. 뷰 재작성데이터베이스의 기본 테이블이 변경될 때, 뷰의 정의를 재작성하는 것이 성능 향상에 도움이 될 수 있습니다. 변경된 데이터에 대한 최신 정보를 유지하고 뷰의 쿼리 실행 계획을 최적화하기 위해 뷰를 재작성하는 것이 좋습니다. 일부 데이터베이스 시스템은 뷰를 자동으로 재작성하는 기능을 제공합니다. 이를 활용하여 데이터 변경 시 자동으로 뷰를 갱신하도록 설정할 수 있습니다.3. 쿼리 최적화뷰를 사용하는 쿼리의 성능을 향상시키기 위해 쿼리 최적화를 고려해야 합니다. 쿼리 실행 계획을 분석하고 성능 향상을 위해 적절한 인덱스를 생성하거나 쿼리 튜닝을 수행하는 것이 중요합니다. 데이터베이스 시스템은 쿼리 실행 계획을 제공하여 쿼리의 실행 계획을 분석할 수 있습니다. 이를 통해 비효율적인 연산이나 조인을 확인하고 최적화할 수 있습니다.4. 제약 사항 고려뷰를 사용할 때 주의해야 할 제약 사항도 고려해야 합니다. 예를 들어, 뷰의 정의에 사용된 테이블이나 열이 변경되거나 삭제되면 뷰가 올바르게 동작하지 않을 수 있습니다. 이에 대비하여 뷰를 사용하기 전에 제약 사항을 확인하고 유지보수에 주의해야 합니다.7. 마무리데이터베이스 뷰를 사용하면 복잡한 쿼리를 단순화하고, 보안을 강화하며, 데이터를 가공하거나 형식을 변환할 수 있습니다. 뷰는 사용자에게 필요한 데이터를 간편하게 제공하며, 중복 데이터를 제거하여 일관성을 유지합니다. 또한, 데이터베이스의 성능을 향상시키기 위해 인덱스를 활용하거나 뷰를 재작성하는 등의 방법을 적용할 수 있습니다.뷰를 적절히 활용하면 데이터베이스 작업을 효율적으로 수행할 수 있습니다. 복잡한 쿼리를 단순화하여 개발자와 사용자 모두의 작업을 간편하게 만들고, 데이터의 보안과 일관성을 유지할 수 있습니다. 또한, 성능을 향상시키기 위해 쿼리를 최적화하고 인덱스를 활용하는 등의 방법을 적용하여 데이터베이스의 응답 시간을 줄일 수 있습니다.데이터베이스 뷰는 데이터 관리와 분석 작업에 있어 필수적인 도구입니다. 우리는 이제 뷰의 개념, 생성 방법, 사용 사례, 성능 고려 사항 등을 다루며 데이터베이스 뷰의 활용에 대해 배웠습니다. 이제 여러분은 데이터베이스 뷰를 효과적으로 활용하여 더 나은 데이터 관리와 분석을 실현할 수 있을 것입니다.데이터베이스 뷰의 중요성을 인지하고, 적절한 상황에서 뷰를 적용하여 데이터베이스 작업을 더욱 효율적으로 수행할 수 있도록 노력해보세요. 데이터베이스 뷰는 데이터의 가시성과 일관성을 제공하며, 데이터 관리와 분석의 효율성을 높여줄 것입니다.끄적끄적..이 글은 Hibernate ‘introduce @View annotation’ PR을 보고 삘받아서 정리한 글입니다. 2023년에 ORM에서 뷰를 사용할수 있게 될수도 있다는 것에 좋아하는게 맞나 모르겠지만… 일단은 좋습니다. 나오면 얼른 사용해보고 싶네요." }, { "title": "자바 동시성 프로그래밍 - 메모리 모델과 동기화부터 고급 기법까지", "url": "/posts/java-concurrency-programming/", "categories": "Java", "tags": "Java", "date": "2023-05-15 08:12:00 +0900", "snippet": "1. 서론1.1 동시성 프로그래밍의 중요성과 자바 메모리 모델의 역할동시성 프로그래밍은 현대 소프트웨어 개발에서 매우 중요한 개념입니다. 동시성은 여러 개의 작업이 동시에 실행되는 것을 의미하며, 이는 프로그램의 성능과 반응성을 향상시킬 수 있습니다. 하지만 동시에 실행되는 작업들 간의 상호작용은 잘못된 결과나 예상치 못한 동작을 초래할 수도 있습니다.자바 메모리 모델은 동시성 프로그래밍에서 스레드 간의 상호작용과 메모리의 동기화를 관리하는 역할을 합니다. 메모리 모델은 스레드가 공유 변수를 읽고 쓸 때의 동작 방식과 가시성 등을 규정합니다. 이를 통해 스레드 간의 안전한 데이터 공유와 일관된 실행 결과를 보장할 수 있습니다.1.2 동시성 문제와 그로 인해 발생할 수 있는 버그와 성능 이슈동시성 프로그래밍은 여러 동시 실행되는 스레드들 사이에서 발생하는 문제를 다루는데, 주요한 문제들은 다음과 같습니다: 경합 상태(Race Condition): 여러 스레드가 동시에 공유 변수에 접근하여 수정하는 경우, 의도치 않은 결과가 발생할 수 있습니다. 스레드 간의 실행 순서나 타이밍에 따라 결과가 달라지며, 이로 인해 버그가 발생할 수 있습니다. 교착상태(Deadlock): 두 개 이상의 스레드가 서로가 가진 자원을 기다리며 무한히 대기하는 상태입니다. 이로 인해 프로그램이 멈추는 현상이 발생하며, 제대로 동작하지 않는 원인이 될 수 있습니다. 스레드 안전성 문제: 여러 스레드가 동시에 같은 자료구조나 객체를 수정하는 경우, 예기치 않은 결과가 발생할 수 있습니다. 이로 인해 잘못된 데이터 처리, 일관성 없는 상태 등의 문제가 발생할 수 있습니다.또한 동시성은 성능에도 영향을 미칠 수 있습니다. 스레드 간의 경쟁이나 동기화 메소드의 과도한 호출은 소요된 시간 및 자원의 비용을 증가시킬 수 있습니다. 동시에 실행되는 스레드들이 서로 경쟁하거나 동기화를 위해 대기하는 경우, 성능 저하가 발생할 수 있습니다. 특히, 잘못된 동기화 기법이 사용될 경우 스레드 간의 대기 시간이 늘어나거나 병목 현상이 발생할 수 있습니다.따라서 동시성 문제를 올바르게 다루고 해결하는 것은 중요합니다. 올바른 동기화 기법과 스레드 간의 협력 및 통신 방법을 사용하여 동시성 문제를 예방하고, 성능 이슈를 최소화할 수 있습니다.2. 자바 스레드 모델과 스레드 안정성2.1 자바에서 스레드를 생성하고 관리하는 방법자바에서는 Thread 클래스를 상속받거나 Runnable 인터페이스를 구현하여 스레드를 생성하고 실행할 수 있습니다. Thread 클래스를 상속받은 경우 run() 메서드를 오버라이드하여 스레드가 실행할 코드를 작성합니다. Runnable 인터페이스를 구현한 경우 run() 메서드를 구현하고, 해당 객체를 Thread 클래스의 생성자에 전달하여 스레드를 생성합니다.스레드는 start() 메서드를 호출하여 실행됩니다. JVM은 스레드를 스케줄링하여 실행하며, 여러 스레드가 동시에 실행될 수 있습니다.// Thread 클래스를 상속받아 스레드 생성하기class MyThread extends Thread { @Override public void run() { // 스레드가 실행할 작업을 작성 System.out.println(\"스레드가 실행됩니다.\"); }}// Runnable 인터페이스를 구현하여 스레드 생성하기class MyRunnable implements Runnable { @Override public void run() { // 스레드가 실행할 작업을 작성 System.out.println(\"스레드가 실행됩니다.\"); }}public class Main { public static void main(String[] args) { // Thread 클래스를 상속받은 스레드 생성하기 MyThread thread1 = new MyThread(); thread1.start(); // 스레드 실행 // Runnable 인터페이스를 구현한 스레드 생성하기 MyRunnable runnable = new MyRunnable(); Thread thread2 = new Thread(runnable); thread2.start(); // 스레드 실행 }}2.2 스레드 안전성 개념과 스레드 안전한 코드의 필요성스레드 안전성은 여러 스레드가 동시에 접근해도 안전하게 동작하는 코드를 의미합니다. 스레드 안전한 코드는 경합 상태(Race Condition)와 같은 동시성 문제를 피하고, 정확하고 일관된 결과를 보장합니다.스레드 안전한 코드를 작성하기 위해서는 동기화(Synchronization) 메커니즘을 사용해야 합니다. 동기화를 통해 여러 스레드가 동시에 접근하더라도 데이터의 일관성과 안정성을 보장할 수 있습니다. 자바에서는 synchronized 키워드를 사용하여 메서드 또는 블록을 동기화할 수 있습니다.스레드 안전한 코드는 다중 스레드 환경에서 안전하게 동작하는데, 이는 프로그램의 정확성과 신뢰성을보장합니다. 스레드 안전한 코드는 동시성 문제로부터 자유로워져 예기치 않은 버그와 잠재적인 문제를 방지할 수 있습니다. 따라서 개발자는 스레드 안전성을 고려하여 코드를 작성하고, 필요한 동기화 메커니즘을 적절히 사용하여 데이터의 일관성과 안정성을 확보해야 합니다.class Counter { private int count = 0; // synchronized 키워드를 사용하여 동기화된 메서드 public synchronized void increment() { count++; } public int getCount() { return count; }}public class Main { public static void main(String[] args) { Counter counter = new Counter(); // 여러 스레드에서 동시에 increment() 메서드 호출 Runnable runnable = () -&gt; { for (int i = 0; i &lt; 1000; i++) { counter.increment(); } }; // 여러 스레드 생성 Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); // 스레드 실행 thread1.start(); thread2.start(); try { // 스레드 실행이 완료될 때까지 대기 thread1.join(); thread2.join(); } catch (InterruptedException e) { e.printStackTrace(); } // 최종 결과 출력 System.out.println(\"Count: \" + counter.getCount()); }}3. 자바 메모리 모델(Memory Model)3.1 자바 메모리 모델 개념과 목적자바 메모리 모델은 멀티스레드 환경에서 스레드들이 메모리를 공유하는 방식과 동작을 정의하는 규칙의 모음입니다. 목적은 다음과 같습니다. 스레드 간의 동작과 상호작용을 정확하게 제어하기 위해 일관된 동작을 보장합니다. 다중 스레드로부터 안전하게 공유된 데이터에 접근할 수 있는 방법을 제공합니다.자바 메모리 모델은 스레드 간의 상호작용을 통해 가시성(Visibility)와 순서(Ordering)를 관리합니다. 가시성은 한 스레드에서 변경한 데이터가 다른 스레드에 얼마나 빠르게 보이는지를 의미하며, 순서는 연속된 연산의 실행 순서를 의미합니다.3.2 메인 메모리, 캐시, 메모리 가시성자바 메모리 모델은 메인 메모리와 캐시(Cache) 등의 다양한 메모리 영역을 다룹니다. 메인 메모리: 시스템 전체의 주 메모리로 모든 스레드가 공유하는 공간입니다. 캐시: 프로세서의 캐시 메모리로 각 코어마다 독립적으로 존재하며, 메인 메모리로부터 데이터를 가져와 캐시에 저장합니다.메모리 가시성은 메인 메모리와 캐시 간의 데이터 일관성을 다루는 개념입니다. 메인 메모리에 저장된 데이터가 캐시에 복사되거나, 캐시에 있는 데이터가 메인 메모리로 반영되는 시점에 따라 스레드 간의 데이터 가시성이 달라집니다.3.3 멀티코어 아키텍처에서의 동작멀티코어 아키텍처에서는 여러 개의 코어가 병렬로 동작하며, 각 코어마다 독립적인 캐시가 있습니다. 이로 인해 각 스레드가 서로 다른 캐시를 사용하면서 메모리 가시성 문제가 발생할 수 있습니다. 자바 메모리 모델은 이러한 문제를 해결하기 위해 volatile 키워드와 동기화 기법을 제공합니다.volatile 키워드를 사용하여 변수를 선언하면 변수를 메인 메모리에 저장하고 캐시를 건너뛰고 메인 메모리에서 값을 읽고 쓰도록 보장합니다. volatile 변수를 사용하면 스레드 간의 가시성이 확보되어 데이터의 일관성이 유지됩니다.또한 동기화 기법인 synchronized 키워드를 사용하여 임계 영역을 설정하거나 Lock과 Condition을 활용하여 스레드 간의 동기화를 달성할 수 있습니다. 이를 통해 여러 스레드가 동시에 접근하더라도 데이터 일관성과 스레드 안전성을 유지할 수 있습니다.자바 메모리 모델은 멀티코어 아키텍처에서의 동작을 고려하여 스레드 간의 메모리 가시성과 일관성을 제어함으로써 동시성 프로그래밍에서 발생할 수 있는 문제를 해결합니다. 이를 통해 안정적이고 정확한 동시성 프로그램을 개발할 수 있습니다.4. 동기화(Synchronization)4.1 개념동기화는 여러 스레드가 동시에 공유된 자원에 접근할 때, 이를 조절하여 데이터의 일관성과 스레드 안전성을 보장하는 메커니즘입니다. 동기화는 경쟁 상태(Race Condition)와 같은 동시성 문제를 해결하고, 스레드 간의 상호작용을 조율하여 예상치 못한 결과를 방지합니다.4.2 자바에서의 동기화 메커니즘자바에서는 synchronized 키워드와 volatile 키워드를 제공하여 동기화를 구현할 수 있습니다. synchronized 키워드: 메서드나 블록에 synchronized 키워드를 사용하여 임계 영역을 설정합니다. 임계 영역에 진입한 스레드는 해당 객체의 모니터 락(Monitor Lock)을 획득하고, 다른 스레드는 해당 영역에 진입하지 못하고 대기합니다. 이를 통해 스레드 간의 동기화가 이루어집니다.public synchronized void synchronizedMethod() { // 동기화가 필요한 작업 수행} volatile 키워드: volatile 키워드를 변수에 사용하면 해당 변수의 값을 메인 메모리에 쓰고 읽을 때 캐시를 건너뛰도록 보장합니다. 이를 통해 스레드 간의 가시성을 제공합니다. volatile 변수는 변수 선언 앞에 volatile 키워드를 붙여서 사용합니다.private volatile int sharedVariable;4.3 Atomic 클래스자바는 java.util.concurrent.atomic 패키지에서 Atomic이라는 접두사를 가진 클래스들을 제공합니다. 이들 클래스는 원자적(Atomic) 연산을 수행하여 스레드 간의 동기화를 보장합니다. AtomicInteger, AtomicLong, AtomicBoolean 등의 클래스를 사용하여 원자적인 연산을 수행할 수 있습니다.private AtomicInteger atomicCounter = new AtomicInteger();public void increment() { atomicCounter.incrementAndGet();}어토믹 클래스는 내부적으로 Compare-and-Swap(CAS) 연산을 사용하여 스레드 간의 경쟁 상태를 해결합니다. 이를 통해 동기화 작업을 락 없이 처리할 수 있으며, 성능적으로 우수한 결과를 얻을 수 있습니다.4.4 동기화의 한계동기화는 스레드 간의 상호작용을 보장하고 스레드 안전성을 보장하는 강력한 메커니즘입니다. 하지만 동기화는 오버헤드와 성능 저하의 가능성이 있습니다. 락을 획득하고 해제하는 과정에서 추가적인 작업이 필요하며, 여러 스레드가 동기화된 영역에 접근하기 위해 경쟁하면서 대기하게 됩니다. 이는 프로그램의 실행 속도를 느리게 만들 수 있습니다.또한 동기화의 과도한 사용은 교착상태(Deadlock)와 같은 문제를 발생시킬 수 있습니다. 교착상태는 두 개 이상의 스레드가 서로가 소유한 리소스를 기다리며 무한히 대기하는 상태를 말합니다. 이는 프로그램의 정상적인 실행을 방해하고 데드락을 해결하기 위한 복잡한 알고리즘을 요구합니다.따라서 동기화를 사용할 때는 신중하게 접근해야 합니다. 필요한 부분에만 동기화를 적용하고, 동기화의 범위를 최소화하여 성능을 향상시킬 수 있습니다. 또한 교착상태와 같은 문제를 피하기 위해 동기화 코드를 신중하게 설계하고 데드락 상황을 예방할 수 있는 방법을 고려해야 합니다.자바에서는 동기화를 위해 synchronized 키워드 외에도 Lock 인터페이스와 그 구현체인 ReentrantLock을 제공합니다. Lock을 사용하면 더 세밀한 제어와 고급 동기화 기능을 활용할 수 있습니다. 하지만 Lock은 synchronized 키워드보다 사용이 복잡하고 실수할 가능성이 높기 때문에 적절한 상황에서 사용해야 합니다.동기화는 스레드 간의 상호작용을 조율하여 스레드 안전성과 데이터 일관성을 보장하는 중요한 개념입니다. 하지만 오버헤드와 성능 저하의 가능성을 고려하여 적절하게 사용해야 합니다.5. 스레드 간의 협력과 통신스레드 간의 협력과 통신은 동시성 프로그래밍에서 중요한 측면입니다. 여러 스레드가 동시에 실행되는 환경에서는 스레드들이 데이터를 공유하고 상호작용해야 합니다. 자바에서는 몇 가지 메커니즘을 제공하여 스레드 간의 협력과 통신을 가능하게 합니다.5.1 wait(), notify(), notifyAll() 메서드wait(), notify(), notifyAll()은 자바의 모든 객체가 가지고 있는 메서드로, 스레드 간의 협력과 통신을 위해 사용됩니다. 이 메서드들은 스레드가 객체의 모니터 락을 확보하고 있을 때 호출해야 합니다. wait(): 현재 스레드를 일시적으로 대기 상태로 전환시키고, 다른 스레드가 객체의 모니터 락을 획득하고 notify() 또는 notifyAll()을 호출하여 대기 중인 스레드를 깨울 때까지 대기합니다. notify(): 대기 중인 스레드 중 하나를 선택하여 깨웁니다. 선택된 스레드는 모니터 락을 얻을 수 있는 시도를 합니다. notifyAll(): 대기 중인 모든 스레드를 깨웁니다. 깨어난 스레드들은 모니터 락을 얻을 수 있는 시도를 합니다.5.2 스레드 간의 데이터 공유스레드 간의 데이터 공유는 동시성 프로그래밍에서 주의해야 할 중요한 측면입니다. 여러 스레드가 동일한 데이터에 접근하면서 데이터 일관성 문제나 경합 상태(Race Condition)와 같은 문제가 발생할 수 있습니다.동기화 메커니즘을 사용하여 스레드 간의 데이터 공유를 관리할 수 있습니다. synchronized 키워드를 사용하여 메서드나 블록을 동기화할 수 있으며, volatile 키워드를 사용하여 변수의 가시성과 순서를 보장할 수 있습니다.public class DataSharingExample { private int sharedData = 0; public synchronized void increment() { sharedData++; } public synchronized void decrement() { sharedData--; } public int getSharedData() { return sharedData; }}5.3 스레드 간의 협력스레드 간의 협력은 작업을 조율하고 스레드의 실행 순서를 제어하는 것을 의미합니다. 스레드 간의 협력을 위해 다양한 기법을 사용할 수 있습니다. Lock과 Condition: Lock 인터페이스와 Condition 인터페이스는 스레드 간의 협력을 위한 기능을 제공합니다. Lock은 synchronized 키워드보다 더 세밀한 제어가 가능하며, Condition은 스레드의 대기와 신호를 관리할 수 있습니다. wait()과 notify(): 앞서 언급한 wait()과 notify() 메서드를 사용하여 스레드 간의 대기와 신호를 조절할 수 있습니다. wait()을 호출한 스레드는 대기 상태로 전환되며, 다른 스레드가 notify()을 호출하여 대기 중인 스레드를 깨울 수 있습니다. BlockingQueue: BlockingQueue 인터페이스는 스레드 간의 작업을 조율하기 위한 자료구조입니다. 큐에 요소를 추가하거나 제거할 때 스레드가 차단되어 대기하도록 할 수 있습니다. 이를 통해 생산자-소비자 패턴과 같은 협력적인 작업을 구현할 수 있습니다. CountDownLatch: CountDownLatch 클래스는 특정 스레드가 여러 개의 작업이 완료될 때까지 대기하는 동안 다른 스레드들이 작업을 수행할 수 있도록 하는 데 사용됩니다. 주로 메인 스레드가 여러 개의 작업 스레드가 완료될 때까지 기다리는 상황에서 활용됩니다. CyclicBarrier: CyclicBarrier 클래스는 지정된 수의 스레드가 특정 지점에서 만날 때까지 기다릴 수 있도록 하는 동기화 기법입니다. 모든 스레드가 도착하면 지정된 작업을 수행하거나 다음 단계로 진행할 수 있습니다.이러한 스레드 간의 협력과 통신 기법을 적절하게 활용하면 스레드 간의 작업 조율과 데이터 공유를 효과적으로 관리할 수 있습니다.public class ThreadCooperationExample { private Lock lock = new ReentrantLock(); private boolean isDataAvailable = false; public void produceData() { lock.lock(); try { // 데이터 생성 및 저장 isDataAvailable = true; } finally { lock.unlock(); } } public void consumeData() { lock.lock(); try { while (!isDataAvailable) { // 데이터가 생성되기를 기다림 } // 데이터 소비 isDataAvailable = false; } finally { lock.unlock(); } }}6. 고급 동시성 기법과 패턴동시성 문제를 해결하기 위해 고급 동시성 기법과 패턴을 사용할 수 있습니다. 이러한 기법과 패턴은 스레드 간의 경합 상태와 성능 문제를 해결하고, 코드의 가독성과 확장성을 향상시키는 데 도움이 됩니다. 이 섹션에서는 일부 일반적인 고급 동시성 기법과 패턴을 살펴보겠습니다.6.1 스레드풀(Thread Pool)스레드풀은 작업을 처리하기 위해 미리 생성된 스레드의 풀을 유지하고 관리하는 기법입니다. 스레드풀을 사용하면 작업을 처리하기 위해 매번 스레드를 생성하고 제거하는 비용을 줄일 수 있습니다. 대신, 스레드풀에서 사용 가능한 스레드 중 하나를 할당하여 작업을 실행합니다.public class ThreadPoolExample { public static void main(String[] args) { // 스레드풀 생성 ExecutorService executor = Executors.newFixedThreadPool(5); // 작업 제출 for (int i = 0; i &lt; 10; i++) { executor.execute(new WorkerThread(\"Task \" + i)); } // 작업 완료 후 스레드풀 종료 executor.shutdown(); }}class WorkerThread implements Runnable { private String task; public WorkerThread(String task) { this.task = task; } @Override public void run() { // 작업 실행 System.out.println(\"Executing: \" + task); }}위의 예제에서는 Executors.newFixedThreadPool(5)를 사용하여 크기가 5인 스레드풀을 생성합니다. 10개의 작업을 제출하고 각 작업은 스레드풀에서 사용 가능한 스레드 중 하나에 할당되어 실행됩니다.6.2 락의 종류와 성능락은 여러 스레드가 공유 자원에 접근하는 것을 제어하기 위해 사용됩니다. 자바에서는 synchronized 키워드를 사용하여 단일 스레드에 의한 잠금을 구현할 수 있습니다. 그러나 synchronized는 단일 스레드에 의한 잠금만 지원하며, 여러 스레드에 의한 동시 접근을 허용하지 않습니다. 이에 대비하여 다양한 락의 종류가 있습니다. ReentrantLock: synchronized와 유사한 기능을 제공하는 락으로, 재진입 가능 합니다. 재진입 가능한 기능을 제공하는 락으로, synchronized와 달리 더욱 세밀한 제어가 가능합니다. 다음은 ReentrantLock의 예제 코드입니다.public class ReentrantLockExample { private static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) { Thread t1 = new Thread(new WorkerThread()); Thread t2 = new Thread(new WorkerThread()); t1.start(); t2.start(); } static class WorkerThread implements Runnable { @Override public void run() { lock.lock(); // 락 획득 try { // 임계 영역 System.out.println(\"Critical section\"); } finally { lock.unlock(); // 락 해제 } } }}위의 예제에서는 ReentrantLock을 사용하여 임계 영역을 보호합니다. lock.lock()을 호출하여 락을 획득하고, lock.unlock()을 호출하여 락을 해제합니다. 이를 통해 여러 스레드 간의 상호 배제를 구현할 수 있습니다. ReadWriteLock: 읽기 작업과 쓰기 작업을 구분하여 여러 스레드가 동시에 읽기 작업을 수행할 수 있도록 지원합니다. 쓰기 작업이 진행 중일 때는 다른 스레드의 쓰기 작업과 읽기 작업이 불가능합니다. StampedLock: ReadWriteLock과 유사한 기능을 제공하지만, 읽기 락과 쓰기 락의 개념을 확장하여 조금 더 세밀한 제어가 가능합니다. 추가적으로 optimistic read와 write lock을 지원하여 성능을 향상시킬 수 있습니다.6.3 Thread-Safe Collections자바에서는 다양한 Thread-Safe Collection을 제공합니다. ConcurrentHashMap: 동시성 환경에서 안전하게 사용할 수 있는 해시 맵입니다. 여러 스레드가 동시에 맵에 접근하여 요소를 추가, 제거, 검색할 수 있습니다. CopyOnWriteArrayList: 동시성 환경에서 안전하게 사용할 수 있는 리스트입니다. 요소를 추가하거나 수정하는 작업이 일어날 때, 새로운 복사본을 만들어 작업을 수행합니다. 따라서 읽기 작업은 락 없이 동시에 수행할 수 있습니다. ConcurrentLinkedQueue: 동시성 환경에서 안전하게 사용할 수 있는 큐입니다. 여러 스레드가 동시에 큐에 접근하여 요소를 추가하거나 제거할 수 있습니다.public class ThreadSafeDataStructureExample { public static void main(String[] args) { // ConcurrentHashMap 예제 ConcurrentHashMap&lt;String, Integer&gt; concurrentMap = new ConcurrentHashMap&lt;&gt;(); concurrentMap.put(\"Key1\", 1); concurrentMap.put(\"Key2\", 2); int value1 = concurrentMap.get(\"Key1\"); System.out.println(value1); // CopyOnWriteArrayList 예제 CopyOnWriteArrayList&lt;String&gt; copyOnWriteList = new CopyOnWriteArrayList&lt;&gt;(); copyOnWriteList.add(\"Item1\"); copyOnWriteList.add(\"Item2\"); String item1 = copyOnWriteList.get(0); System.out.println(item1); // ConcurrentLinkedQueue 예제 ConcurrentLinkedQueue&lt;Integer&gt; concurrentQueue = new ConcurrentLinkedQueue&lt;&gt;(); concurrentQueue.offer(1); concurrentQueue.offer(2); int element = concurrentQueue.poll(); System.out.println(element); }}위의 예제에서는 각각 ConcurrentHashMap, CopyOnWriteArrayList, ConcurrentLinkedQueue를 사용하여 Thread-Safe한 자료구조를 보여줍니다. 이 자료구조들은 여러 스레드에서 안전하게 동작하며, 동시에 요소를 추가하거나 제거할 수 있습니다.7. 동시성 문제와 디버깅동시성 프로그래밍에서는 동시에 실행되는 여러 스레드로 인해 발생할 수 있는 다양한 문제들이 있습니다. 이러한 동시성 문제들은 버그의 원인이 될 뿐만 아니라 성능 저하와 잘못된 동작을 야기할 수 있습니다. 이번 섹션에서는 동시성 프로그래밍에서 주로 발생하는 동시성 문제의 종류와 디버깅 방법에 대해 알아보겠습니다.7.1 경합 상태 (Race Condition)경합 상태는 여러 스레드가 공유된 자원에 동시에 접근하고 수정할 때 발생하는 문제입니다. 경합 상태는 예측할 수 없는 결과를 초래하며, 스레드 스케줄링과 같은 환경 변화에 따라 결과가 달라질 수 있습니다. 경합 상태를 해결하기 위해서는 상호 배제(Mutual Exclusion)와 같은 동기화 기법을 사용해야 합니다.7.2 교착상태 (Deadlock)교착상태는 두 개 이상의 스레드가 서로 상대방이 점유한 자원을 기다리며 무한히 대기하는 상태를 말합니다. 이는 상호 배제, 점유와 대기, 비선점, 순환 대기라는 네 가지 조건이 동시에 충족될 때 발생합니다. 교착상태를 해결하기 위해서는 교착상태의 조건 중 하나를 제거하거나, 교착상태를 회피하기 위한 알고리즘을 사용해야 합니다.7.3 스레드 안전성 문제스레드 안전성 문제는 여러 스레드가 동시에 접근하는 공유된 데이터의 일관성을 유지하지 못하는 문제입니다. 스레드 간의 데이터 경쟁이 발생하여 데이터 일관성이 깨지거나 잘못된 연산이 수행될 수 있습니다. 스레드 안전성 문제를 해결하기 위해서는 동기화 메커니즘을 사용하거나 스레드 안전한 자료구조를 활용해야 합니다.7.4 동시성 문제 디버깅 방법동시성 문제를 디버깅하는 것은 다른 종류의 버그를 디버깅하는 것보다 어렵습니다. 동시성 문제는 실행 시점에 따라 결과가 달라질 수 있으며, 일시적이고 재현이 어려워질 수 있습니다. 하지만 몇 가지 디버깅 기법을 활용하여 동시성 문제를 해결할 수 있습니다. 다음은 동시성 문제 디버깅에 도움이 되는 방법들입니다. 로그 및 출력 문장 분석: 로그와 출력 문장을 분석하여 동시성 문제의 원인을 찾을 수 있습니다. 스레드 간의 순서, 데이터의 상태 및 변경 이력 등을 확인하여 문제를 파악할 수 있습니다. 동기화 기법 검토: 동기화 기법의 적용 여부를 검토합니다. 경합 상태를 방지하기 위해 적절한 동기화 메커니즘을 사용하고, 교착상태를 해결하기 위해 교착상태의 조건을 제거하거나 회피 알고리즘을 적용합니다. 스레드 동작 분석: 각 스레드의 동작을 분석하여 문제가 발생하는 시점과 조건을 확인합니다. 스레드의 동기화 영역, 공유 데이터 접근, 대기 상태 등을 검토하여 문제의 원인을 찾을 수 있습니다. 도구 활용: 다양한 디버깅 도구를 활용하여 동시성 문제를 분석할 수 있습니다. 스레드 덤프 분석 도구, 데드락 검출 도구, 모니터링 도구 등을 사용하여 문제를 진단하고 해결할 수 있습니다. 테스트와 검증: 동시성 문제를 방지하기 위해 적절한 테스트와 검증을 수행해야 합니다. 다양한 상황에서의 동시성 동작을 시뮬레이션하고 테스트 케이스를 작성하여 문제를 발견하고 수정할 수 있습니다.동시성 문제는 복잡하고 예측하기 어려운 문제이지만, 위의 디버깅 방법들을 적절히 활용하면 문제를 해결할 수 있습니다. 동시성 문제를 이해하고 적절한 동기화 기법과 디버깅 기법을 사용하여 안정적이고 효율적인 동시성 프로그램을 개발할 수 있도록 노력해야 합니다.8. 결론이 글에서는 자바 메모리 모델과 동시성 프로그래밍의 중요성을 강조하였습니다. 동시성 프로그래밍은 효율적이고 안정적인 다중 스레드 애플리케이션을 구현하기 위해 필수적입니다.자바 메모리 모델을 이해하고, 적절한 동기화 기법을 사용하여 스레드 간의 동기화를 보장할 수 있습니다. 또한, 동기화를 통해 경합 상태와 같은 동시성 문제를 해결할 수 있습니다.스레드 간의 협력과 통신을 위해 wait(), notify(), notifyAll() 메서드를 사용할 수 있으며, 고급 동시성 기법과 패턴을 활용하여 동시성을 개선할 수 있습니다.동시성 문제와 디버깅에 대해서는 로그 분석, 동기화 기법 검토, 스레드 동작 분석, 도구 활용, 테스트와 검증 등을 통해 문제를 해결할 수 있습니다.자바 메모리 모델과 동시성 프로그래밍에 대한 이해는 안정적이고 효율적인 소프트웨어 개발을 위해 중요합니다. 개발자들은 동시성 프로그래밍에 대한 주의사항을 숙지하고, 적절한 기법과 패턴을 활용하여 안전하고 성능 우수한 애플리케이션을 개발할 수 있어야 합니다." }, { "title": "JOOQ 알아보기", "url": "/posts/jooq/", "categories": "JOOQ", "tags": "JOOQ, Spring Boot", "date": "2023-04-19 08:12:00 +0900", "snippet": "JOOQJOOQ란?JOOQ(Java Oriented Querying)는 Java를 사용하여 안전한 SQL 쿼리를 작성할 수 있도록 하는 데이터베이스 쿼리 프레임워크 입니다. 개발자가 SQL 쿼리를 보다 직관적이고 자연스럽게 작성할 수 있도록 하며 type-safe한 API를 제공합니다. 이를 통해 컴파일시 오류를 발견 / 수정할 수 있습니다.JOOQ는 PostgreSQL, Oracle, MySQL과 같은 관계형 데이터베이스들을 지원합니다. NoSQL은 기본적으로 지원하지 않습니다. 일부 제한적인 기능을 제공하긴 하는데 그럴빠에야 Spring Data MongoDB 와 같이 해당 영역에 특화된 프레임워크를 사용하는게 좋습니다.JOOQ는 단순 쿼리 작업 외에도 데이터베이스 스키마 생성, 코드 생성 및 기타 데이터베이스 관련 작업을 지원하므로 Java에서 데이터베이스 기반 애플리케이션을 구축하는데 유용한 도구입니다.JOOQ vs QuerydslJOOQ과 Querydsl은 개발자가 type-safe한 SQL 쿼리를 작성할 수 있도록 한다는 공통점이 있습니다. 그러나 두 프레임워크 사이에는 몇가지 차이점이 있습니다. DSL 구문 JOOQ는 SQL 구문을 기반으로 DSL을 사용합니다. Querydsl은 Java 구문을 기반으로 하는 DSL을 사용합니다. 지원 범위 JOOQ는 관계형 데이터베이스들을 중점으로 지원합니다. Querydsl은 관계형 데이터베이스와 비관계형 데이터베이스(NoSQL)을 모두 지원합니다. 코드 생성 JOOQ는 데이터베이스 스키마를 기반으로 Java 클래스를 생성합니다. QueryDSL은 자바 코드를 바탕으로 Java 클래스를 생성합니다. 그래서 둘중 무엇을 선택해야 하는가?사실 개인적인 생각으론 JOOQ와 Querydsl를 비교하는건 잘못됐다고 생각합니다. 어떤 Persistence Framework를 선택했느냐(ORM이냐 SQL이냐) 에 따라 달라지기 때문이죠.스프링 기반 웹 어플리케이션을 개발할때 관계형 데이터베이스를 사용한다면 Persistence Framework로 ORM 혹은 SQL Mapper를 선택합니다. ORM의 경우 Hibernate - Spring Data JPA - Querydsl조합을 사용하고 SQL Mapper의 경우 MyBatis를 사용합니다.SQL Mapper 를 선택했다면 JOOQ는 MyBatis 대신 사용할 수 있는 프레임워크라고 할 수 있습니다. XML 기반인 MyBatis와 다르게 Java로 type-safe하게 쿼리를 작성할 수 있기 때문이죠.따라서 만약 개발중인 어플리케이션이 MyBatis를 사용하고 type-safe하게 쿼리를 작성할 수 있는 프레임워크를 찾고있다면 JOOQ는 완벽한 프레임워크라고 볼 수 있습니다. ORM을 사용한다면 비즈니스 로직에는 Hibernate - Spring Data JPA - Querydsl조합을 사용하고 통계성 데이터나 대용량 배치 작업을 할때, 혹은 native query가 필요할 때 JOOQ를 사용하는 것이 좋다고 생각합니다.Spring Boot에서 JOOQ 사용하기Spring Boot 에서 JOOQ를 사용하는 방법에 대해 알아보도록 하겠습니다.1.환경설정빌드 도구는 maven을 사용하며 db는 PostgreSQL을 사용합니다.&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jooq&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;42.6.0&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;다음으로 테이블을 생성합니다.CREATE TABLE author ( id SERIAL NOT NULL PRIMARY KEY, first_name VARCHAR(50), last_name VARCHAR(50) NOT NULL);CREATE TABLE book ( id SERIAL NOT NULL PRIMARY KEY, title VARCHAR(100) NOT NULL);CREATE TABLE author_book( author_id INT NOT NULL, book_id INT NOT NULL, PRIMARY KEY (author_id, book_id), CONSTRAINT fk_ab_author FOREIGN KEY (author_id) REFERENCES author (id) ON UPDATE CASCADE ON DELETE CASCADE, CONSTRAINT fk_ab_book FOREIGN KEY (book_id) REFERENCES book (id));테이블을 생성했다면 spring datasource를 정의합니다. 만약 방언을 지정하지 않는다면 DEFAULT 를 사용합니다.spring.datasource.url=jdbc:postgresql://localhost:5432/jooqspring.datasource.username=usernamespring.datasource.password=passwordspring.datasource.driver-class-name=org.postgresql.Driverspring.jooq.sql-dialect= postgreslogging.level.org.jooq.tools.LoggerListener=DEBUG이제 maven 플러그인을 통해 생성한 테이블을 기반으로 자바 클래스를 생성할 것입니다.&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;properties-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;initialize&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;read-project-properties&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;files&gt; &lt;file&gt;src/main/resources/application.properties&lt;/file&gt; &lt;/files&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;&lt;plugin&gt; &lt;groupId&gt;org.jooq&lt;/groupId&gt; &lt;artifactId&gt;jooq-codegen-maven&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;generate-postgres&lt;/id&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- JDBC connection parameters --&gt; &lt;jdbc&gt; &lt;url&gt;${spring.datasource.url}&lt;/url&gt; &lt;user&gt;${spring.datasource.username}&lt;/user&gt; &lt;password&gt;${spring.datasource.password}&lt;/password&gt; &lt;driver&gt;${spring.datasource.driver-class-name}&lt;/driver&gt; &lt;/jdbc&gt; &lt;!-- Generator parameters --&gt; &lt;generator&gt; &lt;database&gt; &lt;name&gt; org.jooq.meta.postgres.PostgresDatabase &lt;/name&gt; &lt;includes&gt;.*&lt;/includes&gt; &lt;excludes/&gt; &lt;inputSchema&gt;public&lt;/inputSchema&gt; &lt;includeSystemSequences&gt;true&lt;/includeSystemSequences&gt; &lt;/database&gt; &lt;generate&gt; &lt;pojos&gt;true&lt;/pojos&gt; &lt;pojosEqualsAndHashCode&gt;true&lt;/pojosEqualsAndHashCode&gt; &lt;javaTimeTypes&gt;true&lt;/javaTimeTypes&gt; &lt;fluentSetters&gt;true&lt;/fluentSetters&gt; &lt;sequences&gt;true&lt;/sequences&gt; &lt;/generate&gt; &lt;target&gt; &lt;packageName&gt;com.keencho.jooq.model&lt;/packageName&gt; &lt;directory&gt;target/generated-sources/jooq&lt;/directory&gt; &lt;/target&gt; &lt;/generator&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;프로퍼티 파일에 접근하기 위해 properties-maven-plugin을 사용합니다. jooq-codegen-maven 플러그인을 사용하며 설정에 jdbc 정보, db명, 옵션, 타겟 패키지, 타겟 디렉토리 등 필요한 정보들을 입력합니다.필요한 정보를 모두 입력 했다면 mvn clean generate-sources 명령을 수행합니다. 아래처럼 클래스가 생성되었다면 성공입니다.2. 코드작성1. 전처리테스트 수행 전에 테이블을 초기화하고 새로운 값을 넣는 전처리 코드입니다.@AutowiredDSLContext dslContext;@BeforeAllpublic void setUp() { // author dslContext.delete(AUTHOR).execute(); dslContext.alterSequence(AUTHOR_ID_SEQ).restart().execute(); dslContext .insertInto( AUTHOR, AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME ) .valuesOfRows( Stream.of(\"홍길동\", \"김철수\", \"박영희\").map(name -&gt; DSL.row( name.substring(0, 1), name.substring(1, 3) )).collect(Collectors.toList()) ) .execute(); // book dslContext.delete(BOOK).execute(); dslContext.alterSequence(BOOK_ID_SEQ).restart().execute(); dslContext .insertInto( BOOK, BOOK.TITLE ) .valuesOfRows( Stream.of( \"이펙티브 자바\", \"리팩토링\", \"DDD START!\", \"토비의 스프링3\", \"Do it! 점프 투 파이썬\", \"혼자 공부하는 머신러닝+딥러닝\", \"개발자를 위한 챗GPT 활용법\", \"C언어로 쉽게 풀어쓴 자료구조\", \"비전공자도 이해할 수 있는 AI 지식\", \"모던 자바스크립트 Deep Dive\" ) .map(DSL::row) .collect(Collectors.toList()) ) .execute(); var rand = new Random(); var authorList = dslContext.selectFrom(AUTHOR).fetch(); var bookList = dslContext.selectFrom(BOOK).fetch(); bookList.forEach(book -&gt; dslContext .insertInto( AUTHOR_BOOK, AUTHOR_BOOK.AUTHOR_ID, AUTHOR_BOOK.BOOK_ID ) .valuesOfRows( DSL.row( authorList.get(rand.nextInt(authorList.size())).get(AUTHOR.ID), book.get(BOOK.ID) ) ) .execute() );} author, book, author_book 테이블의 모든 row를 dslContext.delete(TABLE).excute() 메소드로 삭제합니다. 시퀀스가 존재하는 author, book 테이블의 경우 dslContext.alterSequence(SEQUENCE).restart().execute() 메소드를 통해 시퀀스를 초기화 합니다. 각 테이블에 데이터를 삽입합니다.참고로 dslContext의 경우 spring.datasource를 올바르게 정의하였다면 JooqAutoConfiguration 클래스에 의해 자동으로 bean으로 등록되어 사용할 수 있게 됩니다.2. 조회후 custom class로 결과 반환SELECT b.title, concat(a.first_name, a.last_name)FROM author_book ab LEFT JOIN author a ON ab.author_id = a.id LEFT JOIN book b ON ab.book_id = b.idWHERE a.first_name = '홍';static class Projection { String author; String title;}@Testvoid fetchIntoCustomClass() { var list = dslContext .select( BOOK.TITLE, AUTHOR.FIRST_NAME.concat(AUTHOR.LAST_NAME).as(\"author\") ) .from(AUTHOR_BOOK) .leftJoin(AUTHOR).on(AUTHOR.ID.eq(AUTHOR_BOOK.AUTHOR_ID)) .leftJoin(BOOK).on(BOOK.ID.eq(AUTHOR_BOOK.BOOK_ID)) .where(AUTHOR.FIRST_NAME.eq(\"홍\")) .fetchInto(Projection.class); Assertions.assertTrue(list.stream().allMatch(item -&gt; StringUtils.hasText(item.title) &amp;&amp; item.author.charAt(0) == '홍'));}select \"public\".\"book\".\"title\", (\"public\".\"author\".\"first_name\" || \"public\".\"author\".\"last_name\") as \"author\"from \"public\".\"author_book\" left outer join \"public\".\"author\" on \"public\".\"author\".\"id\" = \"public\".\"author_book\".\"author_id\" left outer join \"public\".\"book\" on \"public\".\"book\".\"id\" = \"public\".\"author_book\".\"book_id\"where \"public\".\"author\".\"first_name\" = '홍'org.jooq.tools.LoggerListener - Fetched result : +-------------------+------+org.jooq.tools.LoggerListener - : |title |author|org.jooq.tools.LoggerListener - : +-------------------+------+org.jooq.tools.LoggerListener - : |토비의 스프링3 |홍길동 |org.jooq.tools.LoggerListener - : |Do it! 점프 투 파이썬 |홍길동 |org.jooq.tools.LoggerListener - : |C언어로 쉽게 풀어쓴 자료구조 |홍길동 |org.jooq.tools.LoggerListener - : |모던 자바스크립트 Deep Dive|홍길동 |org.jooq.tools.LoggerListener - : +-------------------+------+org.jooq.tools.LoggerListener - Fetched row(s) : 4결과가 잘 반환된 것을 확인할 수 있습니다. alias로 필드명을 지정하면 해당 필드에 값이 할당되는 것까지 마음에 드네요.3. updateUPDATE authorSET first_name = '성'WHERE id = 1 or id = 2;@Testvoid update() { var condition = AUTHOR.ID.eq(1).or(AUTHOR.ID.eq(2)); dslContext .update(AUTHOR) .set(AUTHOR.FIRST_NAME, \"성\") .where(condition) .execute(); var list = dslContext.selectFrom(AUTHOR).where(condition).fetchInto(Author.class); Assertions.assertTrue(list.stream().allMatch(item -&gt; item.getFirstName().equals(\"성\")));}update \"public\".\"author\"set \"first_name\" = '성'where ( \"public\".\"author\".\"id\" = 1 or \"public\".\"author\".\"id\" = 2)org.jooq.tools.LoggerListener - Affected row(s) : 2select \"public\".\"author\".\"id\", \"public\".\"author\".\"first_name\", \"public\".\"author\".\"last_name\"from \"public\".\"author\"where ( \"public\".\"author\".\"id\" = 1 or \"public\".\"author\".\"id\" = 2)org.jooq.tools.LoggerListener - Fetched result : +----+----------+---------+org.jooq.tools.LoggerListener - : | id|first_name|last_name|org.jooq.tools.LoggerListener - : +----+----------+---------+org.jooq.tools.LoggerListener - : | 1|성 |길동 |org.jooq.tools.LoggerListener - : | 2|성 |철수 |org.jooq.tools.LoggerListener - : +----+----------+---------+org.jooq.tools.LoggerListener - Fetched row(s) : 2author 테이블의 id가 1 혹은 2인 row의 first_name을 ‘성’ 으로 변경하는 쿼리를 수행하였고 작업이 정상적으로 수행되었습니다.4. deleteDELETEFROM authorWHERE id = 1;@Testvoid delete() { dslContext.delete(AUTHOR).where(AUTHOR.ID.eq(1)).execute(); var authorList = dslContext.selectFrom(AUTHOR).fetchInto(Author.class); var authorBookList = dslContext.selectFrom(AUTHOR_BOOK).fetchInto(AuthorBook.class); Assertions.assertTrue(authorList.stream().noneMatch(item -&gt; item.getId() == 1)); Assertions.assertTrue(authorBookList.stream().noneMatch(item -&gt; item.getAuthorId() == 1));}delete from \"public\".\"author\"where \"public\".\"author\".\"id\" = 1org.jooq.tools.LoggerListener - Affected row(s) : 1select \"public\".\"author\".\"id\", \"public\".\"author\".\"first_name\", \"public\".\"author\".\"last_name\"from \"public\".\"author\"org.jooq.tools.LoggerListener - Fetched result : +----+----------+---------+org.jooq.tools.LoggerListener - : | id|first_name|last_name|org.jooq.tools.LoggerListener - : +----+----------+---------+org.jooq.tools.LoggerListener - : | 2|김 |철수 |org.jooq.tools.LoggerListener - : | 3|박 |영희 |org.jooq.tools.LoggerListener - : +----+----------+---------+org.jooq.tools.LoggerListener - Fetched row(s) : 2select \"public\".\"author_book\".\"author_id\", \"public\".\"author_book\".\"book_id\"from \"public\".\"author_book\"org.jooq.tools.LoggerListener - Fetched result : +---------+-------+org.jooq.tools.LoggerListener - : |author_id|book_id|org.jooq.tools.LoggerListener - : +---------+-------+org.jooq.tools.LoggerListener - : | 3| 1|org.jooq.tools.LoggerListener - : | 3| 2|org.jooq.tools.LoggerListener - : | 2| 4|org.jooq.tools.LoggerListener - : | 2| 5|org.jooq.tools.LoggerListener - : | 3| 7|org.jooq.tools.LoggerListener - : +---------+-------+org.jooq.tools.LoggerListener - Fetched row(s) : 5 (or more)author 테이블의 id가 1인 row를 삭제하였고 author, author_book 테이블의 row가 삭제된것을 확인할 수 있습니다.번외(DAOImpl)JOOQ는 중복되는 불필요한 코드를 줄이기 위해 생성된 DAO에 대한 기본 구현체들을 제공합니다. 스프링 환경에서는 AbstractSpringDAOImpl 클래스를 상속한 DAO를 주입받아 사용할 수 있으며 이 DAO 클래스는 jooq-codegen-maven 플러그인에 설정값을 추가하는것으로 쉽게 생성할 수 있습니다.&lt;generate&gt; &lt;pojos&gt;true&lt;/pojos&gt; &lt;pojosEqualsAndHashCode&gt;true&lt;/pojosEqualsAndHashCode&gt; &lt;javaTimeTypes&gt;true&lt;/javaTimeTypes&gt; &lt;fluentSetters&gt;true&lt;/fluentSetters&gt; &lt;sequences&gt;true&lt;/sequences&gt; &lt;!-- for spring--&gt; &lt;daos&gt;true&lt;/daos&gt; &lt;springDao&gt;true&lt;/springDao&gt; &lt;springAnnotations&gt;true&lt;/springAnnotations&gt;&lt;/generate&gt;doas, spirngDao, springAnnotations 세개의 설정값을 추가하였습니다. 설정값을 추가했다면 코드를 생성하고 관련 클래스들이 생성되었는지 확인합니다.그럼 이제 생성된 DAO들을 주입받고 사용할 수 있습니다.@AutowiredAuthorDao authorDao;@AutowiredAuthorBookDao authorBookDao;@Testvoid select() { var list = authorDao.fetch(Tables.AUTHOR.FIRST_NAME, \"홍\"); Assertions.assertTrue(list.stream().allMatch(item -&gt; item.getFirstName().equals(\"홍\")));}@Testvoid update() { authorDao.update( authorDao .fetchRangeOfId(1, 2) .stream() .peek(item -&gt; item.setFirstName(\"성\")) .collect(Collectors.toList()) ); Assertions.assertTrue(authorDao.fetchRangeOfId(1, 2).stream().allMatch(item -&gt; item.getFirstName().equals(\"성\")));}@Testvoid delete() { authorDao.deleteById(1); Assertions.assertFalse(authorDao.existsById(1)); Assertions.assertTrue(authorBookDao.fetchByAuthorId(1).isEmpty());}확실히 코드가 줄긴 했습니다. 다만 그냥 dslContext를 사용하는 것에 비해 많은 기능(메소드)들이 제거되어 있습니다. 따라서 간단한 쿼리정도에만 사용하고 나머지는 dslContext를 그대로 사용하는게 좋아 보입니다.결론JOOQ에 대해 살펴 보았습니다. 위에서 살펴본 예제는 정말 간단한 CRUD만 테스트 해본 것으로 실제 JOOQ는 보다 많은 DB의 기능을 지원합니다.또한 문서가 매우 잘되어 있어서 처음 시작하시는 분들도 최소한의 삽질로 사용해 보실 수 있을것 같습니다. 만약 ORM을 사용하지 않고 SQL Mapper 기반의 어플리케이션을 개발중이시거나 기존의 Querydsl에 익숙하신 분들이라면 JOOQ를 사용해 보시는게 어떨까요? 유저 매뉴얼: https://www.jooq.org/doc/latest/manual 포스팅에 사용된 코드: https://github.com/keencho/java-sandbox/tree/master/blog-example-code/use-jooq-in-spring-boot" }, { "title": "Web Worker", "url": "/posts/web-workers/", "categories": "Javascript", "tags": "Javascript", "date": "2023-04-13 12:12:00 +0900", "snippet": "Web Worker개요Javascript는 싱글 스레드로 동작합니다. 브라우저 자체는 싱글 스레드로 동작하지 않으며 브라우저는 다음과 같은 스레드들로 구성되어 있습니다. 메인 스레드(Main Thread): 브라우저의 주요 스레드로, HTML, CSS, JavaScript의 처리, 렌더링, 이벤트 처리, 네트워크 요청 처리 등을 담당합니다. 렌더링 스레드(Rendering Thread): 브라우저 창에 내용을 그리는 데 필요한 작업을 처리하는 스레드입니다. 레이아웃 계산, 페인팅 작업 등이 여기에 포함됩니다. 네트워크 스레드(Network Thread): 네트워크 요청과 응답 처리를 담당하는 스레드입니다. JavaScript 엔진 스레드(JavaScript Engine Thread): JavaScript 코드를 처리하는 스레드로, V8 엔진 등의 JavaScript 엔진이 여기에 속합니다. 이벤트 루프 스레드(Event Loop Thread): 비동기 처리를 위한 이벤트 루프를 실행하는 스레드입니다. Web Worker 스레드: 웹 워커를 사용하여 병렬 처리를 지원하기 위해 사용되는 스레드입니다. Web Worker는 메인 스레드와 별개로 실행됩니다.다음과 같이 팩토리얼을 계산하는 함수가 있다고 가정해 봅시다.&lt;body&gt; &lt;button id=\"btn\"&gt;Button&lt;/button&gt;&lt;/body&gt;&lt;script&gt; document .getElementById('btn') .addEventListener('click', function() { alert('You clicked the button.') }); function doFactorial(number) { let result = BigInt(1); for (let i = 1; i &lt;= number; i ++) { result *= BigInt(i); } return result; } doFactorial(100000)&lt;/script&gt;버튼에 이벤트를 바인딩하고 doFactorial() 함수를 호출하여 팩토리얼을 계산하였습니다. 이 코드를 그대로 실행시켜보면 브라우저는 정상적으로 동작하지 않습니다.설령 버튼 UI가 렌더링 되었다고 해도 버튼을 클릭할 수 없을 것입니다. 버튼을 클릭할 수 없는 시간은 doFactorial() 함수의 인자값이 커지면 커질수록 늘어납니다.원인은 간단합니다. 렌더링, 함수 처리가 모두 메인 스레드에서 동작하기 때문입니다. doFactorial() 함수가 메인 스레드에서 동작하고 있기 때문에 그 기간동안은 UI와 관련된 이벤트들은 처리할 수 없게 되는 것이죠.async function doFactorial(number) { let result = BigInt(1); for (let i = 1; i &lt;= number; i ++) { result *= BigInt(i); } return result;}doFactorial(1000000)이번에는 doFactorial() 함수를 비동기 함수로 바꾸고 실행해 봅시다. 이번에는 버튼을 클릭할 수 있을까요?안타깝자만 결과는 동일합니다. 코드 블로킹이 발생하기 때문입니다. 비동기 작업도 일반적으로 메인 스레드에서 동작하기 때문에 매우 큰 계산이나 무한 반복 루프를 실행하여 코드 블로킹이 발생하는 경우 메인 스레드가 차단되어다른 작업을 수행할 수 없게 됩니다.Web worker이럴때 Web worker를 사용하면 됩니다. Web worker는 독립적인 스레드를 사용하기 때문에 메인 스레드와 관계 없이 작업을 수행할 수 있습니다.Web worker ScopeWeb worker는 별도의 스코프에서 실행됩니다. Web worker 스코프는 전역객체 window 를 갖지 않습니다. 따라서 Web worker 에서 DOM API에 접근할 수 없습니다. 만약 UI 업데이트를 수행하고자 한다면 메인 스레드와의 통신이 필요합니다.대신 self 를 사용하여 Web worker만의 스코프인 WorkerGlobalScope 에 접근할 수 있습니다. 또한 importScriopts() 함수를 사용하여 외부 스크립트를 로드하고 해당 스크립트의 전역 변수에 접근할 수 있습니다.ErrorWeb worker 내에서 에러가 발생한 경우 메인 스레드에 전파되지 않습니다. 다만 메인 스레드에서 에러를 확인해야 하는 경우 .onerror() 메소드를 통해 에러를 확인할 수 있습니다.// Web Worker 내에서 onerror 이벤트 핸들러 등록worker.onerror = function(event) { console.error(event.message);};작업 속도 &amp; 자원 점유Web worker의 작업 속도에 영향을 주는 요소는 다음과 같습니다. CPU 성능: Web Worker는 JavaScript 코드를 실행하는 데에 CPU를 사용합니다. 따라서 CPU 성능이 높을수록 Web Worker의 작업 속도가 빨라집니다. 메모리 용량: Web Worker가 사용하는 메모리 용량이 많을수록 브라우저의 성능이 떨어질 수 있습니다. 메모리가 부족한 상황에서는 브라우저가 Web Worker를 중지하거나 작업 속도가 느려질 수 있습니다. 네트워크 대역폭: Web Worker가 네트워크 작업을 수행하는 경우, 네트워크 대역폭이 작을수록 Web Worker의 작업 속도가 느려질 수 있습니다. 특히 대용량 파일을 다운로드하거나 업로드하는 작업을 수행하는 경우에는 이러한 영향이 큽니다. 작업의 복잡도: Web Worker가 수행하는 작업의 복잡도가 높을수록 작업 속도가 느려질 수 있습니다. 예를 들어, 큰 배열을 정렬하는 작업은 더 많은 시간이 소요됩니다. 웹 브라우저의 종류: Web Worker의 작업 속도는 웹 브라우저의 종류에 따라 다를 수 있습니다. 다른 브라우저에서는 작업 속도가 다르게 나타날 수 있습니다.만약 브라우저에서 사용할수 있는 CPU의 최대치가 100이라고 가정해 봅시다. 메인 스레드에서 30의 CPU를 사용한다고 하면, Web worker의 스레드는 얼마만큼의 CPU 자원을 사용할수 있을까요?Web worker는 별도의 스레드에서 동작하기 때문에, 메인 스레드에서의 CPU 점유율과는 독립적으로 동작합니다. 따라서 메인 스레드에서 30의 CPU를 점유하고 있더라도, Web worker 스레드는 여전히 시스템에서 사용 가능한 자원의 최대치인 100까지 사용할 수 있습니다.하지만, 실제로 Web worker에서 사용 가능한 CPU 자원의 양은 시스템 상황에 따라 제한될 수 있습니다. 예를 들어, 운영체제나 브라우저 자체적으로 스레드별 CPU 자원의 할당량을 조정하거나, 시스템의 전체 자원이 부족할 경우 Web worker도 제한될 수 있습니다. 따라서 Web worker를 사용할 때에는, 특정 작업이 얼마나 많은 CPU 자원을 사용하는지 고려하고, 가능하면 여러 개의 Web worker를 사용하여 작업을 분산시키는 것이 좋습니다.장단점장점 멀티 스레드 처리: Web Worker는 메인 스레드와 별개의 스레드에서 동작하므로, 메인 스레드와 별개로 병렬 처리가 가능합니다. 따라서 복잡한 계산이나 대용량 데이터 처리 등을 빠르게 처리할 수 있습니다. 안정성: Web Worker는 메인 스레드와 독립적으로 동작하기 때문에, Web Worker에서 발생하는 에러가 메인 스레드에 영향을 주지 않습니다. 또한 Web Worker는 크래시나 버그로 인해 전체 애플리케이션이 멈추는 것을 방지할 수 있습니다. 사용성: Web Worker는 웹 애플리케이션에서 일반적으로 사용되는 JavaScript와 동일한 문법을 사용합니다. 따라서 개발자가 추가적인 학습 없이 쉽게 사용할 수 있습니다.단점 UI 조작 제한: Web Worker는 메인 스레드와 독립적으로 동작하기 때문에, UI 조작이 불가능합니다. 따라서 Web Worker 내에서 UI 조작을 해야하는 경우, 메인 스레드와 메시지를 주고받아야 합니다. 메모리 사용량: Web Worker는 별도의 스레드에서 동작하기 때문에, 추가적인 메모리를 사용합니다. 따라서 Web Worker를 지속적으로 사용하는 경우, 메모리 사용량이 증가할 수 있습니다. 파일 로딩 제약: Web Worker는 별도의 스레드에서 동작하기 때문에, 메인 스레드와 별개의 파일 로딩을 해야합니다. 따라서 Web Worker에서 로딩할 수 있는 파일의 종류가 제한적입니다.예제간단한 예제를 통해 Web worker의 사용법에 대해 알아보겠습니다. Web worker가 실행될 수 있는 별도의 파일이 필요합니다.// worker.jsself.onmessage = function(event) { console.log('message received'); const number = event.data; let result = BigInt(1); for (let i = 1; i &lt;= number; i ++) { result *= BigInt(i); } self.postMessage(result);}worker 파일에서 self를 이용하여 WorkerGlobalScope 에 접근이 가능합니다. 해당 스코프에 접근하여 메세지를 수신했을때 팩토리얼을 계산하는 코드를 작성하였습니다. 넘겨진 파라미터는 event.data로 접근할 수 있습니다.계산이 끝난 경우 self.postMessage() 메소드를 통해 결과값을 전달합니다.// main.jsif (window.Worker) { const worker = new Worker('worker.js'); worker.postMessage(1000000); worker.onmessage = function(event) { console.log('result: ', event.data); worker.terminate(); }} new Worker() 생성자를 통해 Web worker 객체를 생성합니다. worker.postMessage() 메소드를 통해 파라미터를 전달합니다. worker.onmessage() 메소드를 통해 결과를 전달받고 console에 결과를 기록합니다. 이때 woker에서 파라미터에 접근하는 방식과 동일하게 event.data로 결과에 접근할 수 있습니다. 더 이상 수행할 작업이 없을땐 worker.terminate() 메소드로 worker를 종료합니다." }, { "title": "JPA 에서 Java Record 사용하기", "url": "/posts/use-java-record-in-jpa-hibernate/", "categories": "JPA", "tags": "JPA, Hibernate", "date": "2023-03-09 20:12:00 +0900", "snippet": "JPA 에서 Java Record 사용하기Java Record자바 레코드는 JDK14에서 preview로써 처음 등장했으며 JDK16에서 정식 스펙으로 포함된 기능입니다. 이는 클래스의 기능과 데이터 구조를 결합한 새로운 클래스 유형입니다.레코드는 record 예약어를 사용합니다. 레코드의 구성요소들은 메서드의 매개변수 정의와 동일한 형태의 구문을 사용하여 정의되며 타입과 이름을 포합합니다.예를들어 좌표를 나타내는 레코드는 다음과 같이 정의할 수 있습니다.public record Coordinates(double x, double y) { ...}두 좌표 (x, y)를 보유하는 불변 객체를 생성하며 기존 클래스 형태의 객체에서 일반적으로 정의한 상용어구 코드 (constructor, getter, toString, hashCode, equals)를 정의하지 않아도 기본 구현을 제공하므로 클래스보다 간결한 방법으로 객체를 정의할 수 있습니다.Entity로 사용하기아쉽게도 JPA에서는 Record를 Entity로 사용할 수 없습니다. JPA 사양을 만족하는 엔티티 객체는 다음과 같은 요구사항을 충족해야 하기 때문입니다. @Entity 어노테이션이 선언되어야 합니다. 매개변수가 없는 생성자가 public 혹은 protected으로 선언되어 있어야 합니다. JPA 구현체들이 쿼리 결과를 매핑할때 객체를 인스턴스화 할수 있어야 하기 때문입니다. JPA 구현체가 프록시 객체를 생성해야 하기 때문에 final로 선언되지 않아야 합니다. 엔티티 객체임을 확인하기 위해 하나 혹은 그 이상의 필드를 선언해야 합니다. 필드를 db 컬럼으로 매핑하기 위해서 필드는 final이 아니어야 합니다. 필드에 접근하기 위해 getter와 setter를 제공해야 합니다.레코드는 매개 변수가 없는 생성자를 지원하지 않으며 클래스 자체가 final 클래스입니다. 레코드의 필드들 또한 final이며, 접근 메소드들 또한 요구사항에 적합한 네이밍 전략을 따르지 않습니다. 이러한 이유들 때문에 아직까지 JPA 에서는 Record를 Entity로 사용할 수 없습니다.DTO로 사용하기조회한 정보를 변경하지 않는다면 DTO는 최고의 선택입니다. 이는 엔티티를 직접 조회하는 것보다 성능이 뛰어나며 도메인 모델과 API를 분리할 수 있습니다.JPA의 Criteriabuilder.construct 메소드를 사용하여 CriteriaQuery에서 생성자 호출을 정의할 수 있습니다.public record OrderRecord( String id, OrderStatus status, String fromAddress, String fromName, String fromPhoneNumber, String toAddress, String toName, String toPhoneNumber, String itemName, int itemPrice, LocalDateTime createdDateTime) { }@Testpublic void recordDTO() { var cb = entityManager.getCriteriaBuilder(); var cq = cb.createQuery(OrderRecord.class); var root = cq.from(Order.class); cq.select( cb.construct( OrderRecord.class, root.get(Order_.id), root.get(Order_.status), root.get(Order_.fromAddress), root.get(Order_.fromName), root.get(Order_.fromPhoneNumber), root.get(Order_.toAddress), root.get(Order_.toName), root.get(Order_.toPhoneNumber), root.get(Order_.itemName), root.get(Order_.itemPrice), root.get(Order_.createdDateTime) ) ); cq.where(cb.notEqual(root.get(Order_.status), OrderStatus.FAILED)); var q = entityManager.createQuery(cq); var list = q.getResultList();}임베디드 객체로 사용하기Hibernate &lt; 6.0Hiberante 6.0 이전 버전에서는 record 객체를 임베디드 객체로 사용할 수 없습니다. 이유는 위의 record 객체를 entity로 사용할 수 없는 이유와 같습니다.Hibernate &gt;= 6.0Hibernate는 6 버전에서 EmbeddableInstantiator 기능을 소개하였습니다. 이 기능을 통해 enbeddable 객체를 보다 유연하게 인스턴스화 할수 있게 되었습니다. 이에대한 사이드 이펙트로 record 객체를 입데디드 객체로 사용할 수 있게 되었습니다.@Entity@Table(name = \"order_new\")public class Order { ... @Embedded ShippingInfo fromInfo; @Embedded ShippingInfo toInfo; ...}@Embeddablepublic record ShippingInfo( String name, String address, String number) { }Order 엔티티에 fromInfo와 toInfo 라는 배송지 정보를 임베디드 객체로써 선언하였습니다. :warning: 위 예제가 동작하려면 네이밍 전략이 ImplicitNamingStrategyComponentPathImpl 이어야 합니다. (&lt;property name=”hibernate.implicit_naming_strategy” value=”org.hibernate.boot.model.naming.ImplicitNamingStrategyComponentPathImpl” /&gt;)ShippingInfo 객체를 임베디드 객체로써 사용하기 위해 이를 하이버네이트에 알려야 합니다. 이를 위해 EmbeddableInstatiator을 커스텀 하겠습니다.public class ShippingInfoInstantiator implements EmbeddableInstantiator { @Override public Object instantiate(ValueAccess valueAccess, SessionFactoryImplementor sessionFactory) { var name = valueAccess.getValue(0, String.class); var address = valueAccess.getValue(1, String.class); var street = valueAccess.getValue(2, String.class); return new ShippingInfo(name, address, street); } @Override public boolean isInstance(Object object, SessionFactoryImplementor sessionFactory) { return object instanceof ShippingInfo; } @Override public boolean isSameClass(Object object, SessionFactoryImplementor sessionFactory) { return object.getClass().equals(ShippingInfo.class); }}위 간단한 클래스에서 가장 중요한 것은 instantiate 메소드 입니다. 하이버네이트는 객체의 모든 필드 정보를 알파벳 순서로 담고 있는 ValueAccess 객체를 호출합니다. ValudeAccess객체에 인덱스로 접근하여 값을 원하는 유형으로 캐스팅 할 수 있습니다. 객체 인스턴스화에 필요한 모든 매개 변수를 추출한 후 마지막에 모든 매개 변수를 담고 있는 생성자를 생성하기 때문에 record를 임베디드 객체로써 사용할 수 있게 되는 것입니다.@Embeddable@EmbeddableInstantiator(ShippingInfoInstantiator.class)public record ShippingInfo( String name, String address, String number) { }위 코드처럼 @EmbeddableInstantiator 어노테이션을 통해 커스텀한 EmbeddableInstatiator 클래스를 지정하기만 하면 끝입니다.이제 이 record 임베디드 객체가 잘 동작하는지 조회해 보도록 하겠습니다.public record OrderRecord( String id, OrderStatus status, ShippingInfo fromShippingInfo, ShippingInfo toShippingInfo, String itemName, int itemPrice, LocalDateTime createdDateTime) { }결과를 반환받기 위한 dto 객체입니다.@Testpublic void recordDTO() { var cb = entityManager.getCriteriaBuilder(); var cq = cb.createQuery(OrderRecord.class); var root = cq.from(Order.class); cq.select( cb.construct( OrderRecord.class, root.get(Order_.id), root.get(Order_.status), root.get(\"fromInfo\"), root.get(\"toInfo\"), root.get(Order_.itemName), root.get(Order_.itemPrice), root.get(Order_.createdDateTime) ) ); cq.where(cb.like(root.get(\"fromInfo\").get(\"name\"), \"%김%\")); var q = entityManager.createQuery(cq); var list = q.getResultList();}테스트 코드입니다. record 객체에는 setter 메소드를 작성할 수 없어 jpamodelgen 프로세서가 접근할 수 없습니다. 따라서 부득이하게 string 형태로 임베디드 객체에 접근하였습니다.select o1_0.id, o1_0.status, o1_0.fromInfo_address, o1_0.fromInfo_name, o1_0.fromInfo_number, o1_0.toInfo_address, o1_0.toInfo_name, o1_0.toInfo_number, o1_0.itemName, o1_0.itemPrice, o1_0.createdDateTime from order_new o1_0 where o1_0.fromInfo_name like ? escape ''정상적으로 sql문이 생성되었음을 확인할 수 있습니다.Hibernate &gt;= 6.2Hibernate 6.2 버전부터는 커스텀 EmbeddableInstatiator 클래스조차 필요 없게 되었습니다. record 객체에 @Embeddable 객체를 붙이는 것만으로도 record 객체를 임베디드 객체로 사용할 수 있습니다.@Embeddablepublic record ShippingInfo( String name, String address, String number) { }물론 기존과 같이 커스텀 EmbeddableInstatiator 를 만들어 지정할 수도 있습니다." }, { "title": "Hibernate 에서 PostgreSQL JSONB 다루기", "url": "/posts/hibernate-postgresql-jsonb/", "categories": "Hibernate", "tags": "Hibernate, PostgreSQL", "date": "2023-02-26 20:12:00 +0900", "snippet": "Hibernate 에서 PostgreSQL JSONB 다루기Hibnerate6 이전 버전에서 PostgreSQL의 JSONB 타입을 다루기 위해선 직접 UserType 인터페이스를 구현하거나 이런 라이브러리 를 사용하여 JSONB 타입을 다뤘었습니다.Hibernate6 부터는 이러한 기능을 표준으로 제공하기 시작하여 JSON 컬럼을 entity의 속성으로서 사용할 수 있게 되었습니다. :warning: 아래 내용은 Hibernate6 이상의 버전을 필요로 합니다. Hibernate6 이전 버전을 사용하는 경우 (Hibernate 5.x, Spring Boot 2.x, Spring Data JPA 2.x…) 그냥 hypersistence-utils를 사용하시길 권장드립니다.@JdbcTypeCode@JdbcTypeCode 어노테이션을 필드에 붙여주기만 하면 Hibernate는 RDB의 종류에 따라 자동으로 JSON 컬럼을 정의합니다. PostgreSQL 경우엔 JSONB 타입이 되겠죠. 런타임에는 직렬화 / 역직렬화 할 수 있는 JSON 라이브러리를 사용하여 값을 컨트롤 합니다.@Datapublic class ShippingInfo implements Serializable { private String name; private String number; private String address;}@Entity@Data@Table(name = \"order_new\")public class Order { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; @JdbcTypeCode(SqlTypes.JSON) ShippingInfo fromInfo; @JdbcTypeCode(SqlTypes.JSON) ShippingInfo toInfo;}[DEBUG] [main] SQL - create table order_new ( id bigint not null, fromInfo jsonb, toInfo jsonb, primary key (id) )UserType 인터페이스 구현UserType 인터페이스를 통해 직접 타입을 구현하는 방법도 있습니다. UserType 인터페이스는 Hibernate6 버전 이전에도 존재했었습니다. 다만 Hibernate6 버전으로 올라오면서 이 인터페이스는 제네릭을 사용하도록 변경되었습니다.Hibernate5 - UserTypeHibernate6 - UserType뿐만 아니라 기존에는 @TypeDef 어노테이션으로 사용할 JSON 타입 클래스를 먼저 등록하고 @Type 어노테이션을 사용했었어야 했다면, 이제 @Type 어노테이션만 사용하면 되도록 변경되었습니다. (@TypeDef 어노테이션은 삭제 되었습니다.)@Entity@Data@Table(name = \"order_new\")@TypeDef(name = \"ShippingInfoJsonType\", typeClass = ShippingInfoJsonType.class)public class Order { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; @Column @Type(type = \"ShippingInfoJsonType\") ShippingInfo fromInfo; @Column @Type(type = \"ShippingInfoJsonType\") ShippingInfo toInfo;}@Entity@Data@Table(name = \"order_new\")public class Order { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; @Column @Type(ShippingInfoJsonType.class) ShippingInfo fromInfo; @Column @Type(ShippingInfoJsonType.class) ShippingInfo toInfo;}조금이나마 코드가 간결해지고 type-safe 하게 변경된 것을 확인할 수 있습니다.사용법은 위와같고 UserType 인터페이스 구현 방법은 기존과 동일합니다. 필요한 메소드들을 제네릭 타입에 맞게 재정의 하면 됩니다.public class ShippingInfoJsonType implements UserType&lt;ShippingInfo&gt; { @Override public int getSqlType() { return SqlTypes.JSON; } @Override public Class&lt;ShippingInfo&gt; returnedClass() { return ShippingInfo.class; } ...}JSONB 컬럼의 필드 업데이트setter 메소드를 사용해 JSONB 컬럼의 필드값을 변경하면 트랜잭션 종료 이후 Hibernate에 의해 값이 업데이트 됩니다.private static void doInTransaction(Runnable task) { entityManager.getTransaction().begin(); task.run(); entityManager.getTransaction().commit();}...doInTransaction(() -&gt; { var order = entityManager.find(Order.class, 1L); order.getFromInfo().setNumber(\"01000001111\");});sql 로그는 다음과 같습니다.[DEBUG] [main] SQL - update order_new set fromInfo=?, toInfo=? where id=?[TRACE] [main] bind - binding parameter [1] as [JSON] - [ShippingInfo(name=석설수, number=01000001111, address=서울특별시 구로구 개포로109길)][TRACE] [main] bind - binding parameter [2] as [JSON] - [ShippingInfo(name=성인건, number=01018657836, address=서울특별시 금천구 개포로34길)][TRACE] [main] bind - binding parameter [3] as [BIGINT] - [1]id가 1인 엔티티를 찾고 받는쪽 배송정보 JSONB 컬럼의 번호를 01000001111 로 변경하였습니다. 정상적으로 잘 업데이트 된 것을 확인할 수 있습니다.JSONB 조건검색그렇다면 받는쪽 배송정보 JSONB 컬럼의 성이 김 으로 시작하는 row를 검색하려면 어떻게 해야 할까요? sql로 표현하면 다음과 같습니다.select *from order_new owhere o.frominfo -&gt;&gt; 'name' like '김%';Hibernate &lt; 6.2Hibernate 6.2 이전 버전에서는 아쉽게도 JPQL로 위 sql을 표현할 수 없습니다. 어쩔수 없이 createNativeQuery(query) 메소드를 사용할 수 밖에 없습니다.entityManager.createNativeQuery(\"SELECT * FROM order_new o WHERE o.frominfo -&gt;&gt; 'name' LIKE :name \") .setParameter(\"name\", \"김%\") .getResultList()[DEBUG] [main] SQL - SELECT * FROM order_new o WHERE o.frominfo -&gt;&gt; 'name' LIKE ?[TRACE] [main] bind - binding parameter [1] as [VARCHAR] - [김%]Hibernate &gt;= 6.2Hibernate 6.2 버전에서 드디어 JPQL로 JSONB 컬럼을 탐색할 수 있게 되었습니다. 정확히는 Embeddable 객체(클래스)를 JSON 컬럼으로써 매핑 할수 있게 되었습니다.@Data@Embeddablepublic class ShippingInfo implements Serializable { private String name; private String number; private String address;}일단 ShippingInfo 클래스에 @Embeddable 어노테이션을 붙여 JPA가 이 클래스를 인식할 수 있도록 합니다.@Test@DisplayName(\"JPQL JSONB 조건 테스트 (version &gt;= 6.2\")public void jsonbJPQLCondition() { var orderList = entityManager .createQuery(\"SELECT o FROM Order o WHERE o.fromInfo.name LIKE :name\", Order.class) .setParameter(\"name\", \"김%\") .getResultList(); Assertions.assertTrue(orderList.stream().allMatch(o -&gt; o.getFromInfo().getName().startsWith(\"김\")));}[DEBUG] [main] SQL - select o1_0.id, o1_0.fromInfo, o1_0.toInfo from order_new o1_0 where cast(o1_0.fromInfo-&gt;&gt;'name' as varchar(255)) like ? escape ''[TRACE] [main] bind - binding parameter [1] as [VARCHAR] - [김%]테스트 코드와 sql 로그입니다. Hibernate가 JPQL을 올바른 sql로 변환했네요.QueryDSLJPQL로 JSON 컬럼을 탐색할 수 있다는 것은 QueryDSL에서도 사용할 수 있게 되었다는 것을 의미합니다.이번에는 QueryDSL로 조금 복잡한 쿼리를 구현해 보도록 하겠습니다. 조건은 다음과 같습니다. 보내는 분의 성은 김이 아니어야 함 보내는 분의 성으로 결과를 그루핑함 결과를 내림차순으로 정렬하여 어떤 성이 가장 많은지 확인 위 조건을 sql로 표현하면 다음과 같습니다.select substr(o.frominfo -&gt;&gt; 'name', 1, 1), count(*)from order_new owhere o.frominfo -&gt;&gt; 'name' not like '김%'group by substr(o.frominfo -&gt;&gt; 'name', 1, 1)order by count(*) desc;실무에서 이런 요구사항이 던져진다면 ‘그냥 날쿼리 쓸까?’ 라는 생각이 들곤 합니다. 그러나 이제는 QueryDSL로 이런 쿼리도 표현할 수 있게 되었습니다.일단 조회결과로 반환될 dto를 만들어 줍니다.@Datapublic class OrderAggregationDTO { private String lastName; private int count; @QueryProjection public OrderAggregationDTO(String lastName, int count) { this.lastName = lastName; this.count = count; }}다음으로 QueryDSL 코드를 작성합니다.public void jsonbQueryDSLCondition() { var q = QOrder.order; var lastName = q.fromInfo.name.substring(0, 1); var count = q.count(); var list = jpaQueryFactory() .select(new QOrderAggregationDTO(lastName, count)) .from(q) .where(q.fromInfo.name.startsWith(\"김\").not()) .groupBy(lastName) .orderBy(count.desc()) .fetch(); list.forEach(item -&gt; System.out.printf(\"성: %s / 갯수: %d개%n\", item.getLastName(), item.getCount()));}[DEBUG] [main] SQL - select substr(cast(o1_0.fromInfo-&gt;&gt;'name' as varchar(255)),1,1), count(o1_0.id) from order_new o1_0 where cast(o1_0.fromInfo-&gt;&gt;'name' as varchar(255)) not like ? escape '!' group by substr(cast(o1_0.fromInfo-&gt;&gt;'name' as varchar(255)),1,1) order by count(o1_0.id) desc[TRACE] [main] bind - binding parameter [1] as [VARCHAR] - [김%]성: 주 / 갯수: 6개성: 변 / 갯수: 4개성: 신 / 갯수: 4개성: 진 / 갯수: 4개성: 탁 / 갯수: 4개성: 박 / 갯수: 3개성: 조 / 갯수: 3개성: 엄 / 갯수: 3개성: 노 / 갯수: 3개성: 오 / 갯수: 3개성: 석 / 갯수: 2개성: 홍 / 갯수: 2개성: 차 / 갯수: 2개성: 선 / 갯수: 2개성: 심 / 갯수: 2개성: 표 / 갯수: 2개성: 우 / 갯수: 2개성: 손 / 갯수: 2개성: 남 / 갯수: 2개성: 도 / 갯수: 2개성: 소 / 갯수: 2개성: 은 / 갯수: 2개성: 허 / 갯수: 2개성: 안 / 갯수: 2개성: 전 / 갯수: 2개성: 유 / 갯수: 2개성: 이 / 갯수: 2개성: 곽 / 갯수: 1개성: 편 / 갯수: 1개성: 한 / 갯수: 1개성: 황 / 갯수: 1개성: 권 / 갯수: 1개성: 현 / 갯수: 1개성: 용 / 갯수: 1개성: 원 / 갯수: 1개성: 성 / 갯수: 1개sql 로그와 결과를 확인합니다. 이번에도 JPQL이 올바른 sql로 변환된 것을 확인할 수 있습니다.결론Hibernate6 버전부터 쉽게 JSON 타입의 컬럼을 다를 수 있게 되었습니다. 뿐만 아니라 6.2 버전부터는 조회 또한 JPQL로 표현할 수 있게 되었는데요,이를 통해 만약 QueryDSL을 사용한다면 querydsl-sql, blaze-persistence 를 사용하지 않고 querydsl-jpa만으로 type-safe한 쿼리를 작성할 수 있게 되었습니다.또한 CTE 와 Partitioning 등 새로운 기능들이 Hibernate 6.2 버전에서 선보여질 예정입니다.JPA의 태생적인 한계를 Hibernate를 통해 하나 둘 해결할 수 있게 되었으니 앞으로는 개발자가 조금더 쉽고 간단하게 자바로 db를 컨트롤 할 수 있을것 같습니다." }, { "title": "동일한 클래스 안에서 새로운 트랜잭션 생성하기", "url": "/posts/spring-new-transaction-in-same-class/", "categories": "Spring", "tags": "Spring, JPA", "date": "2023-02-19 20:12:00 +0900", "snippet": "동일한 클래스 안에서 새로운 트랜잭션 생성하기동일한 클래스 내에서 새로운 트랜잭션을 만들어 예외를 회피하려고 하는 경우, 의도했던 것과는 다른 결과가 나올 수 있습니다.@Transactionalpublic void answer(Inquiry inquiry) { System.out.println(TransactionSynchronizationManager.getCurrentTransactionName()); try { this.updateAnswer(inquiry); } catch (Exception ignored) { } try { this.updateAnswerer(inquiry); } catch (Exception ignored) { }}@Transactional(propagation = Propagation.REQUIRES_NEW)public void updateAnswer(Inquiry inquiry) { System.out.println(TransactionSynchronizationManager.getCurrentTransactionName()); // update answer}@Transactionalpublic void updateAnswerer(Inquiry inquiry) { System.out.println(TransactionSynchronizationManager.getCurrentTransactionName()); // update answerer // exception}예를들어 위 코드와 같이 updateAnswer 로직을 새로운 트랜잭션으로 처리한다고 가정합니다.따라서 updateAnswererAndThrowException 메소드에서 예외가 발생하더라도 별도의 트랜잭션에서 update 작업이 수행되었기 때문에 값이 정상적으로 저장되었을 것이라 생각했습니다.그러나 작업이 끝난 이후 조회해보니 값이 업데이트 되지 않은 것을 확인하였습니다. 아무리봐도 코드에 이상한점은 없어 TransactionSynchronizationManager.getCurrentTransactionName() 를 통해 각 메소드로 진입할때 트랜잭션 이름을 확인해보기로 하였습니다.com.keencho.application.service.InquiryService.answercom.keencho.application.service.InquiryService.answercom.keencho.application.service.InquiryService.answerupdateAnswer 메소드의 전파레벨을 REQUIRES_NEW로 지정했음에도 새로운 트랜잭션이 시작되지 않았습니다.원인원인은 스프링 공식 문서 에 설명되어 있습니다. In proxy mode (which is the default), only external method calls coming in through the proxy are intercepted. This means that self-invocation (in effect, a method within the target object calling another method of the target object) does not lead to an actual transaction at runtime even if the invoked method is marked with @Transactional. Also, the proxy must be fully initialized to provide the expected behavior, so you should not rely on this feature in your initialization code — for example, in a @PostConstruct method.문제는 Spring의 AOP 프록시는 확장되지 않고 외부에서의 호출을 가로채기 위해 서비스 인스턴스를 감싼다는 것입니다.만약 서비스 내부에서 this 통해 다른 메소드를 호출한다면 이는 인스턴스에 의해 바로 호출되며 래핑 프록시는 이를 가로챌 수 없습니다. (당연히 프록시는 이러한 인스턴스 내부의 호출을 전혀 인식하지 못합니다.)해결해결 방법은 다음과 같습니다. Spring AOP 대신 AspectJ 사용 self-injection 클래스 분리AOP 라이브러리를 변경하는것은 주의가 필요합니다. 예상하는 결과와 다른 결과를 받을 수 있습니다.self-injection은 클래스 내부에서 자기 자신을 또 주입하는 방법입니다.@Servicepublic class InquiryService { @Lazy @Autowired private InquiryService self; ...}다만 bean 이름이 겹쳐 등록이 안될수도 있으니 위와같이 @Lazy 어노테이션과 함께 사용해야 합니다.사실 새로운 트랜잭션이 필요한 경우 클래스를 분리하여 Spring AOP가 호출을 가로채 새로운 트랜잭션을 시작할 수 있도록 하는게 가장 좋은 방법입니다. 다만 클래스가 분리됨에 따라 관리 포인트가 조금 늘어날 수 있다는 단점이 있습니다.그 외에도 람다를 사용한 이런 방법도 사용할 수 있습니다.어플리케이션의 전체적인 설계를 망치지 않는 범위에서 방법을 찾아 적용해 보시기 바랍니다." }, { "title": "Spring Data JPA - Custom QueryDSL Repository 만들기", "url": "/posts/spring-data-jpa-custom-querydsl-repository/", "categories": "JPA", "tags": "JPA, Spring Data JPA, QueryDSL", "date": "2023-01-31 20:12:00 +0900", "snippet": "Spring Data JPA - Custom QueryDSL Repository 만들기Spring Data JPA를 사용한다면 JpaRepository를 상속한 repository를 만들고 미리 정해진 키워드 들을 사용하여 메소드(쿼리)를 사용하곤 합니다.그러나 메소드의 길이가 길어진다면 (findByNameAndAgeAndGenderAndPhoneNumberAnd…) 여간 보기싫은게 아닙니다. 물론 @Query어노테이션을 사용하여 직접 쿼리를 작성할 수 있지만 sql을 직접 작성해야 한다는 단점이 존재합니다.또한 QueryDSL을 사용하고자 한다면 이를 사용할 계층이 필요합니다. service 계층에 관련 메소드를 작성할 수도 있겠지만 설계 측면에서 봤을때 쿼리는 repository 계층에서 작성하는게 좋겠죠.Customizing Individual Repositories개별 repository를 확장하고 필요한 쿼리를 작성하기 위해(Query DSL을 사용하기 위해) 새로운 커스텀 클래스가 필요합니다. 다행히 Spring Data JPA 공식문서에는 관련 내용과 적용 방법이 자세히 적혀 있습니다.커스텀 인터페이스를 만들고 해당 인터페이스를 구현하는 클래스를 만든 후에 기존 repository가 커스텀 인터페이스를 상속하게 하면 된다고 하네요.// 기존 repositorypublic interface CustomerRepository extends JpaRepository&lt;Customer, UUID&gt; {}// custom interfacepublic interface CustomCustomerRepository { List&lt;Customer&gt; findVIPCustomer();}// custom interface를 구현하는 클래스@Componentpublic interface CustomCustomerRepositoryImpl implements CustomCustomerRepository { @PersistenceUnit EntityManagerFactory emf; @Autowired JPAQueryFactory queryFactory; @Override List&lt;Customer&gt; findVIPCustomer() { ... }}// 기존 repository 수정public interface CustomerRepository extends JpaRepository&lt;Customer, UUID&gt;, CustomCustomerRepository {}보통은 위와같이 인터페이스, 클래스를 작성하여 repository를 확장해서 사용하며 QueryDSL이 필요한 경우 JPAQueryFactory를 주입받아 사용하곤 합니다. :warning: 공식 문서에 의하면 custom interface를 구현하는 클래스는 Impl을 접미사로 붙여야 한다고 나와 있습니다. @EnableJpaRepositories(repositoryImplementationPostfix = “MyPostfix”) 어노테이션을 사용하면 접미사를 변경할 수도 있습니다.QueryDSL을 모듈화하여 공통으로 사용하기QueryDSL을 사용한다면 뭔가 아쉽습니다. 동일한 형태의 인터페이스, 클래스를 repository마다 만들어서 보일러플레이트 코드가 늘어나기 때문입니다.Spring Data JPA는 이때 사용할 수 있는 QuerydslPredicateExecutor 라는 인터페이스를 제공합니다.하지만 해당 인터페이스를 살펴보면 조회 결과를 DTO로 반환하거나 명시적 join을 사용할 수 없다는 단점이 존재합니다. QuerydslRepositorySupport 추상클래스를 상속하여 사용하는 방법도 있지만 해당 방법도 한계는 존재합니다.이러한 단점들을 극복한 CustomRepository를 만들어 보고자 합니다. 기존의 JpaRepository처럼 인터페이스 상속만으로 사용 가능하게 하는것이 목표입니다.@FunctionalInterfacepublic interface QueryHandler { JPAQuery&lt;?&gt; apply(JPAQuery&lt;?&gt; query);}public interface CustomJpaSearchQuery&lt;T&gt; { List&lt;T&gt; findList(Predicate predicate, QueryHandler queryHandler, Sort sort); Page&lt;T&gt; findPage(Predicate predicate, QueryHandler queryHandler, Pageable pageable); &lt;P&gt; List&lt;P&gt; selectList(Predicate predicate, Class&lt;P&gt; type, Map&lt;String, Expression&lt;?&gt;&gt; bindings, QueryHandler queryHandler, Sort sort); &lt;P&gt; Page&lt;P&gt; selectPage(Predicate predicate, Class&lt;P&gt; type, Map&lt;String, Expression&lt;?&gt;&gt; bindings, QueryHandler queryHandler, Pageable pageable);}구현해야할 인터페이스는 위와 같습니다. 조회 결과를 entity 타입으로 받거나 Projection(DTO)으로 받을 수 있는 메소드 들입니다. 페이징도 지원합니다.public class DefaultCustomJpaSearchQuery&lt;T&gt; implements CustomJpaSearchQuery&lt;T&gt; { private final EntityManager entityManager; private final JPAQueryFactory queryFactory; private final EntityPath&lt;T&gt; path; // querydsl-apt 에 의해 생성된 q 클래스가 존재할 경우 -&gt; path로 q 클래스 사용 public DefaultCustomJpaSearchQuery(JpaEntityInformation&lt;T, ?&gt; entityInformation, EntityManager entityManager) { this.entityManager = entityManager; this.queryFactory = new JPAQueryFactory(entityManager); this.path = SimpleEntityPathResolver.INSTANCE.createPath(entityInformation.getJavaType()); } // 생성된 q 클래스가 없거나 이를 사용할 수 없는 경우 (ex. unit test) - 새로운 entity path 생성 public DefaultCustomJpaSearchQuery(Class&lt;T&gt; type, EntityManager entityManager) { this.entityManager = entityManager; this.queryFactory = new JPAQueryFactory(entityManager); this.path = new EntityPathBase&lt;&gt;(type, \"entity\"); } ...}CustomJpaSearchQuery를 구현한 클래스입니다. JpaQueryFactory와 EntityPath를 클래스 생성 시점에 정의하였습니다. 메소드들은 글이 너무 길어지기 때문에 포함하지 않았습니다. 전체 코드는 블로그 하단의 링크에서 확인하실 수 있습니다.다음으로 CustomJpaSearchQuery 인터페이스를 확장하는 인터페이스를 만들어줍니다. 이 인터페이스는 CustomJpaSearchQuery 뿐만 아니라 JpaRepository인터페이스도 확장하여 기존의 기능을 사용할 수 있는 인터페이스 입니다.@NoRepositoryBeanpublic interface CustomJpaQueryDSLRepository&lt;T, ID&gt; extends JpaRepository&lt;T, ID&gt;, CustomJpaSearchQuery&lt;T&gt; {}@NoRepositoryBean 어노테이션을 붙여 최종적으로 생성되는 repository에 @Repository 어노테이션을 붙이지 않아도 동작하도록 하였습니다.위 인터페이스는 한가지 문제가 있습니다. 기본적으로 repository의 구현체는 JpaRepositoryFactoryBean 에 의해 결정되는데 아무런 설정도 하지 않았기 때문에 위에서 만든 구현체는 당연히 repository의 구현체로서 인식(동작)하지 않습니다. 이 구현체를 인식하게 하기 위해서는 Custom Factory Bean이 필요합니다.public class CustomJpaQueryDSLRepositoryFactory&lt;T extends Repository&lt;E, ID&gt;, E, ID&gt; extends JpaRepositoryFactoryBean&lt;T, E, ID&gt; { public CustomJpaQueryDSLRepositoryFactory(Class&lt;? extends T&gt; repositoryInterface) { super(repositoryInterface); } @Override protected RepositoryFactorySupport createRepositoryFactory(EntityManager entityManager) { return new QueryDSLRepositoryFactory(entityManager); } private static class QueryDSLRepositoryFactory extends JpaRepositoryFactory { private final EntityManager entityManager; public QueryDSLRepositoryFactory(EntityManager entityManager) { super(entityManager); this.entityManager = entityManager; } @Override protected RepositoryComposition.RepositoryFragments getRepositoryFragments(RepositoryMetadata metadata) { var fragments = super.getRepositoryFragments(metadata); if (CustomJpaQueryDSLRepository.class.isAssignableFrom(metadata.getRepositoryInterface())) { var impl = super.instantiateClass( DefaultCustomJpaSearchQuery.class, this.getEntityInformation(metadata.getDomainType()), this.entityManager ); fragments = fragments.append(RepositoryFragment.implemented(impl)); } return fragments; } }}Bean이 생성되는 시점에 repository를 검색하여 만약 repository가 CustomJpaQueryDSLRepository 인터페이스를 확장하고 있다면 DefaultCustomJpaSearchQuery 클래스를 인터페이스의 구현체로 지정하고 repository 로써 동작하게 하는 Custom Factory Bean 클래스 입니다.이제 Custom Factory Bean 클래스를 사용하겠다고 선언만 하면 됩니다.@SpringBootApplication@EnableJpaRepositories(repositoryFactoryBeanClass = CustomJpaQueryDSLRepositoryFactory.class)public class SpringBootApplication { ...}설정은 모두 끝났습니다. CustomJpaQueryDSLRepository를 상속하여 사용하면 끝입니다. 당연히 기존처럼 메소드 기반으로 쿼리를 생성하여 사용할 수도 있습니다.public interface OrderRepository extends CustomJpaQueryDSLRepository&lt;Product, UUID&gt; { List&lt;Order&gt; findByProductPriceGreaterThanOrderByNameDesc(Long price);}아래는 사용 예시 입니다.private Map&lt;String, Expression&lt;?&gt;&gt; buildBindings() { var oq = QOrder.order; var cq = QCustomer.customer; var bindings = new HashMap&lt;String, Expression&lt;?&gt;&gt;(); bindings.put(\"id\", oq.id); bindings.put(\"name\", oq.name); bindings.put(\"dtCreatedAt\", oq.dtCreatedAt); bindings.put(\"customerId\", cq.id); bindings.put(\"customerName\", cq.name); bindings.put(\"customerAge\", cq.age); bindings.put(\"customerGender\", cq.gender); return bindings;} private void select() { var predicate = new BooleanBuilder(); predicate.and(oq.product.price.gt(3000L)); var customRepositorySelectList = orderRepository.selectList( predicate, OrderDTO.class, this.buildBindings(), query -&gt; query.leftJoin(cq).on(cq.id.eq(oq.customerId)), new QSort(oq.name.desc()) ); ...}:memo: 전체 코드 및 테스트는 링크에서 확인하실 수 있습니다." }, { "title": "Hibernate, Spring Data JPA, QueryDSL 에서 UNION 사용하기", "url": "/posts/jpa-union/", "categories": "JPA", "tags": "JPA, Hibernate, Spring Data JPA, QueryDSL", "date": "2023-01-15 14:12:00 +0900", "snippet": "JPA 에서 union 사용하기JPA 환경에서 집합 연산자(UNION, UNION ALL, INTERSECT, EXCEPT)를 사용할 수 있을까요? JPA의 버전이 3.1까지 올라왔어도 아직 이에대한 명세, 지원은 없는 것으로 보입니다.native sql로 작성하면 안되는 쿼리는 없긴 합니다만, 추상화된 SQL을 사용할 수 없다는 단점이 존재합니다.Hibernate가 이에 대한 해답이 될 수 있는데요, Hibernate6 버전부터는 집합 연산자들을 지원하기 시작했으며 QueryDSL은 JPASQLQuery 클래스를 사용하면 일반적인 native sql보다는 type-safe 하게 쿼리 작성이 가능합니다.이 포스팅에서는 간단한 예제로 JPA 환경에서 UNION을 사용하는 방법에 대해 알아보고자 합니다.0. 문제 확인 &amp; 예제 세팅Order 라는 파티션 테이블이 존재해야 하는데 모종의 이유로 파티션 테이블을 사용하지 못하고 주기적으로 테이블을 직접 분리해야 하는 상황이라고 가정합니다.요청받은 데이터의 조건은 다음과 같습니다. Order 테이블을 상속하는 모든 테이블의 주문을 합쳐 주세요. 22년 6월 데이터는 받는분의 성씨가 ‘김’씨인 주문만 조회해 주세요. 23년 1월 데이터는 상품가격이 10만원 이상인 주문만 조회해 주세요. 합쳐진 주문에서 상태값이 FAILED인 주문은 제외해 주세요. 이를 sql로 표현하면 다음과 같이 간단히 표현할 수 있습니다. (PostgreSQL 기준)select o.*from ( select * from order_2206 where order_2206.toname like '김%' union all select * from order_2209 union all select * from order_2301 where order_2301.itemprice &gt;= 100000 ) as owhere status &lt;&gt; 'FAILED'FROM 절에 서브쿼리를 써야하고 그 서브쿼리는 UNION ALL 을 사용합니다. 이제 위 sql문을 JPA로 옮겨보려고 합니다.다음은 테스트에 사용될 라이브러리 &amp; 엔티티 입니다.dependencies { implementation group: 'org.postgresql', name: 'postgresql', version: '42.5.1' implementation 'org.hibernate.orm:hibernate-core:6.2.0.Final' implementation group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.19.0' implementation 'com.querydsl:querydsl-jpa:5.0.0:jakarta' implementation 'com.querydsl:querydsl-sql:5.0.0' annotationProcessor 'com.querydsl:querydsl-apt:5.0.0:jakarta' annotationProcessor 'jakarta.persistence:jakarta.persistence-api:3.1.0' implementation 'org.projectlombok:lombok:1.18.24' annotationProcessor 'org.projectlombok:lombok:1.18.24' testImplementation 'org.springframework.boot:spring-boot-starter-test:3.0.1' testImplementation 'org.junit.jupiter:junit-jupiter-api:5.8.1' testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.8.1'}@Getter@Setter@MappedSuperclasspublic class Order { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; @Enumerated(EnumType.STRING) protected OrderStatus status; protected String fromAddress; protected String fromName; protected String fromPhoneNumber; protected String toAddress; protected String toName; protected String toPhoneNumber; protected String itemName; protected int itemPrice; protected LocalDateTime createdDateTime;}@Entity@Getterpublic class Order_2206 extends Order { // Order_2209 Order_2301 동일}1. HibernateHibernate는 6 버전부터 집합 연산자들에 대한 지원을 시작하였습니다.이로써 JPQL과 Criteria API 에서 UNION을 사용할 수 있게 되었습니다. 이게 가능한 이유는 SQM (Semantic Query Model) 때문인데요, SQM에 대해서는 이 문서를 읽어보시기 바랍니다.Hibernate6 이전 버전들의 경우 Criteria API -&gt; JPQL / HQL -&gt; SQL 로 변환되었던 것에 비해 Hibernate6부터는 Criteria APi와 JPQL / HQL이 모두 SQM으로 컴파일되고 최종적으로 SQM이 SQL로 변환된다고 하네요.쿼리 모델을 통합 함으로써 Hibernate Criteria Query가 JPA (JPQL) 스펙에서 지원하지 않는 기능들을 지원할 수 있게 되었다고 합니다. 그중 하나가 UNION인 셈이죠.1-1. HQL첫번째는 HQL 입니다. 이제 FROM 절에서의 서브쿼리와 UNION을 사용할 수 있기 때문에 alias만 잘 지켜서 정직하게 sql을 작성하면 됩니다.@Test@DisplayName(\"Hibernate HQL UNION ALL\")void hibernateHQL() { // o.id AS id, o.status AS status, o.fromAddress AS fromAddress, o.fromName AS fromName, o.fromPhoneNumber AS fromPhoneNumber, o.toAddress AS toAddress, o.toName AS toName, o.toPhoneNumber AS toPhoneNumber, o.itemName AS itemName, o.itemPrice AS itemPrice, o.createdDateTime AS createdDateTime var fieldsWithAlias = Arrays.stream(Order.class.getDeclaredFields()).map(i -&gt; \"o.\" + i.getName() + \" AS \" + i.getName()).collect(Collectors.joining(\", \")); var query = String.format(\"\"\" SELECT %1$s FROM ( SELECT %1$s FROM Order_2206 AS o WHERE o.toName LIKE :name UNION ALL SELECT %1$s FROM Order_2209 AS o UNION ALL SELECT %1$s FROM Order_2301 AS o WHERE o.itemPrice &gt;= :price ) AS o WHERE o.status &lt;&gt; :status \"\"\", fieldsWithAlias); var list = entityManager .createQuery(query) .setParameter(\"name\", \"김%\") .setParameter(\"price\", 100000) .setParameter(\"status\", OrderStatus.FAILED) .getResultList(); System.out.println(list);}생성되는 쿼리는 다음과 같습니다. [DEBUG] [main] SQL - select o4_0.id, o4_0.status, o4_0.fromAddress, o4_0.fromName, o4_0.fromPhoneNumber, o4_0.toAddress, o4_0.toName, o4_0.toPhoneNumber, o4_0.itemName, o4_0.itemPrice, o4_0.createdDateTime from (select o1_0.id, o1_0.status, o1_0.fromAddress, o1_0.fromName, o1_0.fromPhoneNumber, o1_0.toAddress, o1_0.toName, o1_0.toPhoneNumber, o1_0.itemName, o1_0.itemPrice, o1_0.createdDateTime from Order_2206 o1_0 where o1_0.toName like ? escape '' union all select o2_0.id, o2_0.status, o2_0.fromAddress, o2_0.fromName, o2_0.fromPhoneNumber, o2_0.toAddress, o2_0.toName, o2_0.toPhoneNumber, o2_0.itemName, o2_0.itemPrice, o2_0.createdDateTime from Order_2209 o2_0 union all select o3_0.id, o3_0.status, o3_0.fromAddress, o3_0.fromName, o3_0.fromPhoneNumber, o3_0.toAddress, o3_0.toName, o3_0.toPhoneNumber, o3_0.itemName, o3_0.itemPrice, o3_0.createdDateTime from Order_2301 o3_0 where o3_0.itemPrice&gt;=? ) o4_0(id,status,fromAddress,fromName,fromPhoneNumber,toAddress,toName,toPhoneNumber,itemName,itemPrice,createdDateTime)where o4_0.status!=?[TRACE] [main] bind - binding parameter [1] as [VARCHAR] - [김%][TRACE] [main] bind - binding parameter [2] as [INTEGER] - [100000][TRACE] [main] bind - binding parameter [3] as [VARCHAR] - [FAILED]sql문이 잘 생성되는 것을 확인할 수 있습니다.참고로 이전 버전의 Hibernate와 달리 select문에 별칭이 들어가지 않아 sql문이 조금 예뻐(?) 졌는데요, 이는 Hibernate6의 새로운 데이터 추출 방식과 관련이 있습니다.JDBC API가 제공하는 ResultSet으로 데이터를 추출하는 방법에는 read-by-name 과 read-by-position 이 있는데요,Hibernate6 이전 버전들에서는 read-by-name을 사용해 왔지만 이제는 read-by-position을 사용한다고 합니다. Hibernate 릴리즈 노트 과 JDBC driver maintainer의 글에 따르면 read-by-position 방식이 더 빠르다고 하네요.-- 이전select o1_0.id as id_1_0_0_, o1_0.status as status_1_0_0, o1_0.fromAddress as from_address_1_0_0, o1_0.fromName as from_name_1_0_0, o1_0.fromPhoneNumber as from_phone_number_1_0_0,-- 현재select o1_0.id, o1_0.status, o1_0.fromAddress, o1_0.fromName, o1_0.fromPhoneNumber,1-2. Criteria API (Hibernate 6.2 ++)이번에는 Criteria API를 이용하여 HQL보다 type-safe 하게 쿼리를 작성해 보겠습니다. 당연한 이야기지만 어디까지나 Hibernate의 기능들을 사용하는 것이기 때문에 일반 CriteiaBuilder을 사용하면 안되고 HibernateCriteriaBuilder을 사용해야 합니다.Hibernate 6.2 이전 버전에서는 from 절의 서브쿼리에서 union all을 사용할 수 없습니다. 왜냐하면 HibernateCriteriaBuilder 인터페이스에 위 메소드가 존재하지 않기 때문인데요,위 메소드가 존재함으로써 union all의 결과값으로 JPASubQuery 타입을 리턴받을 수 있고 from 절의 서브쿼리에서 union all 을 사용할 수 있게 되었습니다. Hibernate 6.2 버전부터는 with cte as ( ... ) 구문을 사용할 수 있게 되었는데 아마 이에 영향을 받아 추가되지 않았나 싶습니다.@Test@DisplayName(\"Hibernate Criteria API UNION ALL\")void hibernateCriteriaAPI() { Function&lt;JpaRoot&lt;?&gt;, Selection&lt;?&gt;[]&gt; rootPath = (root) -&gt; Arrays.stream(Order.class.getDeclaredFields()).map(field -&gt; root.get(field.getName()).alias(field.getName())).toArray(Selection[]::new); var scq = hibernateCriteriaBuilder.createQuery(Order.class); BiFunction&lt;Class&lt;? extends Order&gt;, Function&lt;JpaRoot&lt;? extends Order&gt;, Expression&lt;Boolean&gt;&gt;, JpaSubQuery&lt;Tuple&gt;&gt; jpaCriteriaQuery = (clazz, condition) -&gt; { var sq = scq.subquery(Tuple.class); var root = sq.from(clazz); sq.multiselect(rootPath.apply(root)); if (condition != null) { sq.where(condition.apply(root)); } return sq; }; var sq1 = jpaCriteriaQuery.apply(Order_2206.class, (root) -&gt; hibernateCriteriaBuilder.ilike(root.get(\"toName\"), \"김%\")); var sq2 = jpaCriteriaQuery.apply(Order_2209.class, null); var sq3 = jpaCriteriaQuery.apply(Order_2301.class, (root) -&gt; hibernateCriteriaBuilder.ge(root.get(\"itemPrice\"), 100000)); var cq = hibernateCriteriaBuilder.createQuery(Order.class); var root = cq.from(hibernateCriteriaBuilder.unionAll(sq1, sq2, sq3)); cq.multiselect(rootPath.apply(root)); cq.where(hibernateCriteriaBuilder.notEqual(root.get(\"status\"), OrderStatus.FAILED)); var list = entityManager.createQuery(cq).getResultList(); System.out.println(list);} select derived1_0.id, derived1_0.status, derived1_0.fromAddress, derived1_0.fromName, derived1_0.fromPhoneNumber, derived1_0.toAddress, derived1_0.toName, derived1_0.toPhoneNumber, derived1_0.itemName, derived1_0.itemPrice, derived1_0.createdDateTime from (select o1_0.id, o1_0.status, o1_0.fromAddress, o1_0.fromName, o1_0.fromPhoneNumber, o1_0.toAddress, o1_0.toName, o1_0.toPhoneNumber, o1_0.itemName, o1_0.itemPrice, o1_0.createdDateTime from Order_2206 o1_0 where o1_0.toName ilike ? escape '' union all select o2_0.id, o2_0.status, o2_0.fromAddress, o2_0.fromName, o2_0.fromPhoneNumber, o2_0.toAddress, o2_0.toName, o2_0.toPhoneNumber, o2_0.itemName, o2_0.itemPrice, o2_0.createdDateTime from Order_2209 o2_0 union all select o3_0.id, o3_0.status, o3_0.fromAddress, o3_0.fromName, o3_0.fromPhoneNumber, o3_0.toAddress, o3_0.toName, o3_0.toPhoneNumber, o3_0.itemName, o3_0.itemPrice, o3_0.createdDateTime from Order_2301 o3_0 where o3_0.itemPrice&gt;=? ) derived1_0(id,status,fromAddress,fromName,fromPhoneNumber,toAddress,toName,toPhoneNumber,itemName,itemPrice,createdDateTime)where derived1_0.status!=?[TRACE] [main] bind - binding parameter [1] as [VARCHAR] - [김%][TRACE] [main] bind - binding parameter [2] as [INTEGER] - [100000][TRACE] [main] bind - binding parameter [3] as [VARCHAR] - [FAILED]HQL로 작성한 쿼리와는 테이블 별칭만 다를뿐 다른 부분은 일치하는 것을 확인할 수 있습니다.2. Spring Data JPA :warning: 예제 세팅에는 Spring Data JPA 디펜던시가 포함되어 있지 않습니다. 직접 테스트를 원하시는 분들은 3.x.x 버전 이상의 Spring Data JPA를 추가하여 주세요.Spring Data JPA의 경우 3.x.x 버전부터 Hibernate 6 버전을 사용하기 시작하였습니다. 기존에는 @Query 어노테이션을 사용하여 union 쿼리를 작성했었는데요,크게 달라진 것은 없지만 이제 Hibernate 차원에서 union을 지원하기 때문에 이제 native sql로 변환할 필요가 없게 되었습니다. 따라서 Spring Boot 어플리케이션 실행 단계에서 쿼리를 검증할 수 있어 안전한 쿼리 작성이 가능합니다.@Query 어노테이션을 사용할 때 nativeQuery = true 옵션을 주지 않아도 됩니다.@Repositorypublic interface OrderRepository extends JpaRepository&lt;Order_2206, Long&gt; { @Query( value = \"\"\" SELECT u.toAddress, u.toName, u.itemName, u.itemPrice FROM ( SELECT o1.status as status, o1.toAddress as toAddress, o1.toName as toName, o1.itemName as itemName, o1.itemPrice as itemPrice FROM Order_2206 o1 WHERE o1.toName like :name UNION ALL SELECT o2.status as status, o2.toAddress as toAddress, o2.toName as toName, o2.itemName as itemName, o2.itemPrice as itemPrice FROM Order_2209 o2 UNION ALL SELECT o3.status as status, o3.toAddress as toAddress, o3.toName as toName, o3.itemName as itemName, o3.itemPrice as itemPrice FROM Order_2301 o3 where o3.itemPrice &gt;= :itemPrice ) AS u WHERE u.status != :status \"\"\" ) List&lt;Tuple&gt; selectResult(@Param(\"name\") String name, @Param(\"itemPrice\") int itemPrice, @Param(\"status\") OrderStatus orderStatus);}3. QueryDSLJPQL이 union을 지원하지 않기 때문에 흔히 사용하는 JPAQuery 인터페이스로는 서브쿼리와 union을 사용할 수 없습니다.Github의 이슈와 답변에 따르면 Hibernate6가 좀 더 큰 범위에서 사용되면 QueryDSL 6.0 버전부터는 JPQL대신 HQL을 직접 사용하게 될수도 있다고 하네요.1. querydsl-sql방법이 없는것은 아닙니다. JPQL을 사용하는 JPAQuery 대신 바로 native sql로 변환되는 JPASQLQuery 인터페이스를 사용하면 됩니다. 인터페이스를 찾을 수 없다면 종속성에 querydsl-sql이 포함되어있는지 다시한번 확인해 보세요.private static JPASQLQuery&lt;?&gt; query() { return new JPASQLQuery&lt;&gt;(entityManager, new PostgreSQLTemplates());}@Test@DisplayName(\"QueryDSL UNION ALL\")void queryDSL() { var q = QOrder.order; var q1 = QOrder_2206.order_2206; var q2 = QOrder_2209.order_2209; var q3 = QOrder_2301.order_2301; var unionList = query().select(q.toName, q.toAddress, q.itemName, q.itemPrice).from( query().unionAll( query() .select(q1.toName.as(q.toName), q1.toAddress.as(q.toAddress), q1.itemName.as(q.itemName), q1.itemPrice.as(q.itemPrice)) .from(q1) .where(q1.toName.like(\"김%\")) , query() .select(q2.toName.as(q.toName), q2.toAddress.as(q.toAddress), q2.itemName.as(q.itemName), q2.itemPrice.as(q.itemPrice)) .from(q2), query() .select(q3.toName.as(q.toName), q3.toAddress.as(q.toAddress), q3.itemName.as(q.itemName), q3.itemPrice.as(q.itemPrice)) .from(q3) .where(q3.itemPrice.goe(100000)) ).as(String.valueOf(q)) ).fetch(); System.out.println(unionList);}select order1.toName, order1.toAddress, order1.itemName, order1.itemPricefrom ((select order_2206.toName as toName, order_2206.toAddress as toAddress, order_2206.itemName as itemName, order_2206.itemPrice as itemPrice from order_2206 where order_2206.toName like ?) union all (select order_2209.toName as toName, order_2209.toAddress as toAddress, order_2209.itemName as itemName, order_2209.itemPrice as itemPrice from order_2209) union all (select order_2301.toName as toName, order_2301.toAddress as toAddress, order_2301.itemName as itemName, order_2301.itemPrice as itemPrice from order_2301 where order_2301.itemPrice &gt;= ?)) as order1;코드와 sql 입니다. 쿼리를 직접 db에 날리기 전까지 에러(예. PostgreSQL - FROM 절 내의 subquery 에는 반드시 alias를 가져야만 합니다. 에러 발생) 를 잡을수 없지만 일단 돌아가기만 하면 위의 두 방식들보다는 더 type-safe한 쿼리를 작성할 수 있다는 장점이 있습니다.다만 코드를 보시면 앞선 쿼리들과 달리 union한 결과물에서 FAILED 상태를 제외하지 않았는데요,만약 상태를 제외하고자 조건을 추가한다면 JPQL로 변환되지 않기 때문에 status column (varchar)을 자바 enum과 매핑할 수 없어 에러가 발생하게 됩니다.이처럼 querydsl을 사용하는 의미가 약간은 퇴색될 수 있다는 단점이 있습니다.2. Blaze Persistence (QueryDSL Integration)querydsl-sql을 사용할때 발생하는 문제를 해결할 수 있는 방법이 있습니다. 바로 Blaze Persistence 를 사용하는 것입니다.Hibernate, QueryDSL의 깃헙 이슈나 PR들을 볼때 자주 등장하는 라이브러리인데 한번 사용해 보았습니다. 이게 무엇이고 어떻게 사용하는지는문서에 상세하게 설명되어 있으니 한번 읽어보시길 바랍니다.Blaze Persistence는 CTE를 지원합니다. 앞선 예제들에서 사용해왔던 서브쿼리 형태의 쿼리를 CTE로 변경하면 다음과 같습니다.with cte as ( select * from order_2206 where order_2206.toname like '김%' union all select * from order_2209 union all select * from order_2301 where order_2301.itemprice &gt;= 100000) select * from cte where status &lt;&gt; 'FAILED'이를 자바코드로 변환하면 다음과 같습니다.@CTE@Entitypublic class OrderCTE { @Id String id; @Enumerated(EnumType.STRING) OrderStatus status; String fromAddress; String fromName; String fromPhoneNumber; String toAddress; String toName; String toPhoneNumber; String itemName; int itemPrice; LocalDateTime createdDateTime;}private BlazeJPAQueryFactory jpaQueryFactory() { var config = Criteria.getDefault(); criteriaBuilderFactory = config.createCriteriaBuilderFactory(entityManagerFactory); return new BlazeJPAQueryFactory(entityManager, criteriaBuilderFactory);}private &lt;T extends EntityPathBase&lt;? extends Order&gt;&gt; Expression&lt;?&gt;[] binding(T targetQ) { var q = QOrderCTE.orderCTE; return new Expression[] { JPQLNextExpressions.bind(q.id, Expressions.stringPath(targetQ, \"id\")), JPQLNextExpressions.bind(q.status, Expressions.path(OrderStatus.class, \"status\")), JPQLNextExpressions.bind(q.fromAddress, Expressions.stringPath(targetQ, \"fromAddress\")), JPQLNextExpressions.bind(q.fromName, Expressions.stringPath(targetQ, \"fromName\")), JPQLNextExpressions.bind(q.fromPhoneNumber, Expressions.stringPath(targetQ, \"fromPhoneNumber\")), JPQLNextExpressions.bind(q.toAddress, Expressions.stringPath(targetQ, \"toAddress\")), JPQLNextExpressions.bind(q.toName, Expressions.stringPath(targetQ, \"toName\")), JPQLNextExpressions.bind(q.toPhoneNumber, Expressions.stringPath(targetQ, \"toPhoneNumber\")), JPQLNextExpressions.bind(q.itemName, Expressions.stringPath(targetQ, \"itemName\")), JPQLNextExpressions.bind(q.itemPrice, Expressions.path(Integer.class, \"itemPrice\")), JPQLNextExpressions.bind(q.createdDateTime, Expressions.path(LocalDateTime.class, \"createdDateTime\")) };}@Test@DisplayName(\"QueryDSL Blaze Persistence Integration\")void queryDSLBlazePersistenceIntegration() { var q = QOrderCTE.orderCTE; var q1 = QOrder_2206.order_2206; var q2 = QOrder_2209.order_2209; var q3 = QOrder_2301.order_2301; var list = jpaQueryFactory() .with( q, jpaQueryFactory().unionAll( JPQLNextExpressions .select(binding(q1)) .from(q1) .where(q1.toName.like(\"김%\")), JPQLNextExpressions .select(binding(q2)) .from(q2), JPQLNextExpressions .select(binding(q3)) .from(q3) .where(q3.itemPrice.goe(100000)) ) ) .select(q) .from(q) .where(q.status.ne(OrderStatus.FAILED)) .fetch(); System.out.println(list.size());}OrderCTE 라는 클래스를 생성하고 @CTE 어노테이션과 QClass를 생성하기 위해 @Entity 어노테이션을 붙였습니다. OrderCTE 클래스는 일종의 임시 테이블 (db로 치면 view) 이라고 보시면 될것 같습니다.주의해야 할 점은 A, B, C 3개의 엔티티를 union all 하고자 할 때 @Id가 붙은 컬럼의 값은 중복되지 않아야 한다는 것입니다. 만약 A id 1, B id 1, C id 1 3개의 테이블을 union all 하면 3개의 row가 리턴되는 것이 아닌 1개의 row만 리턴되게 됩니다.JPA에서 @Id 어노테이션이 붙은 필드는 유지크해야 하기 때문에 어찌보면 당연한(?) 것이라 생각됩니다.select o1_0.id, o1_0.createdDateTime, o1_0.fromAddress, o1_0.fromName, o1_0.fromPhoneNumber, o1_0.itemName, o1_0.itemPrice, o1_0.status, o1_0.toAddress, o1_0.toName, o1_0.toPhoneNumberfrom (select o1_0.id, o1_0.status, o1_0.fromAddress, o1_0.fromName, o1_0.fromPhoneNumber, o1_0.toAddress, o1_0.toName, o1_0.toPhoneNumber, o1_0.itemName, o1_0.itemPrice, o1_0.createdDateTime from Order_2206 o1_0 where o1_0.toName like ? escape '!' union all select o2_0.id, o2_0.status, o2_0.fromAddress, o2_0.fromName, o2_0.fromPhoneNumber, o2_0.toAddress, o2_0.toName, o2_0.toPhoneNumber, o2_0.itemName, o2_0.itemPrice, o2_0.createdDateTime from Order_2209 o2_0 union all select o3_0.id, o3_0.status, o3_0.fromAddress, o3_0.fromName, o3_0.fromPhoneNumber, o3_0.toAddress, o3_0.toName, o3_0.toPhoneNumber, o3_0.itemName, o3_0.itemPrice, o3_0.createdDateTime from Order_2301 o3_0 where o3_0.itemPrice&gt;=?) o1_0(id,status,fromAddress,fromName,fromPhoneNumber,toAddress,toName,toPhoneNumber,itemName,itemPrice,createdDateTime)where o1_0.status!=?생성되는 sql은 위와 같습니다. 자바 코드로는 CTE를 사용했더라도 마지막에는 서브쿼리 형태의 쿼리로 변환되는 것을 확인할 수 있습니다.실무에서 꽤 복잡한 통계 쿼리에도 적용해 보았는데 결과값이 동일하게 나오는 것으로 보아 계속 사용해도 문제없을 것이라 생각됩니다.결론JPA 환경에서 union을 사용하는 방법들에 대해 알아보았습니다. QueryDSL을 사용한다면 Blaze Persistence를 붙여 사용하는것이 최고의 방법이라고 생각됩니다.Hibernate 버전이 6까지 올라오면서 기존의 JPA로는 불가능했던 일들이 어느정도 가능해진 것으로 보입니다. (EclipseLink는 오래전부터 지원했던것…)특정 상황에서 어쩔 수 없이 사용해 왔던 native sql을 순수 자바 코드로만 표현하게 될 날도 머지 않아 보입니다. 이 포스팅에 사용된 예제 코드는 이곳 에서 확인하실 수 있습니다." }, { "title": "22년 회고", "url": "/posts/2022-retrospect/", "categories": "ETC", "tags": "ETC", "date": "2022-12-31 17:12:00 +0900", "snippet": "2022년어느덧 또 한 해의 마지막 날이 되었다. 아무리 생각해도 12월 31일이 생일이면 나이를 볼 때 너무 손해인것 같다. 갓 성인이 되었을때는 좋았지만 이제는 한살 한살이 큰 부담으로 다가오는것 같다.그래도 아직 만으로는 20대 중반임에 감사한다. 또 그때 가봐야 알겠지만 내년 6월부터는 만 나이로 통일한다고 하니 기대해 봐야겠다.19년, 20년, 21년에는 1년에 한번씩 건강에 큰 문제가 생기곤 했는데, 올해는 자잘한 문제만 있을뿐 삶을 바꿀만한 문제는 없었던 것 같다. 내년에도 문제없이 건강한 삶을 살수 있기를 바란다. (운동하자…)직장생활 4년차햇수로만 보면 만 3년이 지나 4년차 개발자가 되었다. 올해 내체공도 끝나고 연봉도 작지 않은 폭으로 올라 만족하며 다녔던 것 같다.인수인계 받은 시스템도 잘 운영했다고 생각한다. 1년전 이쯔음 새로운 팀장님이 오셔서 2명이서 프로젝트를 이끌게 되었는데, 이분이 잘 이끌어 주셔서 큰 문제없이 시스템을 운영하고 회사생활을 할 수 있었던것 같다.첫 회사에서 3년을 버텨보자. 라는 소기의 목적도 달성했다. 솔루션 회사라서 그런가 그동안 써보고 싶었던 기술은 타협없이 원하는만큼 써봤던것 같다. 한가지 아쉬운점은 AWS ECS나 EKS, 혹은 ec2에 직접 컨테이너를 이용해 서버를 구축해보고 싶었는데 못해서 아쉬운것 같다.성장곡선이 완만해지기 시작한 이제는 새로운 도전을 할때가 아닌가 싶다. 일단 내년 연봉이 오르는걸 봐야겠지만, 올해 했던것들을 돌이켜보면 작년만큼의 연봉인상은 없을듯 하다.(ㅠㅠ) 원하는 만큼 올려주면 다니는거고…원하는 때에 이직 할 수 있게 23년은 연초부터 확실하게 준비 해야겠다. 코딩 테스트 준비하자!개인 코딩, 개인 목표꽉 채워진 잔디밭은 아니지만 이정도면 만족한다. 내년에는 더 빼곡한 잔디밭을 만들수 있도록 해야겠다.내년에는 자바 오픈소스에 기여를 해보고자 한다. 올해는 오픈소스를 뜯어보는 일이 꽤 많았던것 같은데 뜯어보면서 코드 분석의 재미(?)를 느낀것 같다. 내년에도 계속 재미를 느낄수 있었으면 좋겠다.또 사이드 프로젝트를 하나 진행해보고자 한다. 올해는 이렇다할 사이드 프로젝트가 없었는데 내년에는 설령 나만 사용할지라도 설계부터 배포까지 끝을 보는 프로젝트 하나를 만들어가고 싶다.근 10년 가까이 해온 메이플과 관련된 사이트도 하나 만들어 봐야겠다. 사실 이미 존재한다. 프론트만 vercel에 배포중인데, 내년엔 서버와 연동하여 이것저것 해봐야지.지금까지 개발자 생활을 돌아보면 지나치게 새로운 무언가에만 몰두한것 같다. 사실 기술이란게 시간이 지난다고 안돌아 가거나 낡아지는것이 아닌데… 돌이켜보면 왜이렇게 신기술에만 집착했나 싶다.그래서 내년엔 지금 가지고 있는 기술들을 잘 다듬고, 깊게 파보고자 한다. 23년 회고록을 작성할때는 한단계 더 성장해 있었으면 좋겠다.작년에 비해 독서량이 절대적으로 적어진것 같다. 내년에는 책을 많이많이 읽도록 하자.이것저것..차 샀다!차 샀다. 니로 하이브리드라는 차다. 15년된 스포티지를 타다가 온갖 기술들이 접목된 차를 타니 운전하기 너무 편한것 같다. 특히 오토홀드나 차선 이탈방지, 스마트 크루즈 컨트롤 같은 기능들은 이제 없으면 운전 못할것 같다.날이 춥지 않을때는 신경써서 주행하면 연비가 25km 이상까지도 나오곤 했었는데 날이 추워지니 20km 찍기도 힘든것 같다. 겨울에는 대중교통 타야지.건강지난 3년간 참 많은 일들이 있었다. 갑자기 돌발성 난청이 오고 (지금도 한쪽은 잘 안들린다.) 그로인해 먹은 스테로이드로 인해 스테로이드성 여드름이 온몸을 뒤덮고,그로인해 또 먹은 약으로 인해 안구 건조증이 오고, 안구 건조증이 온 상태에서 눈 관리를 하지 않아 26년 인생 처음으로 안경을 쓰게 됐었다. 군대에서도 2.0 1.5를 유지했었던 나였는데… 안경을 써야한다는 말을 들었을때 얼마나 상심했는지 모른다.어쨌든 올 2022년은 술을 많이 마신것 치곤 지난 3년과 다르게 이렇다한 큰 일 없이 건강하게 지낸것 같다. 2023년에도 제발 건강했으면 좋겠다…여행여행가고 싶다. 작년 하반기부터 올해까지 여행을 딱 한번 갔다왔다. 그것도 1박 2일로. 한달만 안식 휴가를 받았으면 좋겠다. 정 안되면 2~3일 휴가쓰고 짧게 부산쪽이라도 다녀와야겠다. 아 일단 같이 갈 사람부터 만드는게 먼저인가?끝으로…올해도 역시 많은 일들이 있었다. 22년 넘어올때는 그냥 아무 생각없이 넘어온것 같은데, 23년을 기다리는 지금은 왠지 설레이고 기대된다. 뭔가 인생에 있어 중요한 한 해가 될 것 같은 기분이다.좋은 일들이 가득하고, 원하는 바를 모두 이룰수 있는 한 해가 되었으면 좋겠다." }, { "title": "JPA 3.1", "url": "/posts/jpa3.1/", "categories": "JPA", "tags": "JPA", "date": "2022-12-26 10:12:00 +0900", "snippet": "JPA 3.1Spring 6.0, Spring Boot 3.0 버전이 릴리즈된지 한달이 지났습니다. 현재 날짜 (2022-12-26) 기준으로는 Spring의 경우 6.0.3, Spring Boot의 경우 3.0.1 버전까지 올라왔네요.Spring Boot를 주요 서버 프레임워크로 사용하는 저로써는 Spring Data JPA가 가장 기대됩니다. 그중에서도 최신 JPA 3.1 스펙을 사용해보기를 기대해 보고 있는데요, 아직까지 하이버네이트 메이저 버전에서는 JPA 3.0에 의존하고 있는 것으로 보입니다.위 이미지는 Hibernate 6.1.6 Final 버전이 의존하고 있는 디펜던시 목록입니다. JPA 3.0에 의존하고 있네요.위 이미지는 최근(2022-12-23)에 출시된 Hibernate 6.2.0 CR 버전이 의존하고 있는 디펜던시 목록입니다. JPA 3.1에 의존하고 있죠? Spring Data JPA 3.0.0 버전이 Hibernate 6.1.4 Final 버전을 사용하고 있기 때문에 2023년 안에는 JPA 3.1 스펙의 기능들을 Spring Boot에서 기본적으로 사용할 수 있을 것으로 보입니다.그래서 오늘은 새로운 Spring Data JPA를 기다리며 JPA 3.1의 새로운 기능들, 변경점에 대해 알아보고자 합니다.JPA 3.0JPA 3.1 출시 이전에 JPA 3.0이 먼저 출시되었겠죠? 현재 시점의 최신 Hibernate 는 JPA 3.0 스펙을 완전히 지원합니다. Hibernate 6.0.x, 6.1.x 버전이 의존하고 있는 종속성을 확인해보면 jakarta.persistence-api:3.x 를 의존하고 있는것을 확인할 수 있습니다.JPA가 3.0 버전으로 올라오면서 기존 JPA 2.2과 비교해 새로생긴 기능은 없습니다. 단, 모든 API클래스, 패키지, 점두사, 및 모든 XML 기반 구성 파일의 스키마 네임스페이스는 변경되었습니다. 간단히 말해 javax.persistence 패키지 내에 존재했던 클래스들이 jakarta.persistence 패키지 내부로 들어갔다고 보시면 되는데요,관련해서는 이 글 을 읽어보시면 도움이 될것 같습니다. 왜 자바EE가 이클립스 재단으로 이관되고 자카르타EE 라는 명칭을 가지게 되었는지 확인하실 수 있습니다.JPA 3.1JPA 3.1은 Jakarta 10 의 일부로서 2022년 3월 30일에 릴리즈 되었습니다. 아래부터는 변경점과 새로운 기능들에 대해 알아보겠습니다.새로 추가된 기능들은 예제를 통해 알아보고자 합니다. 일단 예제 프로젝트부터 구성해 보겠습니다.첫번째로 build.gradle 파일입니다. hibernate-core 6.2.0 CR1 버전을 사용하였습니다. 파라미터 로깅을 위해 log4j2를 사용하며 db는 h2를 사용합니다.plugins { id 'java'}group 'com.keencho'version '1.0-SNAPSHOT'repositories { mavenCentral()}tasks.withType(JavaCompile) { options.encoding = 'UTF-8'}test { useJUnitPlatform()}dependencies { implementation 'org.hibernate.orm:hibernate-core:6.2.0.CR1' implementation 'com.h2database:h2:2.1.214' implementation group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.19.0' implementation 'org.projectlombok:lombok:1.18.24' annotationProcessor 'org.projectlombok:lombok:1.18.24' testImplementation 'org.junit.jupiter:junit-jupiter-api:5.9.1' testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.9.1'}log4j2.xml 파일입니다. 로깅을 위해 사용됩니다.&lt;Configuration monitorInterval=\"60\"&gt; &lt;Properties&gt; &lt;Property name=\"log-path\"&gt;PropertiesConfiguration&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;Console name=\"Console-Appender\" target=\"SYSTEM_OUT\"&gt; &lt;PatternLayout&gt; &lt;pattern&gt; [%-5level] %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %c{1} - %msg%n &lt;/pattern&gt;&gt; &lt;/PatternLayout&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name=\"org.hibernate.SQL\" level=\"debug\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Logger&gt; &lt;Logger name=\"org.hibernate.orm.jdbc.bind\" level=\"trace\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Logger&gt; &lt;Logger name=\"org.hibernate.stat\" level=\"trace\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Logger&gt; &lt;Logger name=\"org.hibernate.SQL_SLOW\" level=\"trace\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Logger&gt; &lt;Logger name=\"org.hibernate.cache\" level=\"trace\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Logger&gt; &lt;Root level=\"info\"&gt; &lt;AppenderRef ref=\"Console-Appender\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt;다음은 /src/main/resources/META-INF/persistence.xml 경로에 존재해야 하는 persistence.xml 파일입니다.&lt;persistence xmlns=\"https://jakarta.ee/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"https://jakarta.ee/xml/ns/persistence https://jakarta.ee/xml/ns/persistence/persistence_3_1.xsd\" version=\"3.1\"&gt; &lt;persistence-unit name=\"pu\" transaction-type=\"RESOURCE_LOCAL\"&gt; &lt;description&gt;JPA3.1 Test with hibernate6 persistence-unit&lt;/description&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;class&gt;com.keencho.jpa31.model.Book&lt;/class&gt; &lt;properties&gt; &lt;property name=\"hibernate.archive.autodetection\" value=\"class, hbm\"/&gt; &lt;property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.H2Dialect\"/&gt; &lt;property name=\"hibernate.connection.driver_class\" value=\"org.h2.Driver\"/&gt; &lt;property name=\"hibernate.connection.url\" value=\"jdbc:h2:mem:db1;DB_CLOSE_DELAY=-1\"/&gt; &lt;property name=\"hibernate.connection.username\" value=\"sa\"/&gt; &lt;property name=\"hibernate.connection.pool_size\" value=\"5\"/&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"create-drop\"/&gt; &lt;property name=\"hibernate.max_fetch_depth\" value=\"5\"/&gt; &lt;property name=\"hibernate.jdbc.batch_versioned_data\" value=\"true\"/&gt; &lt;property name=\"jakarta.persistence.validation.mode\" value=\"NONE\"/&gt; &lt;property name=\"hibernate.service.allow_crawling\" value=\"false\"/&gt; &lt;property name=\"hibernate.session.events.log\" value=\"true\"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt;Book 엔티티 입니다. 이 엔티티로 새로운 기능들을 확인해 볼 것입니다.@Data@NoArgsConstructor@Entitypublic class Book { @Id @GeneratedValue(strategy = GenerationType.AUTO) private UUID id; private String title; private int pages; // 판매율 (소수점) private BigDecimal sellingRates; // 분류기호 (소수점) private BigDecimal classificationSymbol; // 발매일 private LocalDateTime releaseDateTime;}다음은 테스트를 도와줄 설정 클래스 입니다.public class HibernateHelper { public static final HibernateHelper instance = new HibernateHelper(); @Getter private final EntityManagerFactory entityManagerFactory; @Getter private final EntityManager entityManager; private HibernateHelper() { this.entityManagerFactory = Persistence.createEntityManagerFactory(\"pu\"); this.entityManager = entityManagerFactory.createEntityManager(); } public static void beginTransaction() { instance.getEntityManager().getTransaction().begin(); } public static void commit() { instance.getEntityManager().getTransaction().commit(); } public static void close() { instance.getEntityManagerFactory().close(); } public static void persist(Object entity) { instance.getEntityManager().persist(entity); } //////////////////////////////////////////////////////////////////// /////////////////////////////////// SELECT //////////////////////////////////////////////////////////////////// @FunctionalInterface public interface SelectHelper { Selection&lt;?&gt; apply(CriteriaBuilder cb, Root&lt;?&gt; root); } private static CriteriaBuilder getCriteriaBuilder() { // https://hibernate.zulipchat.com/#narrow/stream/132096-hibernate-user/topic/New.20functions.20in.20JPA.203.2E1/near/289429903 // hibernate 6.1.x final 버전까지 jpa3.0에 의존하고 있기 때문에 HibernateCriteriaBuilder 로 형변환 해야한다. // 아직 final은 아니지만 hibernate 6.2.x CR 버전에서는 jpa 3.1에 의존하고 있으며 그냥 이 코드로 사용 가능하다. (형변환이 필요 없다.) return instance.getEntityManager().getCriteriaBuilder(); } public static &lt;T&gt; List&lt;T&gt; listAll(Class&lt;T&gt; rootClass) { return list(rootClass).stream().map(i -&gt; (T) i.get(0)).collect(Collectors.toList()); } public static List&lt;Tuple&gt; list(Class&lt;?&gt; rootClass, SelectHelper... selectHelper) { var cb = getCriteriaBuilder(); var query = cb.createTupleQuery(); var root = query.from(rootClass); var list = new ArrayList&lt;Selection&lt;?&gt;&gt;(); for (SelectHelper helper : selectHelper) { list.add(helper.apply(cb, root)); } if (!list.isEmpty()) { query.multiselect(list); } return instance.getEntityManager().createQuery(query).getResultList(); } public static List&lt;?&gt; list(String query) { return instance.getEntityManager().createQuery(query).getResultList(); }}Hibernate 6.1.x 이하 버전을 사용하신다면 중간의 getCriteriaBuilder() 메소드는 HibernateCriteriaBuilder를 리턴해야 합니다. Hibernate 6.1.x 이하 버전은 JPA 3.0을 사용하기 때문인 것으로 보이며 이 글을 그대로 따라하셨거나 Hibernate 6.2.x 이상 버전을 사용한다면 이는 JPA 3.1을 사용하기 때문에 예제 그대로 CriteriaBuilder를 리턴하게 두시면 됩니다. 자세한 내용은 이 스레드를 확인해 보세요.마지막으로 테스트 클래스 입니다. 테스트 전에 트랜잭션을 시작하고 2개의 엔티티를 영속화 합니다. 아래에서 작성할 모든 예제 테스트들은 이 클래스 내부에 위치하게 됩니다.public class JPA31Test { @BeforeAll public static void init() { // disable hibernate info log Logger logger = Logger.getLogger(\"org.hibernate\"); logger.setLevel(Level.OFF); HibernateHelper.beginTransaction(); var book1 = new Book(); book1.setTitle(\"Effective Java\"); book1.setPages(342); book1.setSellingRates(new BigDecimal(\"48.54\")); book1.setClassificationSymbol(new BigDecimal(\"170.537\")); book1.setReleaseDateTime(LocalDateTime.of(2020, 3, 13, 17, 20, 30)); var book2 = new Book(); book2.setTitle(\"Modern Java in Action\"); book2.setPages(573); book2.setSellingRates(new BigDecimal(\"51.46\")); book2.setClassificationSymbol(new BigDecimal(\"2096.331\")); book2.setReleaseDateTime(LocalDateTime.of(2022, 8, 13, 10, 43, 0)); HibernateHelper.persist(book1); HibernateHelper.persist(book2); HibernateHelper.commit(); } @AfterAll public static void close() { HibernateHelper.close(); }}1. EntityManagerFactory, EntityManager 인터페이스가 java.lang.AutoCloseable 인터페이스를 상속합니다. (변경)순서대로 JPA 3.0의 EntityManagerFactory, JPA 3.1의 EntityManagerFactory 인터페이스 입니다. 이제 SessionFactory와 관련된 자원들을 자동으로 종료하게 됩니다.2. ClassTransformer.transform 메소드가 Persistence API 스펙의 예외를 던집니다. (변경)기존 transform 메소드는 IllegalClassFormatException를 던졌었는데 이제는 TransformerException를 던지게 됩니다.3. java.util.UUID와 GenerationType.UUID 지원 (추가)@GeneratedValue 어노테이션의 생성전략을 AUTO 로 저장하거나 새롭게 추가된 UUID로 지정할 수 있습니다. 만약 베이직 타입을 UUID로 맞추고 생성 전략을 AUTO로 하였다면 자동으로 UUID전략을 따르게 됩니다.다음은 예제입니다. 영속화된 엔티티의 id가 uuid형태가 맞는지 정규 표현식으로 확인합니다.static final String UUID_PATTERN = \"[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\";@Test@DisplayName(\"GenerationType.UUID test\")void testUUID() { var resultList = HibernateHelper.listAll(Book.class); resultList.forEach(book -&gt; { var id = book.getId().toString(); System.out.println(\"uuid: \" + id); Assertions.assertTrue(Pattern.matches(UUID_PATTERN, id)); });}uuid: 9a612d02-1905-497a-9d9b-719918eec58fuuid: 39824f2f-ac89-4357-9a3d-8b67d24a6fe14. Numeric 함수 추가 (추가)CEILING, EXP, FLOOR, LN, POWER, ROUND, SIGN 함수가 JPQL에 추가되었으며 ceiling(), exp(), floor(), ln(), power(), round(), sign() 함수가 Criteria API에 추가되었습니다.@Test@DisplayName(\"new numeric functions test\")void testNewNumericFunctions() { var listAll = HibernateHelper.listAll(Book.class); var resultList = HibernateHelper.list( Book.class, // 수 올림 (cb, root) -&gt; cb.ceiling(root.get(\"sellingRates\")), // 수 내림 (cb, root) -&gt; cb.floor(root.get(\"sellingRates\")), // n번째 자리 반올림 (cb, root) -&gt; cb.round(root.get(\"sellingRates\"), 1), // x를 인수로 하는 e^x 값을 반환 (cb, root) -&gt; cb.exp(root.get(\"sellingRates\")), // 자연 로그를 반환 (cb, root) -&gt; cb.ln(root.get(\"sellingRates\")), // n만큼 거듭제곱하여 반환 (cb, root) -&gt; cb.power(root.get(\"sellingRates\"), 2), // 인수의 부호를 반환 (-1, 0, 1) (cb, root) -&gt; cb.sign(root.get(\"sellingRates\")) ); System.out.println(\"\\n\\n\"); for (var i = 0; i &lt; listAll.size(); i ++) { System.out.println(\"==========================\"); System.out.printf(\"원래 값: %s%n\", listAll.get(i).getSellingRates().toString()); System.out.printf(\"ceiling 함수: %s%n\", resultList.get(i).get(0)); System.out.printf(\"floor 함수: %s%n\", resultList.get(i).get(1)); System.out.printf(\"round 함수: %s%n\", resultList.get(i).get(2)); System.out.printf(\"exp 함수: %s%n\", resultList.get(i).get(3)); System.out.printf(\"ln 함수: %s%n\", resultList.get(i).get(4)); System.out.printf(\"power 함수: %s%n\", resultList.get(i).get(5)); System.out.printf(\"sign 함수: %s%n\", resultList.get(i).get(6)); } System.out.println(\"\\n\\n\");}==========================원래 값: 48.54ceiling 함수: 49floor 함수: 48round 함수: 48.50exp 함수: 1.2040766975298458E21ln 함수: 3.8823882002984553power 함수: 2356.1315999999997sign 함수: 1==========================원래 값: 51.46ceiling 함수: 52floor 함수: 51round 함수: 51.50exp 함수: 2.232513217248359E22ln 함수: 3.940804806853598power 함수: 2648.1316sign 함수: 15. Date Time 관련 함수 추가 (추가)LOCAL DATE, LOCAL DATETIME, LOCAL TIME 함수가 JPQL에 추가되었으며 localDate(), localDateTime(), localTime()함수가 Criteria API에 추가되었습니다.이 함수들은 날짜를 계산하는 함수가 아닌 현재 시간을 반환하는 함수들 입니다.@Test@DisplayName(\"new DateTime functions test\")void testNewDateTimeFunctions() { var now = LocalDateTime.now(); var firstResult = HibernateHelper.list( Book.class, // 현재 시간 (cb, root) -&gt; cb.localTime(), // 현재 날짜 (cb, root) -&gt; cb.localDate(), // 현재 날짜 + 현재 시간 (cb, root) -&gt; cb.localDateTime() ).get(0); System.out.println(\"\\n\\n\"); System.out.println(\"현재시간: \" + now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))); System.out.println(\"시간: \" + firstResult.get(0)); System.out.println(\"날짜: \" + firstResult.get(1)); System.out.println(\"날짜 + 시간: \" + firstResult.get(2)); System.out.println(\"\\n\\n\");}현재시간: 2022-12-26 16:23:16시간: 16:23:16날짜: 2022-12-26날짜 + 시간: 2022-12-26T16:23:16.3235096. EXTRACT 함수 추가 (추가)EXTRACT 함수가 JPQL에 추가되었습니다. 안타깝게도 Criteria API에는 이에 대응하는 함수가 아직 없습니다. PR에 게빈 킹이 요청한 흔적이 있긴 합니다만 JPA 3.1에는 추가되지 않은듯 합니다.@Test@DisplayName(\"new extract functions test\")void textExtractFunctions() { // https://github.com/jakartaee/persistence/pull/356 // criteria api에는 extract 함수가 존재하지 않는다 흑흑 var query = \"\"\" SELECT b.releaseDateTime as releaseDateTime, EXTRACT(YEAR from b.releaseDateTime) as year, EXTRACT(MONTH from b.releaseDateTime) as month, EXTRACT(DAY from b.releaseDateTime) as day, EXTRACT(HOUR from b.releaseDateTime) as hour, EXTRACT(MINUTE from b.releaseDateTime) as minute, EXTRACT(SECOND from b.releaseDateTime) as second FROM Book b \"\"\"; System.out.println(\"\\n\\n\"); var list = HibernateHelper.list(query); for (var i = 0; i &lt; list.size(); i ++) { var result = (Object[]) list.get(i); System.out.println(\"==========================\"); System.out.println(\"오리지널 값 (발매일): \" + result[0]); System.out.println(\"년도: \" + result[1]); System.out.println(\"달: \" + result[2]); System.out.println(\"날짜: \" + result[3]); System.out.println(\"시: \" + result[4]); System.out.println(\"분: \" + result[5]); System.out.println(\"초: \" + result[6]); } System.out.println(\"\\n\\n\");}==========================오리지널 값 (발매일): 2020-03-13T17:20:30년도: 2020달: 3날짜: 13시: 17분: 20초: 30.0==========================오리지널 값 (발매일): 2022-08-13T10:43년도: 2022달: 8날짜: 13시: 10분: 43초: 0.07. Criteria CASE 표현식에서 Expressions를 조건식으로 사용할 수 있도록 지원 (추가)SimpleCase&lt;C, R&gt; when(Expression&lt;? extends C&gt; condition, R result) 메소드와 SimpleCase&lt;C, R&gt; when(Expression&lt;? extends C&gt; condition, Expression&lt;? extends R&gt; result) 메소드가 추가되었습니다.@Test@DisplayName(\"new criteria simple case expressions\")void testCriteriaCaseExpressions() { var list = HibernateHelper.list( Book.class, (cb, root) -&gt; root.get(\"title\"), (cb, root) -&gt; cb.selectCase(root.get(\"title\")) .when(cb.literal(\"Effective Java\"), cb.literal(true)) .otherwise(false) ); System.out.println(\"\\n\\n\"); list.forEach(item -&gt; System.out.println(\"title: \" + item.get(0) + \" / title is 'Effective Java'?: \" + item.get(1))); System.out.println(\"\\n\\n\");}참고로 현재 영속화된 book 엔티티는 2개이며 각각 Effective Java, Modern Java in Action 이라는 제목을 가지고 있습니다.select b1_0.title, case b1_0.title when 'Effective Java' then true else cast(? as boolean) end from Book b1_0[TRACE] 2022-12-27 08:17:10.015 [main] bind - binding parameter [1] as [BOOLEAN] - [false]title: Effective Java / title is 'Effective Java'?: truetitle: Modern Java in Action / title is 'Effective Java'?: false위 테스트를 돌려보면 결과는 위와같이 나오는데요, sql문을 보시면 상수표현식(literal)으로 작성한 첫번째 when 구문은 파라미터가 나중에 바인딩 되는것이 아니고 바로 바인딩 되게 됩니다. 이렇게 되면 내부적으로 Prepared Statements로써 동작하지는 않겠습니다만 sql구문을 Prepared Statements로 풀어낼수 없는 상황은 분명히 존재하므로 필요한 기능이라고 생각됩니다.8. 기타 (수정)기타 수정된 사항들도 있습니다. 이들은 어떤 기능을 추가 / 수정한 것이 아니라 어떤 주제를 명확히, 따지자면 문서 수정작업 정도로 보시면 될것 같습니다. 변경점 원문 그대로와 Github PR 링크를 걸어드리니 한번 읽어보시기 바랍니다. Adds missing definition of single_valued_embeddable_object_field in Jakarta Persistence QL BNF Clarifies mixing types of query input parameters Clarifies definition of the Basic type Clarifies the order of parameters in the LOCATE function Clarifies SqlResultSetMapping with multiple EntityResults and conflicting aliases마치며JPA 3.1에 대해 간략히 알아보았습니다. JPA 3.1에 의존하는 Hibernate Final 버전과 이 Hibernate Final 버전에 의존하는 Spring Data JPA가 얼른 나오면 좋겠습니다. Spring 팀이 어떻게 이를 응용해 새로운 기능들을 만들어낼지 궁금하네요." }, { "title": "Dirty Checking", "url": "/posts/dirty-checking/", "categories": "Hibernate", "tags": "JPA, Hibernate", "date": "2022-10-16 07:12:00 +0900", "snippet": "Dirty Checking1. 개요하이버네이트 / jpa를 사용하다보면 이미 저장된 엔티티의 경우 jpa repository의 save() 메소드를 호출하지 않아도 한 트랜잭션이 종료되면 엔티티의 변경점이 자동으로 업데이트되는 현상을 발견할 수 있습니다.var delivery = deliveryRepository.findById(1L).orElseThrow(() -&gt; new RuntimeException(\"delivery entity not exist!\"));delivery.setDtDeliveryStartedAt(LocalDateTime.now());위와같이 save 메소드 없이 코드를 작성해도 트랜잭션이 종료되면 dtDeliveryStartedAt 컬럼이 업데이트되게 됩니다.위 동작을 Dirty Checking 이라 하는데요, Dirty Checking의 동작 메커니즘은 다음과 같습니다. 영속선 컨텍스트는 플러시가 일어날 경우 엔티티의 변경점들을 대기열에 넣습니다.하이버네이트는 관리되는 엔티티들의 모든 변경을 자동으로 감지하고 개발자들을 대신하여 SQL Update문을 계획하고 수행합니다.하이버네이트는 기본적으로 모든 엔티티의 속성 / 변경점을 확인합니다. 엔티티가 로딩될 때마다 하이버네이트는 모든 엔티티 속성값을 가지고 있는 스냅샷(복사본)을 생성합니다. 그 후 플러시가 일어나는 시점에 엔티티와 스냅샷을 비교하여 변경된 부분을 체크하고 업데이트 합니다.단순히 불러와서 값을 변경했다하여 모두 업데이트되는 것이 아닙니다. Dirty Checking이 일어나려면 아래 조건을 충족해야 합니다. 엔티티가 영속상태인 경우 Transaction 안에서 엔티티의 값을 변경하는 경우 이미 저장된 엔티티인 경우스프링을 사용한다면 @Transactional 어노테이션을 서비스 레이어에서 사용하면 위 조건은 만족하게 됩니다.2. 특정 컬럼만 업데이트하기JPA에서는 모든 필드를 업데이트하는 방식을 기본값으로 사용합니다. 이는 다음과 같은 장점을 갖고 있습니다. 생성되는 쿼리가 모두 같아 어플리케이션이 실행되는 시점에 만들어서 재사용 가능 업데이트되는 컬럼수가 동일하기 때문에 캐싱된 SQL 구문을 사용할 수 있고, 데이터베이스 입장에서 업데이트 컬럼이 작다면 특정 컬럼만 업데이트하는 구문에 비해 성능상 이점을 가져갈 수 있음특정 컬럼만 업데이트하는 방법들을 몇가지 소개하긴 할테지만, 특정 상황이 아닌 경우 오히려 이러한 방법들은 전체 컬럼을 업데이트하는 방법보다 오히려 많은 자원을 소모할 수 있습니다.컬럼이 30개정도 된다면 특정 컬럼만 업데이트 하는 것이 도움이 될테지만 애초에 컬럼이 30개라면 정규화가 잘못되어 있다는 의미겠죠? 항상 생각하는 것이지만 역시 제일 중요한 것은 db 설계 같습니다.1. @DynamicUpdate 사용첫번째 방법은 엔티티에 @DynamicUpdate 어노테이션을 선언하는 것입니다.@Entity@Data@NoArgsConstructor@Table(name = \"delivery\")@DynamicUpdatepublic class Delivery { ...}위와같이 어노테이션을 선언하는 것만으로도 변경 필드만 반영되게 할 수 있습니다.2. @Query 사용두번째 방법은 @Query어노테이션을 사용하여 직접 update 구문을 작성하는 것입니다.@Modifying@Query(\"UPDATE Delivery d set d.dtDeliveryStartedAt = :date where d.deliveryId = :id\")void updateDtDeliveryStartedAtById(@Param(\"date\") LocalDateTime date, @Param(\"id\") Long id);이렇듯 직접 update 문을 작성하여 특정 컬럼만 업데이트 할 수 있습니다. select가 아닌 DML 이기 때문에 @Modifying 어노테이션을 붙여야 함을 잊지 마세요.3. QueryDSL JPAUpdateClause 사용세번째 제가 주로 사용하는 방법은 QueryDSL의 JPAUpdateClause 클래스를 사용하는 방법입니다.@Testpublic void test() { EntityManager em = emf.createEntityManager(); QDelivery q = QDelivery.delivery; JPAUpdateClause jpaUpdateClause = new JPAUpdateClause(em, q); Map&lt;Path&lt;?&gt;, Object&gt; map = new HashMap&lt;&gt;(); map.put(q.dtDeliveryStartedAt, LocalDateTime.now()); List&lt;Path&lt;?&gt;&gt; paths = new ArrayList&lt;&gt;(); List&lt;Object&gt; values = new ArrayList&lt;&gt;(); for (var entry : map.entrySet()) { paths.add(entry.getKey()); values.add(entry.getValue()); } long result = jpaUpdateClause.where(q.deliveryId.eq(1L)).set(paths, values).execute(); Assert.isTrue(result == 1, \"update result is not 1\");}위와같이 JPAUpdateClause 를 사용하면 @Query 어노테이션을 사용하는 방식에 비해 조금더 type-safe하게 update 문을 자바 코드로 작성할 수 있습니다. 위 코드는 많은 보일러플레이트 코드를 만들수 있는데요, 제 경우 별도의 repository를 만들어서 코드를 작성하곤 합니다. (링크 에서 확인해보세요.)그렇다면 아래와 같이 코드량이 줄게 되지요.@Testpublic void test() { var q = QDelivery.delivery; var map = Q.newUpdateMap(); map.put(q.dtDeliveryStartedAt, LocalDateTime.now()); deliveryRepository.updateOne(q.deliveryId.eq(1L), map);}3. 결론Dirty Checking과 특정 컬럼만 업데이트 하는 방법에 대해 알아보았습니다. 모든 컬럼을 업데이트 한다고 하여 성능상에 문제가 생기것은 아니기 때문에 테이블의 컬럼 수와 상황을 잘 고려하여 전체를 업데이트 할지, 특정 컬럼만 업데이트 할지 고민해 보시기 바랍니다." }, { "title": "QueryProjection 에서 한단계 더 나아가 QueryProjectionBuilder 만들기 (2)", "url": "/posts/querydsl-qbuilder-qsetter-2/", "categories": "QueryDSL", "tags": "Java, JPA, QueryDSL", "date": "2022-10-02 07:12:00 +0900", "snippet": "QueryProjection 에서 한단계 더 나아가 QueryProjectionBuilder 만들기 (2) - 개발 과정, 코드 이 포스팅에서 설명하는 코드는 한 프로젝트 내에서 사용할 수 없습니다.라이브러리 프로젝트 A, 실제 어플리케이션 프로젝트 B 로 나뉘어야 합니다.java17, maven 을 사용합니다.1. @QueryProjection 으로 생성되는 클래스 분석클래스를 프로젝션 타입으로 사용하기 위해 우리는 클래스 생성자에 @QueryProjection 어노테이션을 붙이고 Q 클래스를 생성합니다. 생성되는 Q 클래스는 다음과 같습니다./** * com.keencho.libormtest.model.QCustomerDTO is a Querydsl Projection type for CustomerDTO */@Generated(\"com.querydsl.codegen.DefaultProjectionSerializer\")public class QCustomerDTO extends ConstructorExpression&lt;CustomerDTO&gt; { private static final long serialVersionUID = -785800583L; public QCustomerDTO(com.querydsl.core.types.Expression&lt;Long&gt; id, com.querydsl.core.types.Expression&lt;String&gt; loginId, com.querydsl.core.types.Expression&lt;String&gt; password, com.querydsl.core.types.Expression&lt;String&gt; name, com.querydsl.core.types.Expression&lt;Integer&gt; age) { super(CustomerDTO.class, new Class&lt;?&gt;[]{long.class, String.class, String.class, String.class, int.class}, id, loginId, password, name, age); }}ConstructorExpression 클래스를 상속하는 것을 확인할 수 있습니다. 해당 클래스의 생성자를 살펴봅시다. 생성자의 인자로 제네릭 클래스 타입과 프로젝션의 타입을 담은 배열, 그리고 프로젝션의 값을 인자로 받고 있는 것을 확인할 수 있습니다.아래로 내래보면 newInstance(Object... args) 라는 메소드를 확인할 수 있습니다. 이 메소드가 바로 프로젝션을 생성하는 메소드 입니다.생성자와 메소드를 통해 어떻게 생성자를 통해 조회 결과를 반환 받을수 있는지 확인할 수 있습니다.2. 새로운 Expression 클래스 만들기위에서 살펴본 ConstructorExpression 은 어디까지나 클래스의 모든 필드를 매개변수로 갖는 생성자를 위한 조회 방식이기 때문에 빌더패턴을 사용할 수 없습니다. FactoryExpressionBase 클래스를 상속하는 새로운 프로젝션 클래스를 만드는게 낫겠네요. public KcExpression(Class&lt;? extends T&gt; type, Map&lt;String, Expression&lt;?&gt;&gt; bindings) { super(type); this.type = type; if (bindings == null || bindings.isEmpty()) { throw new RuntimeException(\"bindings must not be null or empty!\"); } this.bindings = Collections.unmodifiableMap( bindings .entrySet() .stream() .filter(entry -&gt; entry.getValue() != null) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (x, y) -&gt; y, LinkedHashMap::new)) );}프로젝션 클래스의 생성자 입니다. 클래스 타입과 Map&lt;String, Expression&lt;?&gt;&gt;을 매개변수로 받습니다. 전달받은 map에서 null값을 제외하고 순서를 보장하는 LinkedHashMap으로 다시 생성하여 bindings 변수에 할당합니다.@Overridepublic T newInstance(Object... a) { try { var arr = this.bindings.keySet().toArray(); var rv = this.type.getDeclaredConstructor().newInstance(); for (var i = 0; i &lt; a.length; i ++) { var value = a[i]; if (value != null) { var field = this.type.getDeclaredField((String) arr[i]); field.setAccessible(true); field.set(rv, value); } } return rv; } catch (InstantiationException | IllegalAccessException | NoSuchFieldException | InvocationTargetException | NoSuchMethodException e) { throw new ExpressionException(e.getMessage(), e); }}newInstance 메소드에서는 리플렉션으로 빈 생성자를 생성하고 값이 넘어온 순서대로 필드에 값을 저정합니다. 문제는 이 메소에는 필드의 정보나 타입같은건 넘어오지 않는다는 점입니다.오직 Object... 형식으로 순수한 값만 넘어오기 때문에 현재 넘어오는 값이 어떠한 필드에 매핑되는지 알 수 없습니다. 그래도 다행인 것은 값이 필드 순서대로 넘어온다는 것입니다.제가 위에서 순서를 보장하는 LinkedHashMap을 사용했던 것은 위와같은 이유 때문입니다. 물론 애초에 이 클래스가 생성되는 시점에도 순서가 보장된 map을 파라미터로 넘겨야 합니다. 메소드에 넘어오는 값이 순서를 보장하기 때문에 클래스의 필드정보를 담고 있는 map 객체의 요소가 반환받을 클래스의 필드 선언 순서와 동일하다면 문제없이 반환받을 클래스에 값을 저장하고 넘겨받을 수 있겠죠.3. record를 반환 받을 수 있는 Expression 만들기위에서 만든 Expression 클래스에는 문제가 하나 있습니다. 빈 생성자를 만들고 그 이후에 값을 세팅하는 방식이기 때문에 빈 생성자가 존재할 수 없는 record 타입의 클래스는 사용할 수 없습니다.이를 위해 record 클래스 에 사용할 Expression 클래스는 따로 만들도록 하겠습니다.public class KcRecordExpression&lt;T extends Record&gt; extends KcExpression&lt;T&gt; { private final Class&lt;? extends T&gt; type; public KcRecordExpression(Class&lt;? extends T&gt; type, Map&lt;String, Expression&lt;?&gt;&gt; bindings) { super(type, bindings); if (!type.isRecord()) { throw new RuntimeException(\"This expression is an expression for record. Use KcExpression to bind a regular class.\"); } this.type = type; } @Override public T newInstance(Object... a) { if (this.type.getDeclaredFields().length != a.length) { throw new RuntimeException(\"Because a record type must create an object as a constructor that accepts all variables as arguments, the number of declared fields and the number of parameters to bind must be the same.\"); } try { var arr = getBindings().keySet().toArray(); var fields = new Field[a.length]; for (var i = 0; i &lt; fields.length; i ++) { fields[i] = this.type.getDeclaredField((String) arr[i]); } var matchedConstructor = Arrays .stream(this.type.getConstructors()) .filter(constructor -&gt; { var parameterTypes = constructor.getParameterTypes(); for (var i = 0; i &lt; a.length; i ++) { var constructorType = parameterTypes[i]; var field = fields[i]; if (!constructorType.getTypeName().equals(field.getType().getTypeName())) { return false; } } return true; }).toList(); if (matchedConstructor.size() != 1) { throw new RuntimeException(\"No or more than one constructor exists with the same number and type of parameters. It seems record is not suitable for this case.\"); } var constructor = matchedConstructor.get(0); return (T) constructor.newInstance(a); } catch (Exception e) { throw new ExpressionException(e.getMessage(), e); } }}클래스 생성 시점에 타입이 record 타입인지 검사합니다.newInstance 메소드에는 방어 코드가 존재합니다. 첫째로 반환받을 record 클래스의 필드 갯수와 메소드에 넘어온 파라미터의 갯수가 똑같아야 합니다. 클래스 내에 있는 모든 변수들을 인수로 받을 생성자를 만들어야 하기 때문에 당연합니다.둘째로 사용할 생성자가 명확해야 합니다. record 클래스 내에도 생성자는 여러개 존재할 수 있습니다. 그러나 여기서 사용할 생성자는 자바가 기본으로 만들어주는 모든 변수를 인수로 갖는 생성자여야 합니다. 따라서 값의 타입과 매개변수의 타입, 생성자 매개변수의 숫자를 검증하여 생성자의 조건과 일치하는 단 하나의 생성자만을 사용하도록 하였습니다.4. AnnotationProcessor 만들기커스텀 어노테이션을 만들고 해당 어노테이션이 붙어있는 클래스를 위에서 만든 프로젝션 클래스를 상속하는 클래스로써 새롭게 만들기 위해 AnnotationProcessor를 만들어야 합니다.첫째로 클래스 위에 붙일 어노테이션 입니다.@Documented@Target(ElementType.TYPE)@Retention(RUNTIME)public @interface KcQueryProjection {}다음은 AbstractProcessor를 상속하는 AnnotationProcessor 클래스 입니다. 전체 코드는 이 글 하단의 링크에서 확인하실 수 있습니다.뭔가 특별한 방식이 있을것 같지만 전 그냥 다음과 같이 무식하게 작성하였습니다.여기서 중요한 점은 세가지 입니다. 현재 클래스가 record 인지 일반 클래스인지 구분 -&gt; Expression 클래스가 다름 제네릭 타입으로 원시 타입을 사용할 수 없기 때문에 원시 타입을 참조 타입으로 변경 바인딩 map 객체를 만들때는 순서를 보장하는 LinkedHashMap 사용 (위에서 설명)이제 현재 프로젝트가 AnnotationProcessor로써 동작하게 하기 위해 추가로 필요한 작업을 진행합니다. resources/META-INF/services/javax.annotation.processing.Processor 파일에 AnnotationProcessor 클래스 명시 pom.xml 에 아래 플러그인을 추가하여 컴파일러가 현재 프로젝트를 컴파일할때 annotation processing 작업을 건너 뛰도록 함필요한 작업을 진행하였다면 다음 명령어를 실행하여 local maven repository에 현재 프로젝트가 배포될 수 있도록 합니다.clean install -DskipTests5. 테스트이제 새로운 프로젝트를 하나 만들고 위에서 만든 작업물을 테스트해 보도록 하겠습니다.배포된 프로젝트와 querydsl-apt를 의존성에 추가하고 maven compiler plugin을 추가하고 annotationProcessor를 등록합니다.&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.11.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;17&lt;/source&gt; &lt;target&gt;17&lt;/target&gt; &lt;generatedSourcesDirectory&gt;target/generated-sources/querydsl&lt;/generatedSourcesDirectory&gt; &lt;annotationProcessors&gt; &lt;annotationProcessor&gt; com.querydsl.apt.jpa.JPAAnnotationProcessor &lt;/annotationProcessor&gt; &lt;annotationProcessor&gt; com.keencho.querydsl.KcQuerydslAnnotationProcessor &lt;/annotationProcessor&gt; &lt;/annotationProcessors&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;이제 반환받기 원하는 class, record에 어노테이션을 붙이고 프로젝트를 컴파일 합니다. :warning: 프로젝션 클래스에서 빈 생성자를 필요로 하기 때문에 아래 어노테이션이 붙은 클래스에는 빈 생성자가 필수로 존재해야 합니다. (없다면 NoSuchMethodException 발생)@KcQueryProjectionpublic record CustomerRecord( Long id, String loginId, String password, String name) { }테스트 코드를 작성하고 테스트를 수행합니다.var q = QCustomer.customer;var predicate = new BooleanBuilder();predicate.and(q.name.contains(\"1\"));var bindings = KcQCustomerDTO.builder() .id(q.id) .name(q.name) .loginId(q.loginId) .password(q.password) .build();var list = jpaQueryFactory .select(bindings) .from(q) .where(predicate) .orderBy(q.loginId.asc()) .fetch();var bindings2 = KcQCustomerRecord.builder() .id(q.id) .name(q.name) .loginId(q.loginId) .password(q.password) .build();var list2 = jpaQueryFactory .select(bindings2) .from(q) .where(predicate) .orderBy(q.loginId.asc()) .fetch();for (var i = 0; i &lt; list.size(); i ++) { Assert.isTrue(list.get(i).getName().equals(list2.get(i).name())); Assert.isTrue(list.get(i).getPassword().equals(list2.get(i).password())); Assert.isTrue(list.get(i).getId().equals(list2.get(i).id())); Assert.isTrue(list.get(i).getLoginId().equals(list2.get(i).loginId()));}코드 확인전체 코드는 이곳 에서 확인하실 수 있습니다." }, { "title": "QueryProjection 에서 한단계 더 나아가 QueryProjectionBuilder 만들기 (1)", "url": "/posts/querydsl-qbuilder-qsetter-1/", "categories": "QueryDSL", "tags": "Java, JPA, QueryDSL", "date": "2022-09-28 07:12:00 +0900", "snippet": "QueryProjection 에서 한단계 더 나아가 QueryProjectionBuilder 만들기 (1) - 개요Querydsl을 사용해 db조회를 하는 경우 엔티티 자체를 반환하는 경우는 잘 없습니다. 대부분 필요한 필드만 지정하여 Projections를 사용하여 조회하게 되는것 같습니다.잘 알려진 projection 방법에는 3가지가 있습니다. Projections.bean - getter / setter 필요 Projections.constructor - field 직접 접근 @QueryProjection - ConstructorExpression를 상속받은 Q 클래스 생성 첫번째 방식과 두번째 방식은 조회하려는 필드명이 일치해야 한다는 단점이 있습니다.public class SimpleDTO { Long deliveryId; Long orderId; String field;}...var p = Projections.fields(SimpleDTO.class, q.deliveryId, q.order.orderId, q.fromAddress);예를들어 위와같이 조회했을때 SimpleDTO의 field 와 entity q 객체의 fromAdress의 필드명이 일치하지 않기 때문에 결과를 조회했을때 null이 넘어오게 됩니다.그래서 처음 querydsl을 사용하고 실무에 적용때는 @QueryProjection 방식을 사용했었습니다. Entity의 Q객체를 생성하는 것처럼 DTO Class의 Q객체를 생성하고, 생성한 Q 객체의 생성자를 사용하는 방식이기 때문에 조회하고자 하는 필드의 순서가 정확해야 한다는 조건이 있습니다.또, 기존 코드와의 호환성을 유지하려면 오리지날 DTO Class에 필드를 추가할때마다 생성자를 하나씩 추가해야 한다는 문제가 생겼습니다.public class SimpleDTO { Long deliveryId; Long orderId; String field; @QueryProjection public SimpleDTO(Long deliveryId, Long orderId, String field) { this.deliveryId = deliveryId; this.orderId = orderId; this.field = field; } @QueryProjection public SimpleDTO(Long deliveryId) { this.deliveryId = deliveryId; } @QueryProjection public SimpleDTO(Long deliveryId, Long orderId) { this.deliveryId = deliveryId; this.orderId = orderId; } @QueryProjection public SimpleDTO(Long orderId, String field) { this.orderId = orderId; this.field = field; }}사실 위 문제는 어플리케이션을 설계할때 ‘이렇게 저렇게 하자~’ 라고 미리 정해두고 가면 충분히 해결할수 있는 문제입니다. 하지만 실무에서는 기획이 바뀌는일이 허다하죠. 그래서 저는 저만의 방식으로 Projection을 만들어보기로 하였습니다.초기의 생각처음에 생각했던 방식은 map을 만들고, 조회할 필드와 Expression을 map에 집어넣고 조회후에 다시 리플렉션으로 dto에 값을 꽂아넣는 방식이었습니다. 코드로 풀면 다음과 같습니다.public class SimpleDTO { Long deliveryId; Long orderId; String field; private static Map&lt;String, Expression&lt;?&gt;&gt; bindings; static { var q = Q.delivery; bindings.put(\"deliveryId\", q.deliveryId); bindings.put(\"orderId\", q.order.orderId); bindings.put(\"field\", q.fromAddress); }}...queryFactory.select(SimpleDTO.bindings).from(q).fetch();이 방식은 querydsl에 그다지 의존적이지 않으나 컴파일 시점에 오류를 잡을수 없다는 문제가 존재합니다. 저는 querydsl에 의존적이더라도 오류를 잡을수 있는 방식을 선호하기 때문에 위 방식은 선택하지 않았습니다.QBuilder를 만들자!QueryProjection이 Q클래스를 생성하는 것처럼 builder, setter를 사용할 수 있는 저만의 Q클래스를 만들어보기로 하였습니다. 생성되는 Q 클래스는 다음과 같습니다./** * com.keencho.libormtest.model.KcQCustomerDTO is a KcQuerydsl Projection type for CustomerDTO */public class KcQCustomerDTO extends KcExpression&lt;CustomerDTO&gt; { private static final long serialVersionUID = 842345744L; public KcQCustomerDTO(Builder builder) { super(CustomerDTO.class, builder.buildBindings()); } public static Builder builder() { return new Builder(); } public static class Builder { private com.querydsl.core.types.Expression&lt;java.lang.Long&gt; id; public Builder id(com.querydsl.core.types.Expression&lt;java.lang.Long&gt; id) { this.id = id; return this; } private com.querydsl.core.types.Expression&lt;java.lang.String&gt; loginId; public Builder loginId(com.querydsl.core.types.Expression&lt;java.lang.String&gt; loginId) { this.loginId = loginId; return this; } private com.querydsl.core.types.Expression&lt;java.lang.String&gt; password; public Builder password(com.querydsl.core.types.Expression&lt;java.lang.String&gt; password) { this.password = password; return this; } private com.querydsl.core.types.Expression&lt;java.lang.String&gt; name; public Builder name(com.querydsl.core.types.Expression&lt;java.lang.String&gt; name) { this.name = name; return this; } private com.querydsl.core.types.Expression&lt;java.lang.Integer&gt; age; public Builder age(com.querydsl.core.types.Expression&lt;java.lang.Integer&gt; age) { this.age = age; return this; } public KcQCustomerDTO build() { return new KcQCustomerDTO(this); } public Map&lt;String, Expression&lt;?&gt;&gt; buildBindings() { Map&lt;String, Expression&lt;?&gt;&gt; bindings = new LinkedHashMap&lt;&gt;(); bindings.put(\"id\", this.id); bindings.put(\"loginId\", this.loginId); bindings.put(\"password\", this.password); bindings.put(\"name\", this.name); bindings.put(\"age\", this.age); return bindings; } }}이렇게 되면 필드를 추가해도 기존의 코드에 영향을 주지 않고 런타임에 오류도 잡을수 있고 builder 패턴으로 projection을 생성할 수 있습니다. 물론 바인딩 객체를 만드는것이 type-safe하진 않지만 클래스를 만드는 시점 (컴파일 시점)에 에러가 발생하기 때문에 적어도 런타임에 오류가 발생하는건 막을 수 있게 됩니다.조회할때는 이런식으로 조회할 수 있습니다.var q = QCustomer.customer;var bindings = KcQCustomerDTO.builder() .id(q.id) .name(q.name) .loginId(q.loginId) .build();var predicate = new BooleanBuilder();predicate.and(q.name.contains(\"김\"));var list = jpaQueryFactory .select(bindings) .from(q) .where(predicate) .fetch();다음 포스팅에서…라이브러리 제작 과정 및 자세한 코드는 다음 포스팅에서 다루도록 하겠습니다." }, { "title": "Spring Security custom library 만들기", "url": "/posts/spring-security-custom-library/", "categories": "Spring Security", "tags": "Java, Spring, Spring Security", "date": "2022-09-08 08:12:00 +0900", "snippet": "Spring Security custom library 만들기운영중인 시스템에 적용된 Spring Security custom library를 제 방식대로 바꿔보고 겸사겸사 Spring Security의 동작 방식, 원리에 대해 더 깊게 공부하고자 작성한 글입니다. 이 포스팅은 Spring Security가 어떤 방식으로 인증 / 인가 처리하는지 어느정도 이해하고 있다는 가정 하에 작성된 포스팅입니다.사용된 라이브러리 의존성dependencies { // spring implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.boot:spring-boot-starter-security' implementation 'org.springframework.boot:spring-boot-starter-data-jpa' // jwt implementation 'io.jsonwebtoken:jjwt-api:0.11.5' implementation 'io.jsonwebtoken:jjwt-impl:0.11.5' implementation 'io.jsonwebtoken:jjwt-jackson:0.11.5' // lombok compileOnly 'org.projectlombok:lombok' annotationProcessor 'org.projectlombok:lombok'}Spring Data JPA를 사용하여 entity 객체를 중심으로 인증이 진행되도록 하였습니다.핵심 코드 작성1. 베이스 모델첫째는 베이스 모델입니다. 추후 작성될 대부분의 코드의 제네릭에서 이 모델이 사용되기 때문에 가장 중요한 코드라고 생각됩니다.@EqualsAndHashCode(callSuper = false)@Data@MappedSuperclasspublic abstract class KcAccountBaseModel { @Column(nullable = false) protected String loginId; @Column(nullable = false) protected String password; @Column(nullable = false) protected boolean accountNonExpired = true; @Column(nullable = false) protected boolean accountNonLocked = true; @Column(nullable = false) protected boolean credentialsNonExpired = true; @Column(nullable = false) protected boolean enabled = true; @Column(nullable = false) protected LocalDateTime dtCreatedAt = LocalDateTime.now(); @Column(nullable = false) protected LocalDateTime dtUpdatedAt = LocalDateTime.now(); protected LocalDateTime dtPasswordChangedAt = LocalDateTime.now(); protected LocalDateTime dtLastLoggedInAt; protected LocalDateTime dtLastAccessedAt; @Column(nullable = false) protected int loginAttemptCount = 0;}각각의 테이블은 용도 (사용자, 관리자, 시스템 등..) 으로 나뉘게 되고 각각의 테이블 엔티티들은 위의 베이스 모델을 상속받아 작성될 것입니다.또다른 핵심 베이스 모델은 Security Account 모델입니다.@Datapublic class KcSecurityAccount implements UserDetails, CredentialsContainer { private final Class&lt;?&gt; accountEntityClass; private final String loginId; private String password; private final Set&lt;GrantedAuthority&gt; authorities; private final boolean accountNonExpired; private final boolean accountNonLocked; private final boolean credentialsNonExpired; private final boolean enabled; private final Object data; public KcSecurityAccount(Class&lt;?&gt; accountEntityClass, String loginId, String password, Set&lt;GrantedAuthority&gt; authorities, boolean accountNonExpired, boolean accountNonLocked, boolean credentialsNonExpired, boolean enabled, Object data) { this.accountEntityClass = accountEntityClass; this.loginId = loginId; this.password = password; this.authorities = authorities; this.accountNonExpired = accountNonExpired; this.accountNonLocked = accountNonLocked; this.credentialsNonExpired = credentialsNonExpired; this.enabled = enabled; this.data = data; } @Override public void eraseCredentials() { this.password = null; } @Override public String getUsername() { return null; }}위 모델은 로그인시 인증 객체로 쓰일 모델이며(UserDetails, CredentialsContainer 구현) 각각의 엔티티별로 또다른 객체 데이터를 가지고 있어야 할수도 있기 때문에 Object data 를 통해 이를 충족시키도록 하였습니다.2. 베이스 모델 리포지토리public interface KcAccountRepository&lt;T extends KcAccountBaseModel, ID&gt; extends JpaRepository&lt;T, ID&gt; { T findByLoginId(String loginId);}JPARepository 를 상속하는 베이스모델 리포지토리이며 테이블 엔티티들은 이 리포지토리를 상속하는 또하나의 리포지토리를 작성해야 합니다.3. 로그인 매니저public interface KcLoginManager&lt;T extends KcAccountBaseModel, R extends KcAccountRepository&lt;T, ?&gt;&gt; extends UserDetailsService, KcAccountResolver&lt;T&gt; { Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); int getMaxLoginAttemptCount(); int getMaxLongTermNonUseAllowDay(); @Transactional(readOnly = true) T findByLoginId(String loginId); @Transactional void updateOnLoginSuccess(String loginId); @Transactional int updateLoginAttemptAccount(String loginId); @Transactional void lockAccount(String loginId);}Spring Security를 접해보셨다면 한번쯤은 봤을 loadUserByUsername(String username) 메소드를 가지고있는 인터네이스는 UserDetailsService를 상속하는 로그인 매니저 인터페이스 입니다.뒤에 보면 KcAccountResolver라는 인터페이스를 상속받았는데요, 이것은 핵심 코드 작성 이후 설명할 예정입니다.public interface KcAccountResolver&lt;T extends KcAccountBaseModel&gt; { T getAccountBySecurityAccount(KcSecurityAccount securityAccount);}이런 형태라는것만 알아두시면 좋을것 같습니다.다음은 로그인 매니저를 구현하는 기본 로그인 매니저 클래스 입니다.public abstract class KcDefaultLoginManager&lt;T extends KcAccountBaseModel, R extends KcAccountRepository&lt;T, ?&gt;&gt; implements KcLoginManager&lt;T, R&gt; { private final R repo; public KcDefaultLoginManager(R r) { this.repo = r; } public abstract Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); public abstract int getMaxLoginAttemptCount(); public abstract int getMaxLongTermNonUseAllowDay(); public abstract UserDetails loadUserByUsername(String username) throws UsernameNotFoundException; @Override public T findByLoginId(String loginId) { return repo.findByLoginId(loginId); } @Override public void updateOnLoginSuccess(String loginId) { var account = this.findByLoginId(loginId); Assert.notNull(account, \"account must not be null!\"); var now = LocalDateTime.now(); account.setDtLastAccessedAt(now); account.setDtLastLoggedInAt(now); account.setLoginAttemptCount(0); repo.save(account); } @Override public int updateLoginAttemptAccount(String loginId) { var account = this.findByLoginId(loginId); if (account == null) { return -1; } var currentCount = account.getLoginAttemptCount(); var targetCount = currentCount + 1; if (targetCount &gt;= this.getMaxLoginAttemptCount()) { // 잠궈야 한다면 로그인시도는 초기화한다. targetCount = 0; account.setAccountNonLocked(false); } account.setLoginAttemptCount(targetCount); repo.save(account); return targetCount; } @Override public void lockAccount(String loginId) { var account = this.findByLoginId(loginId); Assert.notNull(account, \"account must not be null!\"); account.setAccountNonLocked(false); repo.save(account); } @Override public T getAccountBySecurityAccount(KcSecurityAccount securityAccount) { return this.findByLoginId(securityAccount.getLoginId()); }}추상메소드로 선언된 메소드들은 엔티티별, 비즈니스 로직별로 다를수 있는 부분이기 때문에 따로 작성하였습니다. (예 - 관리자는 로그인시도 3회 허용, 사용자는 로그인시도 5회 허용 등)그 외 계정 잠금이나 로그인 횟수 관리등 제가 생각했을때 로그인에 있어 가장 기본이라 생각되는 부분을 구현하였습니다.4. 인증 프로바이더다음은 인증 프로바이더입니다. AbstractUserDetailsAuthenticationProvider 를 상속받은 클래스이며 PasswordEncoder와 UserDetailsService를 생성자로 주입받습니다.아까 기본 로그인 매니저를 작성할때 loadUserByUsername() 메소드를 구현해야 했지만 대부분 엔티티별로 다른 로직을 적용해야 하기 때문에 추상 메소드로 두었지요? 바로 그 메소드를 사용하기 위해 UserDetailsService를 주입받습니다.또한 AbstractUserDetailsAuthenticationProvider 클래스의 2개의 추상 메소드를 재정의 합니다. 사용하는 용도에 따라 다른 메소드들도 재정의 할수 있겠네요.public class KcAuthenticationProvider extends AbstractUserDetailsAuthenticationProvider { private final PasswordEncoder passwordEncoder; @Getter private final UserDetailsService userDetailsService; public KcAuthenticationProvider(PasswordEncoder passwordEncoder, UserDetailsService userDetailsService) { this.passwordEncoder = passwordEncoder; this.userDetailsService = userDetailsService; } @Override protected void additionalAuthenticationChecks(UserDetails userDetails, UsernamePasswordAuthenticationToken authentication) throws AuthenticationException { if (authentication.getCredentials() == null) { throw new BadCredentialsException(messages.getMessage(\"AbstractUserDetailsAuthenticationProvider.badCredentials\", \"Bad credentials\")); } var passedPassword = authentication.getCredentials().toString(); if (!passwordEncoder.matches(passedPassword, userDetails.getPassword())) { throw new BadCredentialsException(messages.getMessage(\"AbstractUserDetailsAuthenticationProvider.badCredentials\", \"Bad credentials\")); } } @Override protected UserDetails retrieveUser(String username, UsernamePasswordAuthenticationToken authentication) throws AuthenticationException { UserDetails loadedUser; try { loadedUser = this.userDetailsService.loadUserByUsername(username); } catch (UsernameNotFoundException notFoundException) { throw notFoundException; } catch (Exception ex) { throw new InternalAuthenticationServiceException(ex.getMessage(), ex); } if (loadedUser == null) { throw new InternalAuthenticationServiceException(\"DefaultUserDetailsService returned null, it must be not null!\"); } return loadedUser; }}5. 인증 프로바이더 매니저인증 프로바이더들을 관리할 매니저 클래스를 작성하겠습니다.public class KcAuthenticationProviderManagerImpl implements KcAuthenticationProviderManager { private final Map&lt;Class&lt;?&gt;, KcAuthenticationProvider&gt; authenticationProviderMap = new HashMap&lt;&gt;(); @Override public Collection&lt;KcAuthenticationProvider&gt; getProviders() { return this.authenticationProviderMap.values(); } @Override public KcAuthenticationProvider getAuthenticationProvider(Class&lt;?&gt; accountEntityClass) { return this.authenticationProviderMap.get(accountEntityClass); } public void addAuthenticationProvider(Class&lt;?&gt; accountEntityClass, KcAuthenticationProvider authenticationProvider) { Assert.isTrue(!this.authenticationProviderMap.containsKey(accountEntityClass), \"authenticationProviderMap already has key\"); this.authenticationProviderMap.put(accountEntityClass, authenticationProvider); }}위 클래스는 추후 사용할 어플리케이션에서 bean 으로 등록되어야 합니다. 그리고 각각의 엔티티와 인증 프로바이더를 addAuthenticationProvider 메소드를 통해 추가해야 합니다. (이는 예제 작성할때 다루게 됩니다.)6. JWT 프로바이더Spring Security는 기본적으로 세션 방식입니다. 다만 세션 사용이 제약되는 환경이 있을텐데요. 예를 들어 제 경우 앱개발을 하는데 세션이 의도한대로 동작하지 않아 몇일동안 애먹은 적이 있습니다. 그런 경우를 대비하여 jwt 토큰방식으로 인증하는 대비책을 세워보았습니다. 주의! 이 포스팅에서는 refresh token을 사용하지 않고 1개의 오리지널 토큰만을 사용합니다. 오리지널 토큰의 유효기간을 길게 잡거나 refresh token 인증 방식을 추가로 적용해보세요.public abstract class KcDefaultJwtTokenProvider implements KcJwtTokenProvider { private final UserDetailsService userDetailsService; private final String claimsKeyName = \"loginId\"; private final SecretKey secretKey; public KcDefaultJwtTokenProvider(String secretKey, UserDetailsService userDetailsService) { this.secretKey = Keys.hmacShaKeyFor(secretKey.getBytes(StandardCharsets.UTF_8)); this.userDetailsService = userDetailsService; } public abstract long getExpireDays(); public abstract String getCookieName(); @Override public Authentication getAuthentication(String token) { var claims = this.getClaims(token); var loginId = claims.get(this.claimsKeyName, String.class); if (!StringUtils.hasText(loginId)) { return null; } var userDetails = userDetailsService.loadUserByUsername(loginId); return new UsernamePasswordAuthenticationToken(userDetails, null, userDetails.getAuthorities()); } @Override public String resolveToken(HttpServletRequest request) { var cookies = request.getCookies(); if (cookies != null) { for (var cookie: cookies) { if (this.getCookieName().equals(cookie.getName())) { return cookie.getValue(); } } } return null; } @Override public String createToken(KcSecurityAccount securityAccount) { var claims = Jwts.claims().setSubject(UUID.randomUUID().toString()); claims.put(this.claimsKeyName, securityAccount.getLoginId()); claims.put(\"data\", securityAccount.getData()); var limit = LocalDateTime.now().plusDays(this.getExpireDays()); var date = new Date(); return Jwts.builder().setClaims(claims) .setIssuedAt(date) .setExpiration(Date.from(limit.atZone(ZoneId.systemDefault()).toInstant())) .signWith(this.secretKey, SignatureAlgorithm.HS256) .compact(); } @Override public boolean isValidate(String jwtToken) { if (!StringUtils.hasText(jwtToken)) { return false; } try { var claims = this.getClaims(jwtToken); return claims.getExpiration().after(new Date()); } catch (Exception e) { // ExpiredJwtException, MalformedJwtException, SignatureException 이 넘어올수 있다. // 만료가 되었다면 아예 예외가 넘어올수 있으므로 예외처리 블록으로 묶고 false를 리턴함. return false; } } private Claims getClaims(String jwtToken) { return Jwts .parserBuilder() .setSigningKey(this.secretKey) .build() .parseClaimsJws(jwtToken) .getBody(); }}유효기간 getExpiresDays()와 쿠키 이름 getCookieName()은 추상메소드로 선언하였습니다. jwt 토큰 관련해서는 jjwt 라이브러리를 사용하였습니다.7. jwt 인증 필터jwt 인증을 사용하려면 결국 필터단에서 cookie를 파싱하여 SecurityContextHolder 에 객체를 세팅해줘야 하는데요, 이를위한 공용 필터 코드를 작성 하겠습니다.public class KcJwtAuthenticationFilter extends GenericFilterBean { private final KcJwtTokenProvider jwtTokenProvider; public KcJwtAuthenticationFilter(KcJwtTokenProvider jwtTokenProvider) { this.jwtTokenProvider = jwtTokenProvider; } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { var token = this.jwtTokenProvider.resolveToken((HttpServletRequest) request); if (this.jwtTokenProvider.isValidate(token)) { var authentication = this.jwtTokenProvider.getAuthentication(token); SecurityContextHolder.getContext().setAuthentication(authentication); } chain.doFilter(request, response); }}8. 로그인 서비스마지막 핵심 코드인 로그인 서비스 입니다.public abstract class KcDefaultLoginService&lt;T extends KcAccountBaseModel, R extends KcAccountRepository&lt;T, ?&gt;&gt; implements KcLoginService&lt;T, R&gt; { private final Logger logger = LoggerFactory.getLogger(getClass()); private final KcAuthenticationProviderManager authenticationProviderManager; private final KcLoginManager&lt;T, R&gt; accountLoginManager; private final KcJwtTokenProvider jwtTokenProvider; private boolean isUseJwtToken = false; public KcDefaultLoginService(KcAuthenticationProviderManager authenticationProviderManager, KcLoginManager&lt;T, R&gt; accountLoginManager, KcJwtTokenProvider jwtTokenProvider) { this.authenticationProviderManager = authenticationProviderManager; this.accountLoginManager = accountLoginManager; this.jwtTokenProvider = jwtTokenProvider; // jwtTokenProvider가 주입되었다면 jwt token 방식을 사용하는 것이라고 간주한다. if (this.jwtTokenProvider != null) { this.isUseJwtToken = true; } } public abstract Class&lt;T&gt; getAccountEntityClass(); @Override public Object login(HttpServletResponse response, String loginId, String password) { Authentication authentication; var token = new UsernamePasswordAuthenticationToken(loginId, password); try { var authenticationProvider = authenticationProviderManager.getAuthenticationProvider(getAccountEntityClass()); // 코딩 잘못임. bean에 엔티티 클래스 / 매니저 등록 안함. if (authenticationProvider == null) { logger.error(\"system error: authentication provider manager doesn't have target entity class. check your bean configuration\"); throw new KcSystemException(); } authentication = authenticationProvider.authenticate(token); // 여기까지 들어왔으면 아이디 / 비밀번호는 일치한다는 의미임 // 계정을 잠금처리하는 batch 따로 돌아야 하겠지만 돌기 전에 이곳에 들어왔다고 가정하고 장기 미접속 계정 잠금처리 if (this.isLongTermNotUsed(loginId)) { accountLoginManager.lockAccount(loginId); throw new LockedException(\"account locked\"); } // jwt 인증은 별도로 구현한 인증과정을 거쳐야 한다는 뜻. if (!this.isUseJwtToken) { SecurityContextHolder.getContext().setAuthentication(authentication); } } catch (BadCredentialsException ex) { var cnt = accountLoginManager.updateLoginAttemptAccount(loginId); // 계정 없음 if (cnt == -1) { throw new KcLoginFailureException(); } else { throw new KcLoginFailureException(cnt, accountLoginManager.getMaxLoginAttemptCount()); } } catch (LockedException ex) { // 계정이 잠긴 이유는 일단 2가지라고 생각함. // 1. N회 비밀번호 틀린 경우 // 2. 장기 미접속 // 따라서 장기 미접속 여부 판단해야함. if (isLongTermNotUsed(loginId)) { throw new KcAccountLongTermNotUsedException(); } throw new KcAccountLockedException(); } catch (DisabledException ex) { throw new KcAccountDisabledException(); } catch (KcSystemException ex) { throw new KcSystemException(); } catch (Exception ex) { throw new KcLoginFailureException(\"login failure\"); } accountLoginManager.updateOnLoginSuccess(token.getPrincipal().toString()); var securityUser = (KcSecurityAccount) authentication.getPrincipal(); if (this.isUseJwtToken) { var jwtToken = this.jwtTokenProvider.createToken(securityUser); // 토큰을 쿠키에 저장한다. 이때 이름은 tokenProvider에 정의해둔 이름을 사용한다. // 필터가 그 이름을 알수 있어야 하는데, 여기서는 KcDefaultJwtTokenProvider에도 동일한 jwtTokenProvider를 주입받게 해두었다. // 구현하는 사람 맘대로 하면 된다. 어쨌든 이 쿠키 이름을 필터가 알아야 한다. var cookie = ResponseCookie.from(this.jwtTokenProvider.getCookieName(), jwtToken) .sameSite(\"None\") .httpOnly(true) .secure(true) .maxAge(this.jwtTokenProvider.getExpireDays() * 86400) .build(); response.addHeader(\"Set-Cookie\", cookie.toString()); return jwtToken; } return securityUser.getData(); } /** * 장기 미접속여부 판단 * * @param loginId * @return true if not use for long time */ private boolean isLongTermNotUsed(String loginId) { var account = accountLoginManager.findByLoginId(loginId); var maxLongTermNonUseAllowDay = accountLoginManager.getMaxLongTermNonUseAllowDay(); if (account != null) { if (maxLongTermNonUseAllowDay &gt; 0) { if (account.getDtLastAccessedAt() != null) { if (account.getDtLastAccessedAt().plusDays(maxLongTermNonUseAllowDay).isBefore(LocalDateTime.now())) { return true; } } } } return false; }}jwt 토큰 프로바이더가 주입되었다면 jwt 인증방식을 사용한다고 체크합니다. 또한 jwt 인증방식을 사용한다면 ` SecurityContextHolder.getContext().setAuthentication()` 을 통해 세션에 인증정보를 저장하지 않고 쿠키를 사용합니다.또한 장기미접속 여부 판단을 기본적으로 하게 하였습니다. 이 또한 비즈니스 로직에 따라 재정의 될수 있겠네요.예제 작성간단한 웹 어플리케이션 예제를 작성해 보겠습니다.관리자 와 일반 사용자가 있습니다. 이중 관리자는 jwt 토큰 인증 방식을 사용합니다.1. 엔티티 모델KcAccountBaseModel을 상속받는 엔티티 모델을 작성합니다.@EqualsAndHashCode(callSuper = false, onlyExplicitlyIncluded = true)@Data@Entity@Table(name = \"ADMIN_ACCOUNT\")public class AdminAccount extends KcAccountBaseModel { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id;}@EqualsAndHashCode(callSuper = false, onlyExplicitlyIncluded = true)@Data@Entity@Table(name = \"USER_ACCOUNT\")public class UserAccount extends KcAccountBaseModel { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id;}2. 리포지토리KcAccountRepository를 상속받는 리포지토리를 작성합니다.@Repositorypublic interface AdminAccountRepository extends KcAccountRepository&lt;AdminAccount, Long&gt; {}@Repositorypublic interface UserAccountRepository extends KcAccountRepository&lt;UserAccount, Long&gt; {}3. 로그인 매니저KcDefaultLoginManager 를 상속받는 로그인 매니저를 작성합니다.로그인 매니저를 작성하기 전 KcSeurityAccount 객체의 data에 들어갈 객체를 정의하겠습니다.@Datapublic class LoginAccountData { private String loginId; private LoginAccountType loginAccountType; private Set&lt;String&gt; authorities;}다음은 엔티티별 로그인 매니저 입니다.@Componentpublic class AdminLoginManager extends KcDefaultLoginManager&lt;AdminAccount, AdminAccountRepository&gt; { public AdminLoginManager(AdminAccountRepository adminAccountRepository) { super(adminAccountRepository); } @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() { var set = new HashSet&lt;String&gt;(); set.add(AccountRoleCode.ROLE_COMMON); set.add(AccountRoleCode.ROLE_ADMIN); return set.stream().map(SimpleGrantedAuthority::new).collect(Collectors.toList()); } @Override public int getMaxLoginAttemptCount() { return 5; } @Override public int getMaxLongTermNonUseAllowDay() { return 90; } @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException { if (!StringUtils.hasText(username)) { throw new UsernameNotFoundException(\"Username must be provided!\"); } var account = this.findByLoginId(username); if (account == null) { throw new UsernameNotFoundException(\"Username not found, username=\" + username); } var authorities = this.getAuthorities(); var loginData = new LoginAccountData(); loginData.setLoginId(account.getLoginId()); loginData.setLoginAccountType(LoginAccountType.ADMIN); loginData.setAuthorities(authorities.stream().map(GrantedAuthority::getAuthority).collect(Collectors.toSet())); return new KcSecurityAccount( AdminAccount.class, account.getLoginId(), account.getPassword(), new HashSet&lt;&gt;(authorities), account.isAccountNonExpired(), account.isAccountNonLocked(), account.isCredentialsNonExpired(), account.isEnabled(), loginData ); }}@Componentpublic class UserLoginManager extends KcDefaultLoginManager&lt;UserAccount, UserAccountRepository&gt; { public UserLoginManager(UserAccountRepository userAccountRepository) { super(userAccountRepository); } @Override public Collection&lt;? extends GrantedAuthority&gt; getAuthorities() { var set = new HashSet&lt;String&gt;(); set.add(AccountRoleCode.ROLE_COMMON); set.add(AccountRoleCode.ROLE_USER); return set.stream().map(SimpleGrantedAuthority::new).collect(Collectors.toList()); } @Override public int getMaxLoginAttemptCount() { return 3; } @Override public int getMaxLongTermNonUseAllowDay() { return 30; } @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException { if (!StringUtils.hasText(username)) { throw new UsernameNotFoundException(\"Username must be provided!\"); } var account = this.findByLoginId(username); if (account == null) { throw new UsernameNotFoundException(\"Username not found, username=\" + username); } var authorities = this.getAuthorities(); var loginData = new LoginAccountData(); loginData.setLoginId(account.getLoginId()); loginData.setLoginAccountType(LoginAccountType.USER); loginData.setAuthorities(authorities.stream().map(GrantedAuthority::getAuthority).collect(Collectors.toSet())); return new KcSecurityAccount( UserAccount.class, account.getLoginId(), account.getPassword(), new HashSet&lt;&gt;(authorities), account.isAccountNonExpired(), account.isAccountNonLocked(), account.isCredentialsNonExpired(), account.isEnabled(), loginData ); }}여기서 GrantedAuthority 에 해당되는 role들은 그냥 일반 String text로 부여해도 되지만, 제 경우 아래와 같이 상수만 가지고 있는 클래스를 하나 만들어 사용하였습니다.public class AccountRoleCode { public static final String ROLE_COMMON = \"ROLE_COMMON\"; public static final String ROLE_ADMIN = \"ROLE_ADMIN\"; public static final String ROLE_USER = \"ROLE_USER\";}이렇게 관리하는 편이 추후 hasAuthority()로 경로 권한 체크하는 코드에서 이점을 가질수 있을 것이라 생각했습니다.4. bean 주입다음은 KcAccountResolverManager 와 관리자 엔티티 인증에서 사용될 KcJwtTokenProvider 를 bean으로 주입해야 합니다. PasswordEncoder의 경우 BcryptPasswordEncoder를 사용하였습니다.@Configurationpublic class SecurityBeanContainer { private static final String JWT_SECRET_KEY = \"Qi0P4mU8ABq6M9nMZG5y67E6hmNad14n\"; @Autowired AdminLoginManager adminAccountLoginManager; @Autowired UserLoginManager userLoginManager; @Bean public BCryptPasswordEncoder bCryptPasswordEncoder() { return new BCryptPasswordEncoder(); } @Bean public KcAuthenticationProviderManager authenticationProviderManager() { var authenticationProviderManager = new KcAuthenticationProviderManagerImpl(); authenticationProviderManager.addAuthenticationProvider( AdminAccount.class, new KcAuthenticationProvider(this.bCryptPasswordEncoder(), this.adminAccountLoginManager) ); authenticationProviderManager.addAuthenticationProvider( UserAccount.class, new KcAuthenticationProvider(this.bCryptPasswordEncoder(), this.userLoginManager) ); return authenticationProviderManager; } @Bean(\"adminJwtTokenProvider\") public KcJwtTokenProvider adminJwtTokenProvider() { return new KcDefaultJwtTokenProvider(JWT_SECRET_KEY, this.adminAccountLoginManager) { @Override public long getExpireDays() { return 30; } @Override public String getCookieName() { return \"KEENCHO_JWT_TOKEN\"; } }; }}JWT_SECRET_KEY 의 경우 이곳에서는 상수로 선언하였지만 따로 .properties 나 .yml 설정파일로 빼는 편이 보안상 좋을것 같습니다.각각의 로그인 서비스를 작성할때 KcJwtTokenProvider를 주입받아야 하는데 @Autowired는 사용하지 못하고 @Qualifier를 사용해야 하기 때문에 KcJwtTokenProvider의 경우 별도의 이름을 부여하였습니다.5. 로그인 서비스다음은 로그인 서비스 입니다. 앞서 말한대로 관리자의 경우 KcJwtTokenProvider를 생성자 생성시 주입합니다.@Servicepublic class AdminLoginService extends KcDefaultLoginService&lt;AdminAccount, AdminAccountRepository&gt; { public AdminLoginService( KcAuthenticationProviderManager authenticationProviderManager, AdminLoginManager accountLoginManager, @Qualifier(\"adminJwtTokenProvider\") KcJwtTokenProvider jwtTokenProvider ) { super(authenticationProviderManager, accountLoginManager, jwtTokenProvider); } @Override public Class&lt;AdminAccount&gt; getAccountEntityClass() { return AdminAccount.class; }}@Servicepublic class UserLoginService extends KcDefaultLoginService&lt;UserAccount, UserAccountRepository&gt; { public UserLoginService( KcAuthenticationProviderManager authenticationProviderManager, UserLoginManager accountLoginManager ) { super(authenticationProviderManager, accountLoginManager, null); } @Override public Class&lt;UserAccount&gt; getAccountEntityClass() { return UserAccount.class; }}6. 웹 예제첫째로 커스텀 securityFilterChain을 bean으로 주입합니다. 참고로 Spring Security 최신버전에서는 WebSecurityConfigurerAdapter 가 deprecated되었고 이제는 그냥 bean으로 주입하면 됩니다.@Configuration@EnableWebSecuritypublic class WebSecurityConfiguration { @Autowired KcAuthenticationProviderManager kcAuthenticationProviderManager; @Autowired KcJwtTokenProvider kcJwtTokenProvider; @Bean public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { for (var provider : kcAuthenticationProviderManager.getProviders()) { http.authenticationProvider(provider); } // POST 요청 오픈 http.csrf().disable(); http.headers().frameOptions().disable(); http.httpBasic().disable(); // jwt token 인증부 필터등록 http.addFilterBefore(new KcJwtAuthenticationFilter(kcJwtTokenProvider), UsernamePasswordAuthenticationFilter.class); http .antMatcher(\"/**\").authorizeRequests() // .antMatchers(\"/api/auth/test/admin\").hasAuthority(AccountRoleCode.ROLE_ADMIN) // .antMatchers(\"/api/auth/test/user\").hasAuthority(AccountRoleCode.ROLE_USER) // .antMatchers(\"/**\").permitAll() // .anyRequest().authenticated(); return http.build(); }}api/auth/test/** 의 경우 각각 경로에 따라 role을 가진 사용자만 접근할 수 있도록 설정하였습니다.다음은 로그인 컨트롤러 부분입니다.@RestController@RequestMapping(\"/api\")public class ApiController { @Autowired List&lt;KcLoginService&lt;? extends KcAccountBaseModel, ? extends KcAccountRepository&lt;?, ?&gt;&gt;&gt; kcLoginService; Map&lt;Class&lt;? extends KcAccountBaseModel&gt;, KcLoginService&lt;? extends KcAccountBaseModel, ? extends KcAccountRepository&lt;?, ?&gt;&gt;&gt; loginServiceMap; @PostConstruct public void initMap() { loginServiceMap = new HashMap&lt;&gt;(); for (var svc : kcLoginService) { loginServiceMap.put(svc.getAccountEntityClass(), svc); } } @PostMapping(\"/login\") public Object login( @RequestBody Map&lt;String, String&gt; map, @RequestParam String type, HttpServletResponse response ) { var id = map.get(\"id\"); var pw = map.get(\"pw\"); var clazz = \"admin\".equals(type) ? AdminAccount.class : UserAccount.class; return this.loginServiceMap.get(clazz).login(response, id, pw); } @GetMapping(\"/auth/test/admin\") public String authTestAdmin() { return \"success\"; } @GetMapping(\"/auth/test/user\") public String authTestUser() { return \"success\"; }}최초 시작시 각각의 로그인 서비스들을 map에 넣어 로그인시 엔티티를 통해 올바른 서비스를 찾아갈 수 있도록 하였습니다.마지막으로 위 api를 테스트하는 html을 만들어 테스트하면 모두 완료입니다. 다만 여기에서는 따로 다루지 않겠습니다. (깃허브 링크를 참고하세요.)마무리커스텀 라이브러리를 만들어보며 Spring Security에 대해 조금더 자세히 알아볼수 있었습니다. 위 라이브러리에는 몇개의 문제가 있는데요, 가장 큰 문제는 중복 인증 문제입니다.A 엔티티가 jwt 토큰 인증방식을 사용하고 B 엔티티는 기존 세션 인증방식을 사용한다고 가정했을때, jwt 인증 필터가 항상 세션 인증 필터 뒤에 있기 때문에 A와 B 모두 로그인된 상태해서 인증을 진행하면 A만 인증이 되는 문제가 일어나게 됩니다. 하지만 이는 하나의 어플리케이션 일때만 문제가 되며 만약 엔티티별로 다른 모듈을 만들어 각각의 bean을 주입받는 방식으로 멀티모듈 프로젝트를 구성하면 문제가 되지 않습니다.여기서는 설명하지 않았지만 WebMvcConfigurer 인터페이스의 addArgumentResolvers 메소드를 통해 ControllerAdvice 혹은 Controller의 시작점에 현재 인증된 인증 객체 정보를 가져올수 있게 할수도 있습니다. 예를들어 아래와 같이 말이지요. (깃허브 링크를 참고하세요.)@GetMapping(\"/auth/test/admin\")public AdminAccount authTestAdmin( @KcsAccount(required = true) AdminAccount adminAccount, @KcsAccount(accountType = KcsAccountType.SECURITY_ACCOUNT) KcSecurityAccount securityAccount ) { return adminAccount;}어쨌든 이 포스팅이 자신만의 Spring Security 인증을 구현하려는 분들께 도움이 되었으면 좋겠습니다. 라이브러리: https://github.com/keencho/lib-spring/tree/master/src/main/java/com/keencho/lib/spring/security예제: https://github.com/keencho/java-sandbox/tree/master/spring-security" }, { "title": "Hibernate, JPA 에서 발생하는 Cross Join 문제 해결하기", "url": "/posts/hibernate-cross-join/", "categories": "Hibernate", "tags": "JPA, Hibernate", "date": "2022-07-20 20:12:00 +0900", "snippet": "Hibernate, JPA 에서 발생하는 Cross Join 문제 해결하기개요평화롭게 코딩하던 어느날, 쿼리 결과가 의도한대로 리턴되지 않는 문제가 발생하였습니다. 쿼리에 문제가 있나 싶어 자세히 살펴봤지만 (queryDSL을 사용중이었습니다.) 자바 코드상의 문제는 찾을 수 없었습니다.데이터베이스에 날라가는 sql문을 보고 나서야 문제를 찾을수 있었고 이 포스팅에서는 그 문제와 해결 과정을 적어보려고 합니다.Cross Join 발생문제는 cross join 발생한다는 것이었습니다. 당연히 left join이 발생할 것이라고 생각하였지만 cross join이 발생하여 결과가 의도한대로 리턴되지 않은 것이지요.cross join은 두 테이블에서 행의 모든 조합을 반환하는 join입니다. 하이버네이트가 쿼리를 생성하는 과정에 where 조건절에 cross join과 관련된 조건이 추가되어 원하는 결과가 넘어오지 않았던 것이지요.하이버네이트는 join을 명시적으로 지정하지 않으면 암묵적으로 cross join으로 지정하는 경향(?) 이 었다고 합니다. 하이버네이트의 창시자인 Gavin King, Christian Bauer의 책인 Hibernate in Action 에는 이런 내용이 있습니다. Implicit joins are always directed along many-to-one or one-to-one association, never through a collection-valued association.암묵적 join은 항상 many-to-one 혹은 one-to-one 연관성에 따라 결정되며 컬렉션 값에 따라 지정되지 않는다고 합니다.또한 대부분의 데이터베이스 엔진들은 cross join에 대해 where 조건을 추가하기 때문에 결과적으로 cross join / where 에 의해 원하는 결과값이 나오지 않게 되는 것입니다.문제 재구현jpa / querydsl을 사용해 문제를 재구현 해보겠습니다.‘일’을 위한 테이블이 있습니다. 이 일은 개발자나 영업사원에게 배정될 수 있습니다. 개발자나 영업사원은 한 회사에 속해 있어야 합니다.@Entity@Data@AllArgsConstructor@NoArgsConstructor@Table(name = \"COMPANY\")public class Company { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; public Company(String name) { this.name = name; }}@Entity@Data@AllArgsConstructor@NoArgsConstructor@Table(name = \"DEV_TEAM_MEMBERS\")public class DevTeamMember { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; @ManyToOne(optional = false, fetch = FetchType.LAZY) @JoinColumn(name = \"COMPANY_ID\", foreignKey = @ForeignKey(name = \"DEV_TEAM_MEMBERS___COMPANY\")) private Company company; public DevTeamMember(String name, Company company) { this.name = name; this.company = company; }}@Entity@Data@AllArgsConstructor@NoArgsConstructor@Table(name = \"SALES_TEAM_MEMBERS\")public class SalesTeamMembers { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; @ManyToOne(optional = false, fetch = FetchType.LAZY) @JoinColumn(name = \"COMPANY_ID\", foreignKey = @ForeignKey(name = \"FK_SALES_TEAM_MEMBERS___COMPANY\")) private Company company; public SalesTeamMembers(String name, Company company) { this.name = name; this.company = company; }}@Entity@Data@AllArgsConstructor@NoArgsConstructor@Table(name = \"JOB\")public class Job { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; private LocalDateTime dtCreatedAt = LocalDateTime.now(); @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = \"DEV_TEAM_MEMBERS_ID\", foreignKey = @ForeignKey(name = \"FK_JOB___DEV_TEAM_MEMBERS\")) private DevTeamMember devTeamMembers; @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = \"SALES_TEAM_MEMBERS_ID\", foreignKey = @ForeignKey(name = \"FK_JOB___SALES_TEAM_MEMBERS\")) private SalesTeamMembers salesTeamMembers; public Job(String name, DevTeamMember devTeamMembers, SalesTeamMembers salesTeamMembers) { if (devTeamMembers == null &amp;&amp; salesTeamMembers == null) { throw new RuntimeException(\"Work must be assigned to one person.\"); } this.name = name; this.devTeamMembers = devTeamMembers; this.salesTeamMembers = salesTeamMembers; }}순서대로 회사, 개발자, 영업사원, 일 순서입니다. 테이블을 만들었으니 repository들을 각각 생성합니다.repository를 생성하였다면 test코드를 작성해 보겠습니다. 일단 더미데이터 생성 코드입니다.@BeforeEachvoid initDummyData() { var kakao = companyRepository.save(new Company(\"카카오\")); var naver = companyRepository.save(new Company(\"네이버\")); var kakao_dev_1 = devTeamMembersRepository.save(new DevTeamMember(\"카카오_개발자_1\", kakao)); var kakao_dev_2 = devTeamMembersRepository.save(new DevTeamMember(\"카카오_개발자_2\", kakao)); var naver_dev_1 = devTeamMembersRepository.save(new DevTeamMember(\"네이버_개발자_1\", naver)); var naver_dev_2 = devTeamMembersRepository.save(new DevTeamMember(\"네이버_개발자_2\", naver)); var kakao_sales_1 = salesTeamMembersRepository.save(new SalesTeamMembers(\"카카오_영업사원_1\", kakao)); var kakao_sales_2 = salesTeamMembersRepository.save(new SalesTeamMembers(\"카카오_영업사원_2\", kakao)); var naver_sales_1 = salesTeamMembersRepository.save(new SalesTeamMembers(\"네이버_영업사원_1\", naver)); var naver_sales_2 = salesTeamMembersRepository.save(new SalesTeamMembers(\"네이버_영업사원_2\", naver)); jobRepository.save(new Job(\"AWS EC2 인스턴스 생성\", kakao_dev_1, null)); jobRepository.save(new Job(\"AWS RDS 인스턴스 생성\", kakao_dev_2, null)); jobRepository.save(new Job(\"Spring Boot 프로젝트 생성\", naver_dev_1, null)); jobRepository.save(new Job(\"React 프로젝트 생성\", naver_dev_2, null)); jobRepository.save(new Job(\"매출목표 설정\", null, kakao_sales_1)); jobRepository.save(new Job(\"외근\", null, kakao_sales_2)); jobRepository.save(new Job(\"제품 소개서 만들기\", null, naver_sales_1)); jobRepository.save(new Job(\"월말 수금\", null, naver_sales_2));}다음은 테스트 코드입니다.@Testvoid test() { var q = QJob.job; var r = jpaQueryFactory .select( q.id, new CaseBuilder() .when(q.devTeamMembers.isNotNull()).then(q.devTeamMembers.name) .otherwise(q.salesTeamMembers.name), new CaseBuilder() .when(q.devTeamMembers.isNotNull()).then(q.devTeamMembers.company.name) .otherwise(q.salesTeamMembers.company.name) ) .from(q) .fetch(); Assert.isTrue(r.size() == 8, \"예상된 결과와 다릅니다.\");}job을 8개 생성하였기 때문에 위 테스트는 정상적으로 통과해야 합니다. 그러나 쿼리 결과 list의 size가 0이되어 테스트에 실패합니다.select job0_.id as col_0_0_, case when job0_.dev_team_members_id is not null then devteammem1_.name else salesteamm2_.name end as col_1_0_, case when job0_.dev_team_members_id is not null then company4_.name else company6_.name end as col_2_0_from job job0_ cross join dev_team_members devteammem1_ cross join company company4_ cross join sales_team_members salesteamm2_ cross join company company6_where job0_.dev_team_members_id = devteammem1_.id and devteammem1_.company_id = company4_.id and job0_.sales_team_members_id = salesteamm2_.id and salesteamm2_.company_id = company6_.id생성된 sql 문입니다. 당연히 left join이 걸릴줄 알았으나 cross join이 걸렸습니다. 또한 where 조건이 추가되어 원하는 결과가 나오지 않았습니다.문제 해결문제 해결은 간단합니다. join을 명시적으로 지정해주면 됩니다.@Testvoid test() { var q = QJob.job; var r = jpaQueryFactory .select( q.id, new CaseBuilder() .when(q.devTeamMembers.isNotNull()).then(q.devTeamMembers.name) .otherwise(q.salesTeamMembers.name), new CaseBuilder() .when(q.devTeamMembers.isNotNull()).then(q.devTeamMembers.company.name) .otherwise(q.salesTeamMembers.company.name) ) .from(q) .leftJoin(q.devTeamMembers).leftJoin(q.devTeamMembers.company) .leftJoin(q.salesTeamMembers).leftJoin(q.salesTeamMembers.company) .fetch(); Assert.isTrue(r.size() == 8, \"예상된 결과와 다릅니다.\");}join을 명시적으로 지정하고 테스트를 수행하니 정상적으로 통과합니다!select job0_.id as col_0_0_, case when job0_.dev_team_members_id is not null then devteammem2_.name else salesteamm5_.name end as col_1_0_, case when job0_.dev_team_members_id is not null then company3_.name else company6_.name end as col_2_0_from job job0_ left outer join dev_team_members devteammem1_ on job0_.dev_team_members_id = devteammem1_.id left outer join dev_team_members devteammem2_ on job0_.dev_team_members_id = devteammem2_.id left outer join company company3_ on devteammem2_.company_id = company3_.id left outer join sales_team_members salesteamm4_ on job0_.sales_team_members_id = salesteamm4_.id left outer join sales_team_members salesteamm5_ on job0_.sales_team_members_id = salesteamm5_.id left outer join company company6_ on salesteamm5_.company_id = company6_.id;생성된 sql문을 보면 의도한대로 join이 걸린 것을 확인할 수 있습니다.결론join을 명시적으로 지정하지 않으면 cross join이 걸려 원하는 결과를 얻을수 없는 문제가 발생할 수 있습니다. 이를 피하기 위한 설정(?) 같은게 있으면 좋겠지만 그런 설정은 아직까지 없는 것으로 보입니다.위 문제는 join을 명시적으로 지정하여 문제를 쉽게 해결할 수 있으니 문제 해결에 도움이 되면 좋겠습니다." }, { "title": "AWS Application Load Balancer에 SSL 인증서 적용하기", "url": "/posts/aws-alb-ssl/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2022-07-09 20:12:00 +0900", "snippet": "AWS Application Load Balancer에 SSL 인증서 적용하기개요이 포스팅에서는 .pem 확장자의 인증서를 AWS Application Load Balancer에 적용하는 방법에 대해 설명합니다. AWS ACM이 제공하는 인증서가 아닌 외부에서 ACM으로 가져온 인증서를 적용하는 방법에 대해 설명합니다.1. AWS Certificate Manager 콘솔 이동AWS 콘솔 화면에서 Certificate Manager를 검색하여 인증서 관리 화면으로 이동하고, 좌측의 인증서 가져오기 버튼을 눌러 인증서를 가져오는 화면으로 이동합니다.2. 인증서 세부정보인증서 세부정보에 입력할 인증서 본문, 인증서 프라이빗 키, 인증서 체인이 필요합니다.제 경우 DigiCert Global Root CA 라는 발급자에게 발급받았기 때문에 DigiCertCA 이름의 pem 파일이 존재합니다.일단 인증서 프라이빗 키 항목에 입력할 키를 만들어야 합니다. 키에 해당하는 파일을 열고 암호화를 해제해야 하는 파일인지 확인합니다. 아래 캡쳐와 같이 Proc-Type 항목과 DEK-Info 항목이 보인다면 암호화를 해체해야 하는 키파일 입니다.암호화를 해제한 새로운 .pem 파일을 만드는 방법은 간단합니다. 일단 openssl 이 설치된 환경이 필요한데요, Windows건 Linux건 상관 없습니다. 기존 key.pem 파일을 옮기고 아래 명령어를 수행합니다.openssl rsa -in private-key.pem -out {생성할 파일 이름}정상적인 파일이라면 비밀번호를 입력하라는 안내문구가 뜨는데요, 인증서를 발급받을 당시 발급자에게 제출한 비밀번호를 입력합니다. 파일이 생성되었다면 성공입니다.인증서 본문 - cert.pem인증서 프라이빗 키 - 암호화 해제한 키.pem인증서 체인 - 발급기관명.pem이중 인증서 체인의 경우 불필요한 경우도 있고 발급받은 인증서 형태에 따라 여러가지 단계로 조합해야 하는 경우도 있습니다. 자신의 인증서와 일치하는 인증서 체인 생성 방법은 검색을 통해 알아보시기 바랍니다.3. 인증서 확인 &amp; 생성모든 항목을 입력하였다면 2단계에서는 태그를 추가하고 3단계에서는 도메인, 만료 기간등을 확인한 후에 가져오기 버튼을 눌러 인증서를 가져옵니다.4. 인증서 적용 &amp; 결과학인인증서를 가져왔으니 ALB에 적용해야 합니다. 이를 위해 로드 밸런서 콘솔 화면으로 이동하여 로드 밸런서를 선택하고 리스너 -&gt; 인증서 보기/편집 순으로 이동합니다.저는 이미 인증서를 적용한 상태라 아래 캡쳐와 같이 인증서가 활성화된 상태이지만 처음 적용하는 경우 아무것도 없을 것입니다. Add certificate 버튼을 누르면 Add certificate to listener 화면이 등장하는데요, ACM and IAM certificates 부분에 방금 등록한 인증서가 표시될 것입니다. 해당 인증서를 체크하고 Include as pending below 버튼을 눌러 인증서를 적용하면 끝입니다.이제 조금 시간을 두고 새로고침하며 내 사이트에 접속해보세요.위 캡쳐와 같이 브라우저에서 ssl 인증서를 확인할 수 있고 https 프로토콜로 접속할 수 있다면 성공입니다." }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 6. CodeDeploy 연동 / 마무리", "url": "/posts/aws-cicd-6/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2022-06-17 20:12:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리CodeDeploy 연동 / 마무리S3 접근 IAM 사용자 생성무중단 배포의 흐름은 코드 커밋 - 커밋 트리거로 github action 수행 - gradle 빌드 - jar 파일 s3 전송 - code deploy 트리거 - code deploy가 배포 수행 으로 이루어 집니다.그 중 github action으로 빌드한 jar 파을 s3에 전송하려면 권한이 있어야 하겠죠? 권한을 가진 사용자를 만들어 보겠습니다. IAM -&gt; 사용자 생성으로 사용자 추가 화면으로 이동합니다.자격 증명 유형은 프로그래밍 방식 액세스를 선택합니다.정책 필터에 AmazonS3FullAccess || AWSCodeDeployFullAccess 를 검색해 나온 두가지 정책을 연결합니다. 그 후 쭉쭉 넘겨 사용자를 생성합니다.마지막 5단계에 도달하면 액세스 키와 비밀 액세스 키를 다운받을 수 있는 화면이 나타납니다. 이 자격 증명 csv는 해당 화면에서만 다운받을 수 있으니 잘 저장해 두도록 합니다.S3 버킷 생성사용자를 생성하였으니 S3 버킷을 생성하겠습니다. AWS 콘솔 -&gt; S3 -&gt; 버킷 만들기 로 이동하여 버킷을 생성합니다. 이때 모든 퍼블릭 액세스를 차단합니다.Github Repository 생성 및 코드 commit &amp; push간단한 Spring boot 어플리케이션을 만들어 Github에 올립니다.@RestController@SpringBootApplicationpublic class SimpleApplication { public static void main(String[] args) { SpringApplication.run(SimpleApplication.class, args); } @GetMapping(\"/ip\") public Map&lt;String, String&gt; ip(HttpServletRequest request) throws UnknownHostException { var map = new HashMap&lt;String, String&gt;(); map.put(\"hostIp\", InetAddress.getLocalHost().getHostAddress()); map.put(\"accessIp\", request.getHeader(\"x-forwarded-for\")); return map; }}Github Repository에 시크릿 키 등록Github Action에 시크릿 키를 그대로 노출시키면 보안상 큰 문제가 되겠죠? 따라서 Workflow에서 변수를 사용할 수 있도록 설정에 시크릿 키를 등록하도록 하겠습니다.위 캡쳐와 같이 repository -&gt; settings -&gt; 좌측 secrets -&gt; actions 로 이동하며 IAM 사용자 생성 마지막 단계에서 저장해 두었던 키값들을 등록합니다.S3에 업로드하기github workflow에서 s3로 빌드된 jar 파일을 옮기는 job을 추가합니다. .github/workflows/gradle.yml 파일을 만들고 아래 내용을 넣으면 됩니다.name: github action codedeploy CI/CDon: push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ]permissions: contents: readjobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up JDK 11 uses: actions/setup-java@v3 with: java-version: '11' distribution: 'temurin' - name: Gradlew permission run: chmod +x gradlew - name: Build with Gradle run: ./gradlew clean build - name: Make Directory run: mkdir dist - name: Copy jar run: cp ./build/libs/*.jar ./dist/ - name: zip run: tar -zcvf deploy.tar.gz ./dist - name: Upload to S3 env: AWS_ACCESS_KEY_ID: $ AWS_SECRET_ACCESS_KEY: $ run: | aws s3 cp \\ --region ap-northeast-2 \\ --acl private \\ ./deploy.tar.gz \\ s3://keencho-codedeploy-s3-bucket/$/deploy.tar.gz \\run_number를 사용하지 않으면 action이 수행될때마다 새로운 파일이 overwirte 되기 때문에 백업 파일을 남길수 없습니다. 따라서 run_number를 사용하여 백업 파일을 남길수 있도록 하였습니다.이제 repository에 push 하고 actions탭에 들어가 작업이 잘 수행되는지 확인합니다. 작업이 모두 성공하였다면 s3로 이동해 deploy.tar.gz 파일이 업로드외어 있는지 확인합니다.CodeDeploy 호출s3에 파일이 잘 업로드 되었으니 CodeDeploy를 호출해야 합니다. 스크립트를 수정하기 전 기존 AutoScaling Group에 포함된 인스턴스에 CodeDeploy Agent가 실행중인지 체크하세요.sudo service codedeploy-agent statusThe AWS CodeDeploy agent is running as PID 23029다음은 스크립트를 수정해 보겠습니다.name: github action codedeploy CI/CDon: push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ]permissions: contents: readjobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up JDK 11 uses: actions/setup-java@v3 with: java-version: '11' distribution: 'temurin' - name: Gradlew permission run: chmod +x gradlew - name: Build with Gradle run: ./gradlew clean build - name: Make Directory run: mkdir dist - name: Copy jar run: cp ./build/libs/*.jar ./dist/ - name: zip run: tar -zcvf deploy.tar.gz ./dist - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: $ aws-secret-access-key: $ aws-region: ap-northeast-2 - name: Upload to S3 run: | aws s3 cp \\ --region ap-northeast-2 \\ --acl private \\ ./deploy.tar.gz \\ s3://keencho-codedeploy-s3-bucket/$/deploy.tar.gz \\ - name: Call CodeDeploy run: | aws deploy create-deployment \\ --application-name keencho-codedeploy-app \\ --deployment-group-name keencho-codedeploy-app-group \\ --region ap-northeast-2 \\ --s3-location bucket=keencho-codedeploy-s3-bucket,bundleType=tgz,key=$/deploy.tar.gz \\aws 인증부분을 Configure AWS Credentials로 따로 뻈고, Call CodeDeploy 부분을 추가하였습니다. application-name과 deployment-group-name 이 aws콘솔상의 이름과 일치하는지 다시한번 확인해 보세요.이제 다시 github action을 실행시키고 CodeDeploy 콘솔로 이동하여 배포가 진행되는지 확인합니다. CodeDeploy agent was not able to receive the lifecycle event. Check the CodeDeploy agent logs on your host and make sure the agent is running and can connect to the CodeDeploy server. 라는 에러메시지가 뜨면서 CodeDeploy가 더이상 진행되지 않는다면 이는 CodeDeploy Agent가 설치되어 있지 않다는 뜻입니다.앞 포스팅에서 언급한 것처럼 쉘 스크립트의 첫번째 부분에서 -xe를 제거해 보시고, 그래도 안된다면 구글링을 통해 CodeDeploy Agent를 실행시키는 방법을 찾아보세요.The CodeDeploy agent did not find an AppSpec file within the unpacked revision directory at revision-relative path \"appspec.yml\" 라는 메시지가 뜨며 배포가 실패한다면 지금까지는 성공입니다.appspec.yml 세팅CodeDeploy는 AppSpec file을 배포 관리 yaml 파일로 사용합니다. 이전에 만든 템플릿에는 codedeploy-agent 설치 까지만 진행했었는데요, nginx나 기타 필요한 세팅은 이곳에서 진행됩니다.배포될 스프링 부트 프로젝트의 root에서 codedeploy 라는 폴더를 만들고 appspec.yml 파일을 생성하고 아래 내용을 붙여넣습니다.version: 0.0os: linuxfiles: - source: / destination: /home/ec2-userpermissions: - object: / pattern: \"**\" owner: ec2-user group: ec2-userhooks: AfterInstall: - location: init.sh timeout: 300 runas: root ApplicationStart: - location: deploy.sh timeout: 300 runas: ec2-user ValidateService: - location: validate.sh timeout: 200 runas: ec2-user위 파일을 보시면 hooks 섹션을 확인하실 수 있습니다. hooks 섹션은 이름에서도 유추 가능하듯이 인스턴스 배포시 각각 한번 실행됩니다. 각각의 훅이 실행되는 시점에 대한 자세한 내용은 이곳을 참고 하세요.많은 훅들이 있지만 여기서 사용할 훅은 세가지 입니다. 그중 먼저 AfterInstall 훅에 사용될 스크립트를 정의해 보겠습니다.AfterInstallappspec.yml 파일이 위치한 곳에 init.sh 파일을 생성합니다.#!/bin/bash# 타임존 세팅rm /etc/localtimeln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime# nginx 설치amazon-linux-extras install nginx1 -ysystemctl enable nginxsystemctl start nginxchmod 755 /var/log/nginx# nginx config file 위치 이동mv /home/ec2-user/nginx/*.conf /etc/nginx/conf.d/rm -rf /home/ec2-user/nginxsystemctl restart nginx# open jdk 11 설치amazon-linux-extras install java-openjdk11 -y# deploy.sh 권한 변경chmod +x /home/ec2-user/deploy.sh타임존 세팅, nginx 설치, nginx config 설정, open jdk 11 설치, deploy.sh 권한 변경 작업이 이루어집니다. 이것이 시작 템플릿의 사용자 데이터에서 nginx나 jdk등을 설치하지 않는 이유입니다.위 스크립트를 보시면 중간에 미리 정의해둔 nginx config 파일을 옮기는 부분이 있습니다. 프로젝트 root &gt; codedeploy &gt; nginx 폴더를 생성하고 nginx.conf 파일을 만들고 아래 내용을 작성하겠습니다.server { set $proxy_pass http://127.0.0.1:8090; listen 80; location /health { access_log off; return 200; } location / { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; proxy_read_timeout 120; proxy_buffer_size 128k; proxy_buffers 4 256k; proxy_busy_buffers_size 256k; proxy_pass $proxy_pass; }}이전 포스팅에서 로드밸랜서 / 대상그룹에 대해 알아본 적이 있습니다. 테스트 어플리케이션도 하나 만들었지요. 그때 /health 경로에 대해 200 상태값을 반환하도록 설정하였었는데요, 여기서도 동일합니다. 대상 그룹이 새로운 인스턴스의 health 상태를 체크하기 위해 사용됩니다.ApplicationStart인스턴스 세팅이 끝났으니 SpringBoot 어플리케이션을 실행하는 스크립트가 필요합니다. appspec.yml 파일이 위치한 곳에 deploy.yml 파일을 생성합니다.#!/bin/bashnohup java -jar -Dserver.port=8090 /home/ec2-user/application.jar 1&gt; /dev/null 2&gt;&amp;1 &amp;sleep 30s스프링부트의 포트와 nginx의 포트가 일치하는지 확인합니다.ValidateService혹시 스프링부트가 뜨지 않았다면 어떻게 할까요?(예를들어 하이버네이트의 스키마 검증…) 타겟그룹은 현재인스턴스/health 경로로 health-check 를 진행하였기 때문에 인스턴스 내부의 어플리케이션이 정상적으로 동작하는지 까지는 확인하지 않습니다. 따라서 여기서는 별도의 스크립트를 통해 어플리케이션이 잘 시작되었는지 확인하겠습니다.일단 스프링부트 어플리케이션에 메소드를 추가해 보겠습니다. 단순히 200 상태값을 체크하는데 사용되는 메소드 입니다.@GetMapping(\"/script-health-check\") public ResponseEntity&lt;?&gt; scriptHealthCheck() { return ResponseEntity.ok().build(); }appspec.yml 파일이 위치한 곳에 validate.sh 파일을 생성합니다.#!/bin/bashvalidate () {\tcount=1\twhile [ ${count} -le 11 ]; do\t\tresponse=$(curl --write-out '%{http_code}' --silent --output /dev/null $1)\t\tif [ ${response} -eq 200 ]; then\t\t\tbreak\t\telse\t\t\tif [ ${count} -eq 11 ]; then\t\t\t\techo \"$2 어플리케이션 검증 실패\"\t\t\t\texit 1\t\t\tfi\t\t\techo \"$2 어플리케이션 검증 재시도... ${count}\"\t\t\tsleep 10s\t\t\tcount=$((count + 1))\t\tfi\tdone}validate \"localhost:8090/script-health-check\" \"샘플\"총 열번, 각 시도마다 10초의 유휴시간을 가지는 검증 스크립트 입니다. 검증에 실패한다면 exit1 코드를 통해 CodeDeploy 배포가 종료되게 됩니다.gradle.yml 최종수정필요한 스크립트들이 스프링부터 jar 파일과 같이 압축될수 있도록 yml 파일을 최종 수정합니다.name: github action codedeploy CI/CDon: push: branches: [ \"master\" ] pull_request: branches: [ \"master\" ]permissions: contents: readjobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up JDK 11 uses: actions/setup-java@v3 with: java-version: '11' distribution: 'temurin' - name: Gradlew permission run: chmod +x gradlew - name: Build with Gradle run: ./gradlew clean build - name: Make Directory run: mkdir dist - name: Move Jar run: mv ./build/libs/*.jar ./dist/application.jar - name: Move CodeDeploy Script run: mv ./codedeploy/* ./dist/ - name: zip run: tar -zcvf deploy.tar.gz ./dist - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: $ aws-secret-access-key: $ aws-region: ap-northeast-2 - name: Upload to S3 run: | aws s3 cp \\ --region ap-northeast-2 \\ --acl private \\ ./deploy.tar.gz \\ s3://keencho-codedeploy-s3-bucket/$/deploy.tar.gz \\ - name: Call CodeDeploy run: | aws deploy create-deployment \\ --application-name keencho-codedeploy-app \\ --deployment-group-name keencho-codedeploy-app-group \\ --region ap-northeast-2 \\ --s3-location bucket=keencho-codedeploy-s3-bucket,bundleType=tgz,key=$/deploy.tar.gz \\ Move CodeDeploy Script 부분이 추가되었습니다.결과 확인이제 필요한 스크립트는 모두 작성되었습니다. Github에 push하여 결과를 확인합니다.CodeDeploy 콘솔에서 작업이 성공적으로 끝난것을 확인하였다면 로드밸런서의 DNS 를 통해 사이트로 접속하여 결과물을 확인합니다. 결과를 확인하였다면 이제는 스프링부트 어플리케이션에 테스트 메소드를 추가하여 다시한번 CodeDeploy 배포를 수행해 보세요.CodeDeploy 를 확인하면서 수시로 사이트에 접속해 보세요. 이전 어플리케이션과 최신 어플리케이션이 번갈아가며 호출되는것을 확인할 수 있습니다. 로드밸런서가 어떤 식으로 동작하는지 파악 / 확인하는것은 중요합니다. (포스팅에서 사용된 형태의 스프링 부트 어플리케이션을 사용하셨다면 ec2 인스턴스의 private ip로 확인할 수 있습니다.) 이 시점에서 만약 CodeDeploy Task가 시작조차 못하고 첫번째 단계인 Application Stop 에서 멈춰버린다면, 새로운 인스턴스에 CodeDeploy Agent가 설치되지 않았음을 의미합니다./var/log/cloud-init-output.log 에서 확인하실 수 있는데요, 제 경우는 실수로 NAT 게이트웨이를 삭제해 외부 인터넷 연결이 되지 않아 yum 패키지를 다운받을 수 없어 에러가 발생 하였습니다.다양한 원인이 있을 수 있으므로 문제 발생시 위 로그를 확인해 보세요.모든 TASK가 성공하였습니다!마무리이 시리즈에서는 VPC생성, 서브넷 생성부터 Github Action, CodeDeploy까지 진행해 보았습니다. 사실 실제 프로덕션 환경에서는 실경써줘야할 부분이 더 많습니다.예를들어 CodeDeploy 배포에 실패한 경우 CodeDeploy 에 의해 새롭게 시작된 Auto Scaling Group은 자동종료되지 않기 때문에 새로운 인스턴스가 실행된 상태(아무것도 하지 않는 상태)로 멈추어 이중 과금이 발생합니다. 따라서 이에 대응할 전략이 필요합니다. 제 경우 CodeDeploy의 성공 / 실패 여부를 30초마다 Polling 하는 스크립트를 Jenkins에 포함시켜서 실패한 경우 미리 정의해둔 롤백 람다를 호출하는 방식을 사용하고 있습니다.세세한 부분까지 신경쓰지 못했지만 이 포스팅이 AWS를 이용해 무중단 배포를 구현하시려는 분들께 도움이 되길 바랍니다. 이 포스팅에서 사용된 전체 코드는 이곳 에서 확인하실 수 있습니다." }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 5. AWS 리소스 세팅", "url": "/posts/aws-cicd-5/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2022-06-08 20:12:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리AWS 리소스 세팅Auto Scaling 그룹 생성AWS 리소스 세팅 첫번째 단계로 Auto Scaling 그룹을 생성해 보도록 하겠습니다.템플릿 만들기그룹을 생성하기 전에 템플릿을 먼저 만들어야 합니다. 기존의 인스턴스를 종료하고 새롭게 인스턴스를 띄우는 방식을 사용할 것이기 때문에 새롭게 인스턴스가 시작될때 사용될 템플릿입니다.EC2 콘솔 -&gt; 인스턴스 -&gt; 시작템플릿 -&gt; 시작 템플릿 생성으로 이동하여 생성 화면으로 이동합니다. 이름은 임의로 지정합니다.위와 같이 세팅후에 시작 템플릿을 생성합니다. 가장 중요한것은 마지막 고급 세부정보 부분의 IAM 인스턴스 프로파일과 사용자 데이터 입니다.IAM 인스턴스 프로파일은 AmazonS3FullAccess, AWSCodeDeployRole 와 AWSCodeDeployFullAccess 정책을 필요로 합니다. 사용자 데이터는 인스턴스가 시작된 후에 codedeploy-agent 를 실행해야 하기 때문에 codedeploy-agent 실행에 필요한 파이썬, 루비등을 스크립트로 설치하고 codedeploy-agent를 실행합니다.#!/bin/bash -xesudo yum install -y python3sudo python3 -m pip install --upgrade pipsudo python3 -m pip install awsclisudo yum -y install rubysudo yum -y install wgetsudo wget https://aws-codedeploy-ap-northeast-2.s3.ap-northeast-2.amazonaws.com/latest/installsudo chmod +x ./installsudo ./install autosudo service codedeploy-agent start 위 스크립트에 문제가 발생하시면 #!/bin/bash -xe 에서 -xe를 빼고 진행해 보세요.Auto Scaling 그룹 생성다음은 오토 스케일링 그룹을 생성해야 합니다. EC2 콘솔 -&gt; Auto Scaling -&gt; Auto Scaling 그룹 -&gt; 생성 으로 이동하여 그룹을 생성하겠습니다.방금 만든 템플릿을 시작 템플릿으로 지정합니다.다음 단계에서는 VPC를 지정하고 가용 영역을 세팅합니다. 물론 가용 영역을 추가할 수 있습니다.3단계 에서는 로드밸런서를 지정합니다. 만들어둔 로드밸런서와 타겟 그룹이 있기 때문에 그것을 사용하겠습니다. 상태 확인의 경우 기본이 300초로 되어있는데요, 테스트에서는 너무 길기 때문에 30초로 줄여주도록 하겠습니다.4단계는 그룹 크기 입니다. 원하는 용량, 최소 용량, 최대 용량을 지정할수 있습니다. 테스트기 때문에 모두 한개로 설정하겠습니다. 아래 설정 박스에서는 스케일 in - out할 정책을 지정할 수 있습니다.5단계, 6단계는 각각 알림, 태그로 필요하신 분들은 설정하시면 됩니다.결과 확인하기오토 스케일링 그룹이 잘 생성되었다면 인스턴스가 하나 생성되었을 것입니다. 인스턴스 콘솔로 이동하여 인스턴스가 생성되었는지 확인해 보도록 합시다.로드밸런서의 대상 그룹에도 생성된 인스턴스가 타겟으로 등록되어있는지 확인해 봅니다.Health status가 unhealthy로 되어있습니다. 현재 단계에서는 정상적인 흐름입니다. 앞 포스팅에서 설명했다시피 로드밸런서가 대상 그룹의 health status를 검사하기 위해선 미리 지정해둔 경로의 response status code가 200이 떨어져야 합니다.하지만 우리가 방금 만든 템플릿은 nginx나 다른 웹서버등 로드밸런서의 80포트 요청에 응답할만한 무언가를 설치하지 않았기 때문입니다. 일단 정상이니 다음 단계로 넘어가도록 하겠습니다.CodeDeploy무중단 배포는 AWS CodeDeploy를 사용하여 진행합니다. CodeDeploy에 대한 자세한 설명은 이곳 에서 확인하실 수 있습니다.CodeDeploy 애플리케이션 생성다음은 CodeDeploy 애플리케이션을 생성해야 합니다. AWS 콘솔에서 CodeDeploy를 검색하여 진입한 후에 좌측 메뉴의 배포 -&gt; 애플리케이션으로 이동후 우측 상단의 애플리케이션 생성 버튼을 눌러 애플리케이션을 생성합니다.컴퓨팅 플랫폼의 경우 EC2를 사용합니다. 람다나 ECS 컨테이너도 사용하실 수 있습니다.IAM 역할 생성다음은 배포 그룹을 생성해야 합니다. 배포 그룹을 생성하기 전에 선행되어야할 작업이 몇가지 있는데요, 먼저 IAM 역할을 생성하도록 하겠습니다.AWS 콘솔에서 IAM을 검색해 IAM 관리 콘솔로 이동합니다. 그 후 CodeDeploy 서비스를 사용할 수 있도록 권한을 부여하고 역할을 생성합니다.AutoScalingFullAccess 와 AWSCodeDeployRole 는 미리 정해진 정책을 연결하면 됩니다. 마지막 AmazonAutoScalingPolicy의 경우 개발자가 생성해야 하는 인라인 정책이며, 아래 json을 정책에 붙여넣고 생성하면 됩니다.{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\", \"ec2:CreateTags\", \"ec2:RunInstances\" ], \"Resource\": \"*\" } ]}배포 그룹 생성생성한 애플리케이션의 상세화면으로 이동한 후 배포 그룹 생성 페이지로 이동합니다.위에서 생성한 역할을 서비스 역할로 지정하고 배포 유형은 블루/그린을 선택합니다.위에서 생성한 Auto Scaling 그룹을 선택합니다.배포 설정의 경우 트래픽은 즉시 다시 라우팅하고 원본 인스턴스의 즉시종료 여부는 각각의 정책에 맞게 설정하시면 될것 같습니다.로드밸런서 역시 기존의 로드밸런서를 지정합니다.무중단 배포에 필요한 1차 AWS 세팅이 완료되었습니다. 추가로 필요한 세팅이나 결과 확인은 이후 Github - CodeDeploy 연동시 진행하겠습니다." }, { "title": "Hibernate @Where", "url": "/posts/hibernate-where/", "categories": "Hibernate", "tags": "Hibernate, JPA", "date": "2022-05-26 08:12:00 +0900", "snippet": "Hibernate @Where@Where 란?떄때로 커스텀한 SQL을 이용해 엔티티 혹은 컬렉션을 필터링하고 싶은 경우가 있습니다. 이 경우 Hibernate 에서 제공하는 어노테이션중 @Where 이라는 어노테이션을 사용해 쉽게 필터링 할 수 있습니다.@Target({TYPE, METHOD, FIELD})@Retention(RUNTIME)public @interface Where {\tString clause();}어노테이션 자체도 간단합니다. where 절을 clause 멤버변수에 작성하라고 안내하고 있습니다.Entity에 적용하기간단한 예제 엔티티를 작성해 보겠습니다. 부모 엔티티 (MainOrder)와 자식 엔티티(SubOrder)가 존재하며, 양방향으로 매핑을 해주도록 하겠습니다.@Entity@Data@AllArgsConstructor@NoArgsConstructor@ToString(exclude = {\"activeSubOrderList\", \"deActivatedSubOrderList\", \"fromNameLikeCList\"})@Table(name = \"main_order\")public class MainOrder { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private LocalDate orderDate = LocalDate.now(); @OneToMany(mappedBy = \"mainOrder\", cascade = CascadeType.ALL, orphanRemoval = true, fetch = FetchType.LAZY) @Where(clause = \"active = true\") private List&lt;SubOrder&gt; activeSubOrderList; @OneToMany(mappedBy = \"mainOrder\", cascade = CascadeType.ALL, orphanRemoval = true, fetch = FetchType.LAZY) @Where(clause = \"active = false\") private List&lt;SubOrder&gt; deActivatedSubOrderList; @OneToMany(mappedBy = \"mainOrder\", cascade = CascadeType.ALL, orphanRemoval = true, fetch = FetchType.LAZY) @Where(clause = \"LOWER(from_name) LIKE 'c%'\") private List&lt;SubOrder&gt; fromNameLikeCList;}@Entity@Data@AllArgsConstructor@NoArgsConstructor@Table(name = \"sub_order\")public class SubOrder { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String fromName; private String fromPhoneNumber; private String fromAddress; private String toName; private String toPhoneNumber; private String toAddress; @ManyToOne(optional = false, fetch = FetchType.LAZY) private MainOrder mainOrder; private boolean active;}@Where 어노테이션을 통해 각각의 조건에 부합하는 자식 엔티티 리스트를 가져올수 있도록 하였습니다. @Where 어노테이션의 sql 문의 필드명은 데이터베이스의 필드명이어야 합니다. 예를들어 위 MainOrder 엔티티의 fromNameLikeCList 의 경우 “LOWER(fromName) LIKE ‘c%’” 라고 작성하면 에러가 발생합니다.조회 결과 확인하기엔티티 객체를 작성하였으니 이제 더미데이터를 넣고 테스트를 수행해보겠습니다. 더미데이터는 java-faker 라이브러리를 사용하였습니다.public void initData() { var mainOrderList = new ArrayList&lt;MainOrder&gt;(); for (int i = 0; i &lt; 5; i ++) { var mainOrder = new MainOrder(); mainOrderList.add(mainOrderRepository.save(mainOrder)); } for (int i = 0; i &lt; 100; i ++) { var subOrder = new SubOrder(); subOrder.setActive(i % 3 == 0); subOrder.setFromName(JavaFakerGenerator.getName()); subOrder.setFromPhoneNumber(JavaFakerGenerator.getPhoneNumber()); subOrder.setFromAddress(JavaFakerGenerator.getAddress()); subOrder.setToName(JavaFakerGenerator.getName()); subOrder.setToPhoneNumber(JavaFakerGenerator.getPhoneNumber()); subOrder.setToAddress(JavaFakerGenerator.getAddress()); subOrder.setMainOrder(mainOrderList.get(i % 5)); subOrderRepository.save(subOrder); }}활성:비활성의 비율을 약 1:2 정도의 비율로 세팅하여 테스트 데이터를 생성하였습니다.@Test@Transactionalvoid whereAnnotationTest1() { var mainOrderList = mainOrderRepository.findAll(); mainOrderList.forEach(mainOrder -&gt; { mainOrder.getActiveSubOrderList(); mainOrder.getDeActivatedSubOrderList(); mainOrder.getFromNameLikeCList(); });}테스트 코드입니다. 테스트를 수행한다기 보다 쿼리를 확인할 목적으로 작성하였습니다.select activesubo0_.main_order_id as main_ord9_2_0_, activesubo0_.id as id1_2_0_, activesubo0_.id as id1_2_1_, activesubo0_.active as active2_2_1_, activesubo0_.from_address as from_add3_2_1_, activesubo0_.from_name as from_nam4_2_1_, activesubo0_.from_phone_number as from_pho5_2_1_, activesubo0_.main_order_id as main_ord9_2_1_, activesubo0_.to_address as to_addre6_2_1_, activesubo0_.to_name as to_name7_2_1_, activesubo0_.to_phone_number as to_phone8_2_1_ from sub_order activesubo0_ where ( activesubo0_.active = true ) and activesubo0_.main_order_id=1select deactivate0_.main_order_id as main_ord9_2_0_, deactivate0_.id as id1_2_0_, deactivate0_.id as id1_2_1_, deactivate0_.active as active2_2_1_, deactivate0_.from_address as from_add3_2_1_, deactivate0_.from_name as from_nam4_2_1_, deactivate0_.from_phone_number as from_pho5_2_1_, deactivate0_.main_order_id as main_ord9_2_1_, deactivate0_.to_address as to_addre6_2_1_, deactivate0_.to_name as to_name7_2_1_, deactivate0_.to_phone_number as to_phone8_2_1_ from sub_order deactivate0_ where ( deactivate0_.active = false ) and deactivate0_.main_order_id=1select fromnameli0_.main_order_id as main_ord9_2_0_, fromnameli0_.id as id1_2_0_, fromnameli0_.id as id1_2_1_, fromnameli0_.active as active2_2_1_, fromnameli0_.from_address as from_add3_2_1_, fromnameli0_.from_name as from_nam4_2_1_, fromnameli0_.from_phone_number as from_pho5_2_1_, fromnameli0_.main_order_id as main_ord9_2_1_, fromnameli0_.to_address as to_addre6_2_1_, fromnameli0_.to_name as to_name7_2_1_, fromnameli0_.to_phone_number as to_phone8_2_1_ from sub_order fromnameli0_ where ( LOWER(fromnameli0_.from_name) LIKE 'c%' ) and fromnameli0_.main_order_id=1세 쿼리를 보니 모두 @Where 어노테이션에 작성한 sql문 대로 잘 수행된것을 확인할 수 있습니다.QueryDSL 에서 사용하기QueryDSL 에서도 쉽게 사용할 수 있습니다. 예를들어 fromName이 c로 시작하는 SubOrder가 존재하는 경우에만 MainOrder를 가져와야 한다고 가정해보겠습니다.평소라면 where절에 직접 서브쿼리를 작성해야 하겠지만, @Where 어노테이션을 사용하면 쉽게 표현할 수 있습니다. 다음은 예제 / 테스트 코드입니다.@Test@Transactionalvoid whereAnnotationTest2() { var mq = QMainOrder.mainOrder; var sq = QSubOrder.subOrder; var subQueryFetch = jpaQueryFactory .select(mq) .from(mq) .where(JPAExpressions.select(sq.mainOrder.count()).from(sq).where(mq.eq(sq.mainOrder).and(sq.fromName.startsWithIgnoreCase(\"c\"))).gt(0L)) .fetch(); var whereAnnotationFetch = jpaQueryFactory .select(mq) .from(mq) .where(mq.fromNameLikeCList.size().gt(0L)) .fetch(); Assert.isTrue(subQueryFetch.size() == whereAnnotationFetch.size(), \"test failed\");}첫번째 subQueryFetch의 경우 where절에 직접 서브쿼리를 작성하였고, 두번째 whereAnnotationFetch의 경우 단순히 @OneToMany로 매핑된 엔티티의 사이즈를 불러와 비교하는 방식을 사용하였습니다.select mainorder0_.id as id1_0_, mainorder0_.order_date as order_da2_0_ from main_order mainorder0_ where ( select count(suborder1_.main_order_id) from sub_order suborder1_ cross join main_order mainorder2_ where suborder1_.main_order_id=mainorder2_.id and mainorder0_.id=suborder1_.main_order_id and ( lower(suborder1_.from_name) like 'c%' escape '!' ) )&gt;0select mainorder0_.id as id1_0_, mainorder0_.order_date as order_da2_0_ from main_order mainorder0_ where ( select count(fromnameli1_.main_order_id) from sub_order fromnameli1_ where mainorder0_.id = fromnameli1_.main_order_id and ( LOWER(fromnameli1_.from_name) LIKE 'c%' ) )&gt;0테스트도 통과하고 결과도 동일한것을 확인할 수 있습니다. 이처럼 QueryDSL에도 쉽게 사용가능한 것을 확인할 수 있습니다." }, { "title": "리액트로 멀티탭 구현하기 (feat. Recoil)", "url": "/posts/react-multitab/", "categories": "React", "tags": "React, Recoil", "date": "2022-04-17 08:12:00 +0900", "snippet": "리액트로 멀티탭 구현하기탭을 클릭했을때 해당 탭의 컴포넌트를 보여주는 방식의 코드나 라이브러리는 인터넷에 검색해보면 많습니다. 당장 npm에만 봐도 조금 오래되긴 했지만 이런 라이브러리도 존재합니다.하지만 탭을 클릭했을때 해당 탭을 새로운 컴포넌트로써 오픈하고 오픈된 컴포넌트들의 리스트를 관리하는 형태의 예제코드는 별로 없는것 같습니다. 그래서 오늘은 이러한 방식으로 멀티탭을 구현하는 방법에 대해 알아보겠습니다.사용 라이브러리아래는 이 예제에서 사용될 라이브러리 입니다. bootstrap, react-dnd, recoil, sass 를 사용하였습니다.package.json{ \"name\": \"react-multi-tab\", \"private\": true, \"version\": \"0.0.0\", \"scripts\": { \"dev\": \"vite\", \"build\": \"tsc &amp;&amp; vite build\", \"preview\": \"vite preview\" }, \"dependencies\": { \"bootstrap\": \"^5.1.3\", \"path\": \"^0.12.7\", \"react\": \"^18.0.0\", \"react-bootstrap\": \"^2.3.0\", \"react-dnd\": \"^16.0.1\", \"react-dnd-html5-backend\": \"^16.0.1\", \"react-dom\": \"^18.0.0\", \"recoil\": \"^0.7.2\", \"sass\": \"^1.50.1\" }, \"devDependencies\": { \"@types/react\": \"^18.0.0\", \"@vitejs/plugin-react\": \"^1.3.0\", \"typescript\": \"^4.6.3\", \"vite\": \"^2.9.5\" }}recoil 코드 준비이 예제는 recoil을 사용합니다. 현재 시점에는 모르는 분이 거의 없다고 생각됩니다만, 혹시 처음들어보았거나 잘 모르시다면 이곳 을 읽으며 학습해보세요. 개인적으로 공식 문서를 읽는 것만으로도 쉽게 이해 가능하다고 생각합니다.먼저 RecoilNexus를 준비합니다. recoil은 일종의 훅이기 때문에 함수형 컴포넌트 안에서만 사용해야 합니다. 하지만 저는 유틸성 클래스에서 recoil을 사용하고 싶기 때문에 RecoilNexus를 만들어 일반 클래스에서도 사용 가능하게 하겠습니다.import {RecoilState, RecoilValue, useRecoilCallback} from 'recoil';interface Nexus { get?: &lt;T&gt;(atom: RecoilValue&lt;T&gt;) =&gt; T getPromise?: &lt;T&gt;(atom: RecoilValue&lt;T&gt;) =&gt; Promise&lt;T&gt; set?: &lt;T&gt;(atom: RecoilState&lt;T&gt;, valOrUpdater: T | ((currVal: T) =&gt; T)) =&gt; void reset?: (atom: RecoilState&lt;any&gt;) =&gt; void}const nexus: Nexus = {}export default function RecoilNexus() { nexus.get = useRecoilCallback&lt;[atom: RecoilValue&lt;any&gt;], any&gt;(({ snapshot }) =&gt; function &lt;T&gt;(atom: RecoilValue&lt;T&gt;) { return snapshot.getLoadable(atom).contents }, []) nexus.getPromise = useRecoilCallback&lt;[atom: RecoilValue&lt;any&gt;], Promise&lt;any&gt;&gt;(({ snapshot }) =&gt; function &lt;T&gt;(atom: RecoilValue&lt;T&gt;) { return snapshot.getPromise(atom) }, []) // @ts-ignore nexus.set = useRecoilCallback(({ set }) =&gt; set, []) nexus.reset = useRecoilCallback(({ reset }) =&gt; reset, []) return null}export function getRecoil&lt;T&gt;(atom: RecoilValue&lt;T&gt;): T { return nexus.get!(atom)}export function getRecoilPromise&lt;T&gt;(atom: RecoilValue&lt;T&gt;): Promise&lt;T&gt; { return nexus.getPromise!(atom)}export function setRecoil&lt;T&gt;(atom: RecoilState&lt;T&gt;, valOrUpdater: T | ((currVal: T) =&gt; T)) { nexus.set!(atom, valOrUpdater)}export function resetRecoil(atom: RecoilState&lt;any&gt;) { nexus.reset!(atom)}이렇게 만든 RecoilNexus를 사용하려면 컴포넌트 렌더링하는 Main 코드에 전체 컴포넌트를 RecoilRoot로 감싸주고 내부에 RecoilNexus를 선언해야 합니다.ReactDOM.createRoot(document.getElementById('root')!).render( &lt;RecoilRoot&gt; &lt;RecoilNexus /&gt; &lt;/RecoilRoot&gt;)React18 부터는 ReactDOM.render 대신 ReactDOM.createRoot 를 써야 에러가 나지 않는다는 점을 잊지 말아주세요.다음은 탭 컴포넌트에 쓰일 recoil atom의 데이터 타입입니다.export default interface RouterComponentModel { displayName: string, uniqueKey: string, sequence: number, active: boolean, component: JSX.Element}브라우저에 보여질 이름, 시퀀스, 유니크키, 활성화 여부, 렌더링할 컴포넌트의 정보를 가지고 있습니다.다음은 recoil atom 입니다.import {atom} from 'recoil';import RouterComponentModel from '@/model/router-component.model';const RouterComponentAtom = atom&lt;RouterComponentModel[]&gt;({ key: 'routerComponent', default: []});export default RouterComponentAtom;RouterControlUtil 클래스다음으로 작성할 코드는 RouterControlUtil 클래스 입니다. 앞에서 말했다시피 저는 RouterComponentAtom을 컨트롤하는 코드를 모두 한곳에서 모아 처리할 곳이기 때문에 이 유틸 클래스가 제일 중요하다고 할 수 있습니다.1. 컴포넌트 리스트 추가 / 업데이트위 이미지처럼 탭을 클릭했을시 컴포넌트를 추가하고, 만약 중복탭을 허용하지 않는 경우 이미 열려있는 탭을 active로 만드는 코드를 작성해 보겠습니다.export default class RouterControlUtil { static MAX_TAB_SIZE = 15; static MAX_TAB_MESSAGE = `최대 탭 갯수에 도달하였습니다. (${this.MAX_TAB_SIZE} 개)`; static atom = RouterComponentAtom; static saveOrUpdateComponent = (component: JSX.Element, allowDuplicateTab?: boolean) =&gt; { const value: RouterComponentModel[] = getRecoil(this.atom) if (value.length &gt;= this.MAX_TAB_SIZE) { alert(this.MAX_TAB_MESSAGE); return; } //////////////////// 중복탭을 허용하지 않는 경우 //////////////////// if (allowDuplicateTab !== true) { // 만약 이미 recoil에 저장된 컴포넌트가 있다면 해당 컴포넌트의 show를 true로 세팅하고 끝낸다. if (value.some(v =&gt; v.displayName === component.type.name)) { this.setRecoil([ ...value.map(v =&gt; { return { ...v, active: v.displayName === component.type.name} as RouterComponentModel; })]); return; } // 없다면 새로 추가한다. this.setRecoil([ ...value.map(v =&gt; { return { ...v, active: false } as RouterComponentModel }), this.generateComponentModel(component)]); return; } //////////////////// 중복탭을 허용하는 경우 //////////////////// const componentModel: RouterComponentModel = this.generateComponentModel(component); // 중복탭을 허용하는경우 마지막 시퀀스를 찾아서 새롭게 만들 탭의 시퀀스를 +1 시켜줘야 한다. const filteredList = value.filter(v =&gt; v.displayName === component.type.name); if (filteredList.length === 0) { this.setRecoil([ ...value.map(v =&gt; { return { ...v, active: false } as RouterComponentModel }), componentModel ]); return; } const lastComponent = filteredList.sort((pv, nx) =&gt; pv.sequence - nx.sequence)[filteredList.length - 1]; componentModel.sequence = lastComponent.sequence + 1; this.setRecoil([ ...value.map(v =&gt; { return { ...v, active: false } as RouterComponentModel }), componentModel ]); } private static generateComponentModel = (component: JSX.Element): RouterComponentModel =&gt; { return { displayName: component.type.name, uniqueKey: crypto.randomUUID(), component: component, active: true, sequence: 1 } as RouterComponentModel }} 먼저 현재 탭의 갯수를 파악하여 만약 최대 탭 갯수에 도달하였다면 메시지를 alert하고 종료 합니다. 중복탭을 허용하지 않는 경우 이미 열린 탭이 있다면 해당 탭을 활성 상태로 만들고 종료 합니다. 열린 탭이 없다면 새로운 탭을 추가합니다. 이떄 uniqueKey는 crpyto.randomUUID() 를 이용하였습니다. 중복탭을 허용하는 경우 이미 열린 탭이 있다면 이미 열린 탭들중 마지막 탭을 찾아 새롭게 만들 탭의 시퀀스를 마지막탭의 시퀀스 + 1 처리해야 합니다. 열릴탭을 제외한 나머지 탭들의 active는 false로 세팅합니다.2. 컴포넌트 열기다음은 열린 탭중 하나를 클릭해 클릭한 탭을 활성화하는 코드입니다.static openComponent = (component: RouterComponentModel) =&gt; { const value: RouterComponentModel[] = getRecoil(this.atom) this.setRecoil([ ...value.map((v) =&gt; { return { ...v, active: this.isUniqueKeyEqual(v, component) } as RouterComponentModel })]);}클릭한 탭의 컴포넌트를 인자로 받아 loop를 돌며 uniqueKey가 동일하면 active를 true로 세팅하고, 그렇지 않다면 false로 세팅합니다.3. 컴포넌트 닫기다음은 열려 있는 탭중 하나를 닫는 코드입니다.static closeComponent = (component: RouterComponentModel) =&gt; { let value: RouterComponentModel[] = getRecoil(this.atom); const activeComponent: RouterComponentModel | undefined = value.find(v =&gt; v.active &amp;&amp; this.isUniqueKeyEqual(component, v)); const activeComponentIdx: number = value.findIndex(v =&gt; this.isUniqueKeyEqual(component, v)); // 닫으려는 컴포넌트가 활성화된 컴포넌트라면 맨 앞에있는 컴포넌트를 활성 상태로 만든다 (0이라면 1) if (value.length &gt; 1) { if (activeComponent !== undefined) { if (this.isUniqueKeyEqual(activeComponent, component)) { value = value.map((v, idx) =&gt; { return { ...v, active: (activeComponentIdx === 0 ? (idx === 1) : idx === 0) } as RouterComponentModel }) } } } let filteredValue = value.filter(v =&gt; !this.isUniqueKeyEqual(v, component)); // 시퀀스 재정렬 if (filteredValue.length &gt; 0) { filteredValue = filteredValue.map(v =&gt; { // 지워진 컴포넌트의 시퀀스보다 큰 시퀀스를 가진 컴포넌트 if (v.displayName === component.displayName &amp;&amp; v.sequence &gt; component.sequence) { return { ...v, sequence: v.sequence - 1 } as RouterComponentModel; } return v ; }); } this.setRecoil(filteredValue);}컴포넌트를 닫는 코드에는 주의해야할점이 있습니다. 닫으려는 컴포넌트가 활성화된 컴포넌트라면 활성화될 컴포넌트를 지정해줘야 하지요. 따라서 컴포넌트를 삭제하기 전에 먼저 활성화될 컴포넌트를 지정하는 코드를 작성하였습니다.컴포넌트를 삭제한 후에는 시퀀스를 재정렬하는 코드가 필요합니다. 만약 지워진 컴포넌트의 시퀀스가 2라면 2보다 큰 시퀀스를 가진 같은 이름의 컴포넌트들은 그 시퀀스에서 -1을 해줘야 하지요.RouterControlUtil 클래스 마무리RouterControlUtil 클래스 작성이 완료되었습니다. 남은것은 뷰를 이쁘게 그리고 알맞은 시점에 알맞은 함수를 호출하는것 뿐입니다.Drag and Drop 구현이대로 끝내긴 뭔가좀 아쉽습니다. Drag and drop으로 컴포넌트들의 순서를 변경하는 기능을 구현해보겠습니다. 아래와 같이 동작하는 코드를 작성하겠습니다.1. RouterControlUtil 클래스 수정일단 RouterControlUtil 클래스에 순서를 재지정하는 함수를 추가하겠습니다. 이 함수는 대상 컴포넌트와 대상 컴포넌트가 이동될 위치 인덱스를 인자로 받습니다.static controlOrder = (currentComponent: RouterComponentModel, targetIndex: number) =&gt; { const componentModelValue: RouterComponentModel[] = getRecoil(this.atom) const draggedComponentIndex: number = componentModelValue.findIndex(value =&gt; value.uniqueKey === currentComponent.uniqueKey); if (targetIndex === draggedComponentIndex) return; const existingComponent = componentModelValue[targetIndex]; const newComponentModelValue = componentModelValue.map((item, idx) =&gt; { if (idx === targetIndex) return { ...currentComponent, active: true } as RouterComponentModel; if (idx === draggedComponentIndex) return { ...existingComponent, active: false } as RouterComponentModel; return { ...item, active: false } as RouterComponentModel; }); this.setRecoil(newComponentModelValue);} 대상 컴포넌트의 인덱스를 검색합니다. 만약 이동될 위치와 대상 컴포넌트의 위치가 동일하다면 함수를 종료합니다. 이동될 위치에 이미 존재하는 컴포넌트를 existingComponent 에 할당합니다. 이제 루프를 돌며 순서를 재지정 합니다. 이때 기존의 활성화 여부와 관계없이 대상 컴포넌트들 활성 상태로 만듭니다.2. Drag and Drop 코드 작성Drag and drop 코드를 작성해 보겠습니다. 위 패키지 설명에도 나와있듯이 라이브러리는 react-dnd를 사용하였습니다.interface Props { component: RouterComponentModel, index: number,}const Tab = (props: Props): JSX.Element =&gt; { const [{isDragging}, dragRef] = useDrag(() =&gt; ({ type: 'item', item: props.component, collect: monitor =&gt; ({ isDragging: monitor.isDragging(), }), })) const [spec, dropRef] = useDrop({ accept: 'item', hover: (item, monitor) =&gt; { RouterControlUtil.controlOrder(item as RouterComponentModel, props.index); } }) const ref = useRef(null); const dragDropRef = dragRef(dropRef(ref)); const opacity = isDragging ? 0.3 : 1; return ( &lt;div // @ts-ignore ref={dragDropRef} className={`${style.item} ${RouterControlUtil.isActiveComponent(props.component) ? style.active : ''}`} style={{ ...style, opacity }} &gt; &lt;button className={style.btn} onClick={() =&gt; RouterControlUtil.openComponent(props.component)}&gt; {RouterControlUtil.getTabName(props.component)} &lt;/button&gt; &lt;button className={style.btnClose} onClick={() =&gt; RouterControlUtil.closeComponent(props.component)} /&gt; &lt;/div&gt; )}export default Tab간단한 코드입니다. 핵심은 [spec, dropRef] 부분인데요, hover 이벤트가 일어나면 위에서 작성한 controlOrder() 메소드를 호출하여 순서가 변경될 수 있도록 하였습니다.마무리리액트 라우터를 사용하지 않고 멀티탭을 구현하는 방법을 소개해 보았습니다. 단, 위의 코드로는 실제 url이 변경되지 않기 때문에 만약 url이 변경되게 하고 싶다면 추가 코드를 작성해야할 수도 있습니다.이곳 에서 전체 코드를 확인하실 수 있습니다." }, { "title": "Spring Transaction Propagation", "url": "/posts/spring-transactional-propagation/", "categories": "Spring", "tags": "Spring, Transaction", "date": "2022-03-31 08:12:00 +0900", "snippet": "트랜잭션 전파레벨Spring의 @Transactional 어노테이션에는 많은 옵션이 존재합니다. 저번에 알아본 격리 레벨 도 그중 하나이지요. 이번에 알아볼 옵션은 Propagation(전파레벨) 옵션입니다.트랜잭션에서의 전파는 비즈니스 로직의 트랜잭션 경계를 정의합니다. 스프링은 개발자가 설정한 레벨에 따라 트랜잭션을 시작하고 중지합니다. 스프링은 TransactionManager:getTransaction()을 호출하여 전파레벨에 따라 트랜잭션을 가져오거나 만듭니다. 이는 Transaction Manager의 모든 유형의 전파레벨중 일부를 지원하지만, 특정한 구현체에서만 지원되는 전파레벨도 일부 있습니다.1. REQUIREDREQUIRED는 기본 전파레벨 입니다. 스프링은 활성화된 트랜잭션이 있는지 확인하고 만약 없다면 새로운 트랜잭션을 생성합니다. 존재하는 트랜잭션이 있다면 실행할 비즈니스 로직이 활성화된 트랜잭션에 추가됩니다.@Entity@Data@NoArgsConstructor@AllArgsConstructor@Table(name = \"soccer_player\")public class SoccerPlayer { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String name; @ManyToOne SoccerTeam soccerTeam; @Override public String toString() { return String.format(\"name = %s / soccerTeamName = %s\", name, soccerTeam != null ? soccerTeam.getName() : null); }}@Repositorypublic interface SoccerPlayerRepository extends JpaRepository&lt;SoccerPlayer, Long&gt; {}@Servicepublic class TransactionService { @Autowired SoccerPlayerRepository soccerPlayerRepository; @Autowired TransactionChildService transactionChildService; @Transactional public void required() { SoccerPlayer player1 = new SoccerPlayer(); player1.setName(\"손흥민\"); try { transactionChildService.insert(player1); } catch (Exception ex) { ex.printStackTrace(); } SoccerPlayer player2 = new SoccerPlayer(); player2.setName(\"박지성\"); soccerPlayerRepository.save(player2); }}@Servicepublic class TransactionChildService { @Autowired SoccerPlayerRepository soccerPlayerRepository; @Transactional(propagation = Propagation.REQUIRED) public void insert(SoccerPlayer soccerPlayer) { soccerPlayerRepository.save(soccerPlayer); throw new RuntimeException(\"throw exception\"); }}@SpringBootTestpublic class TransactionPropagationTest { @Autowired TransactionService transactionService; @Autowired SoccerPlayerRepository soccerPlayerRepository; @Test public void REQUIRED() { transactionService.required(); var list = soccerPlayerRepository.findAll(); Assert.notNull(list, \"soccer player list must not be null\"); }}예제 코드입니다. 첫번째 예제라 앞으로의 테스트에 사용되는 entity, repository, service 및 test class 까지 모두 작성하였습니다.부모 메소드에서 자식 메소드를 호출하고 자식 메소드는 에러를 throw 하는 코드입니다. transactionChildService.insert() 를 try - catch 구문으로 감쌌기때문에 문제없는 코드라고 생각이 들 수도 있지만 막상 테스트 결과를 보면 Transaction silently rolled back because it has been marked as rollback-only 라는 에러를 던지고 테스트가 종료됩니다.REQUIRED의 핵심은 모든 트랜잭션이 하나로 연결되는 것이므로 모든 로직이 정상적으로 통과 되더라도 트랜잭션이 끝나는 시점에는 롤백이 되는 것입니다. 자세한 이유는 이 포스트를 통해 확인해 보세요.2. SUPPORTS두번째는 SUPPORTS 전파레벨 입니다. 활성화된 트랜잭션이 있다면 그것을 사용하고, 만약 없다면 트랜잭션 없이 메소드가 수행됩니다.@Testpublic void SUPPORTS_1() { transactionService.supports_1(); var list = soccerPlayerRepository.findAll(); Assert.isTrue(list.size() == 10, \"size of soccer player list must be 2\");}@Testpublic void SUPPORTS_2() { transactionService.supports_2(); var list = soccerPlayerRepository.findAll(); Assert.isTrue(list.size() == 10, \"size of soccer player list must be 2\");}@Transactionalpublic void supports_1() { doSupportsTest();}public void supports_2() { doSupportsTest();}private void doSupportsTest() { for (int i = 0; i &lt; 10; i ++) { try { long seq = (long) i + 1; var soccerPlayer = new SoccerPlayer(); soccerPlayer.setId(seq); soccerPlayer.setName(\"선수\" + seq); transactionChildService.insertSupports(soccerPlayer); } catch (Exception ex) { ex.printStackTrace(); } }}@Transactional(propagation = Propagation.SUPPORTS)public void insertSupports(SoccerPlayer soccerPlayer){ if (soccerPlayer.getId() &gt; 5L) { throw new RuntimeException(\"throw exception\"); } soccerPlayerRepository.save(soccerPlayer);}예제 코드입니다. 테스트를 SUPPORTS_1()과 SUPPORTS_2()로 나눴습니다. 첫번재 테스트의 경우 부모 메소드에 @Transactional을 붙였고 두번째 테스트의 부모 메소드에는 붙이지 않았습니다.결론적으로 두 테스트 모두 실패하긴 하지만 실패 사유는 다릅니다. @Transactional을 붙인 테스트의 경우 insert 자체가 일어나지 않는데 반해 @Transaction을 붙이지 않은 테스트의 경우 축구선수 list의 size가 5가 되었습니다.첫번째 테스트의 경우 자식 메소드가 수행되는 시점에 활성화된 트랜잭션(부모 트랜잭션)이 존재하기 때문에 트랜잭션이 종료되는 시점에 수행된 모든 작업이 롤백되게 됩니다.두번째 테스트의 경우 활성화된 트랜잭션이 없기 때문에 non-transactional 하게 동작하게 되어 5개의 insert된 엔티티는 롤백되지 않게 되는 것이지요.3. MANDATORY세번째는 MANDATORY 입니다. 활성화된 트랜잭션이 있다면 그것을 사용하고, 활성화된 트랜잭션이 없다면 Exception이 발생합니다.@Testpublic void MANDATORY() { transactionService.mandatory(); soccerPlayerRepository.findByName(\"손흥민\").orElseThrow(() -&gt; new RuntimeException(\"soccer player must not be null\"));}public void mandatory() { SoccerPlayer player1 = new SoccerPlayer(); player1.setName(\"손흥민\"); transactionChildService.insertMandatory(player1);}@Transactional(propagation = Propagation.MANDATORY)public void insertMandatory(SoccerPlayer soccerPlayer) { soccerPlayerRepository.save(soccerPlayer);}insertMandatory()가 수행되는 시점에 활성화된 부모 트랜잭션이 없기 떄문에 org.springframework.transaction.IllegalTransactionStateException: No existing transaction found for transaction marked with propagation 'mandatory' Exception이 발생하며 테스트가 종료됩니다.4. NEVER네번째는 NEVER 입니다. 활성화된 트랜잭션이 있다면 Exception이 발생합니다.@Testpublic void NEVER() { transactionService.never(); soccerPlayerRepository.findByName(\"손흥민\").orElseThrow(() -&gt; new RuntimeException(\"soccer player must not be null\"));}@Transactionalpublic void never() { SoccerPlayer player1 = new SoccerPlayer(); player1.setName(\"손흥민\"); transactionChildService.insertNever(player1);}@Transactional(propagation = Propagation.NEVER)public void insertNever(SoccerPlayer soccerPlayer) { soccerPlayerRepository.save(soccerPlayer);}insertNever()가 수행되는 시점에 활성화된 부모 트랜잭션이 있기 때문에 org.springframework.transaction.IllegalTransactionStateException: Existing transaction found for transaction marked with propagation 'never' Exception이 발생하며 테스트가 종료됩니다.5. NOT_SUPPORTED다섯번째는 NOT_SUPPORTED 입니다. 활성화된 트랜잭션 유무에 상관없이 non-transactional 하게 동작합니다. 활성화된 트랜잭션이 있는 경우 트랜잭션을 일시중지 시킵니다. 작업이 끝나면 자동으로 중지한 트랜잭션을 다시 시작합니다.6. NESTED여섯번째는 NESTED 입니다. NESTED는 활성화된 트랜잭션의 중첩 트랜잭션을 시작합니다. 중첩 트랜잭션이 시작할 때 SAVEPOINT가 생성됩니다. 중첩 트랜잭션이 실패한다면 생성한 SAVEPOINT로 롤백합니다. (SAVEPOINT를 지원하는 DB만 사용 가능 - PostgreSQL, Oracle, MSSQL 등) 중첩 트랜잭션은 외부 트랜잭션의 일부이므로 외부 트랜잭션이 끝날 때만 커밋됩니다. 중첩 트랜잭션이 끝났다고 하여 바로 커밋되는 일은 없습니다.@Testpublic void NESTED() { transactionService.nested(); var soccerPlayerList = soccerPlayerRepository.findAll(); Assert.isTrue(soccerPlayerList.size() == 1, \"size of soccer player list must be 1\");}@Transactionalpublic void nested() { SoccerPlayer player1 = new SoccerPlayer(); player1.setName(\"손흥민\"); soccerPlayerRepository.save(player1); SoccerPlayer player2 = new SoccerPlayer(); player2.setName(\"박지성\"); soccerPlayerRepository.save(player2); transactionChildService.insertNested(player2);}@Transactional(propagation = Propagation.NESTED)public void insertNested(SoccerPlayer soccerPlayer) { try { soccerPlayerRepository.save(soccerPlayer); throw new RuntimeException(\"throw error\"); } catch (Exception ex) { ex.printStackTrace(); }}예제 코드입니다. 박지성 이라는 이름을 가진 엔티티를 저장하는 시점에 예외가 발생하였지만 SAVEPOINT까지만 롤백되어 손흥민 이라는 이름을 가진 엔티티는 정상적으로 저장되게 되어 테스트를 통과하게 됩니다.7. REQUIRES_NEW마지막 7번째는 REQUIRES_NEW 입니다. 아마 기본 REQUIRED와 함께 가장 많이 사용하게될 전파레벨이 될텐데요, REQUIRES_NEW는 활성화된 트랜잭션이 있다면 잠시 중지시키고 새로운 트랜잭션을 생성하여 진행합니다.@Testpublic void REQUIRES_NEW() { try { transactionService.requiresNew(); } catch (Exception ex) { ex.printStackTrace(); } var soccerPlayerList = soccerPlayerRepository.findAll(); Assert.isTrue(soccerPlayerList.size() == 1, \"size of soccer player list must be 1\");}@Transactionalpublic void requiresNew() { SoccerPlayer player1 = new SoccerPlayer(); player1.setName(\"손흥민\"); transactionChildService.insertRequiresNew(player1); SoccerPlayer player2 = new SoccerPlayer(); player2.setName(\"박지성\"); soccerPlayerRepository.save(player2); throw new RuntimeException(\"throw error on parent\");}@Transactional(propagation = Propagation.REQUIRES_NEW)public void insertRequiresNew(SoccerPlayer soccerPlayer) { soccerPlayerRepository.save(soccerPlayer);}예제 코드입니다. 부모 트랜잭션에서 RuntimeException을 던졌지만 자식 트랜잭션은 새로운 트랜잭션으로 작업을 수행하였기 때문에 부모 트랜잭션에 영향을 받지 않아 손흥민 이라는 이름을 가진 엔티티는 정상적으로 저장이 되게 되어 테스트를 통과하게 됩니다." }, { "title": "Transaction Isolation", "url": "/posts/database-transaction-isolation-level/", "categories": "Database", "tags": "Database, Transaction", "date": "2022-03-23 20:12:00 +0900", "snippet": "트랜잭션트랜잭션 관리는 데이터베이스와 통신하는 코드를 작성하는 백엔드 개발자라면 누구에게나 중요한 항목입니다. 트랜잭션의 옵션에 따라 성능이 크게 향상될수 있고 메소드 내부에서 일어나는 예외 상황이 달라질수 있기 때문입니다. 따라서 트랜잭션을 이해하고 설정하는 것이 정말 중요하다고 할수 있습니다.Isolation level optionIsolation level (격리수준) 은 트랜잭션의 주요 속성(원자성, 일관성, 격리, 내구성) 중 하나입니다. 동시에 여러 트랜잭션이 처리될 때 한 트랜잭션이 다른 트랜잭션에서 변경하거나 조회하는 데이터를 볼수 있게 허용할지 말지 결정하는 것입니다.예를들어 Spring 에서는 5가지 Isolation level 을 세팅할수 있습니다. 만약 그중 하나인 DEFAULT 를 사용한다면 사용하고 있는 데이터베이스의 기본 옵션을 따라갑니다.PostgreSQL, Oracle, SQL Server의 경우 READ_COMMITTED을 사용하고, MySQL의 경우 REPEATABLE_READ을 사용합니다.격리 수준이 허용하는 옵션들에 대해 먼저 알아보죠.1. Dirty read첫번째는 Dirty read(더티 리드) 입니다. 더티 리드는 하나의 트랜잭션이 아직 커밋되지 않은 다른 트랜잭션의 변경점을 조회할 수 있음을 의미합니다.트랜잭션A가 시작된 이후 트랜잭션B가 시작됩니다. 트랜잭션A는 id가 10인 엔티티를 insert 합니다. 그 후 바로 트랜잭션B는 id가 10인 엔티티를 조회합니다. 그런데 그 이후 트랜잭션A가 문제가 생겨 모두 롤백되어 버렸습니다. 이제 트랜잭션B 에서 조회된 entity 객체는 데이터 무결성을 보장받지 못하는 상태가 되었습니다.2. Non-repeatable read두번째는 Non-repeatable read (반복 가능하지 않은 조회) 입니다. 반복 가능하지 않은 조회는 한 트랜잭션에서 동일한 행을 읽더라도 다른 트랜잭션에서 발생한 update or delete가 적용되기 때문에 행 내의 값이 다를수 있음을 의미합니다.트랜잭션 A가 시작된 이후 트랜잭션B가 시작됩니다. 트랜잭션A는 id가 10인 엔티티를 조회합니다. 그 후 트랜잭션B가 id가 10인 엔티티를 수정합니다. 그 후 다시 트랜잭션A가 id가 10인 엔티티를 조회합니다. 트랜잭션A는 동일한 행을 조회하였지만 그 각각의 결과값은 달라집니다.3. Phantom read마지막 세번쨰는 Phantom read(팬텀 리드) 입니다. 팬텀 리드는 한 트랜잭션에서 동일한 조회 쿼리를 수행하더라도 다른 트랜잭션에서 발생한 insert가 적용되기 때문에 행의 갯수(열)가 다를 수 있음을 의미합니다.트랜잭션A는 entity 테이블에서 모든 row를 조회합니다. 그 후 트랜잭션B에서 id가 10인 엔티티를 추가하였습니다. 그 후 다시 트랜잭션A에서 entity 테이블의 모든 row를 조회한다면 row의 갯수가 한개 늘어났을 것입니다.Non-repeatable read vs Phantom read반복 가능하지 않은 조회와 팬텀 리드 간에는 차이가 있습니다. 예제를 통해 알아보도록 하겠습니다. 트랜잭션 A시작 트랜잭션 B시작 트랜잭션 A에서 X 쿼리(조회) 수행 트랜잭션 B에서 Y 쿼리(INSERT, UPDATE) 수행 트랜잭션 B 커밋 트랜잭션 A에서 X 쿼리(조회) 수행위와같은 플로우로 한 싸이클이 돈다고 가정해 보았습니다. 만약 Non-repeatable read 수준이라면 트랜잭션 B에서 INSERT가 일어나더라도 3번의 결과 행의 갯수와 6번의 결과 행의 갯수는 동일할 것입니다. 만약 UPDATE가 일어났다면 행의 갯수는 동일하지만 각각 행의 값은 다르겠지요.만약 Phantom read 수준이라면 트랜잭션 B에서 INSERT가 일어나면 3번의 결과 행의 갯수와 6번의 결과 행의 갯수는 다를 것입니다. 하지만 UPDATE가 일어난다면 각각 행의 값은 동일하겠지요.Isolation level옵션들에 대해 알아봤으므로 이제 격리 수준의 종류에 대해 정리해보겠습니다. Isolation Level Dirty Read Non-repeatable read Phantom read READ_UNCOMMITTED O O O READ_COMMITTED X O O REPEATABLE_READ X X O SERIALIZABLE X X X 격리 수준을 정리한 표입니다. READ_UNCOMMITTED (레벨 0) 부터 SERIALIZABLE (레벨 3) 까지 확인하실 수 있습니다.Isolation level 선택하기격리 수준에 대한 조정은 동시성, 데이터 무결성과 연관되어 있습니다. 동시성을 증가시키면 (레벨 0) 데이터 무결성에 문제가 발생하고 데이터 무결성을 유지하면 (레벨 3) 동시성이 떨어져 발생하는 비용이 증가하기 때문에 실제 어플리케이션에 영향을 끼치겠지요.처리하는 로직이 모두 다르기 떄문에 특정한 격리 수준이 좋다 나쁘다를 말하기는 어렵겠지요. 따라서 로직에 따라 적절한 수준을 선택하는 것이 중요하다고 볼수 있겠습니다." }, { "title": "JPA EntityListeners 에 의존성 주입하기", "url": "/posts/jpa-entity-listener-di/", "categories": "JPA", "tags": "JPA, Hibernate", "date": "2022-03-17 20:12:00 +0900", "snippet": "JPA EntityListenersJPA를 사용하다보면 여러가지 상황(before insert, after insert, before update, after update…)을 캐치하여 작업을 해야할 경우가 생기곤 합니다. 그럴때 하이버네이트 에서 제공하는 EntityListeners를 사용하여 원하는 작업을 수행할 수 있습니다.위 캡쳐 이미지는 하이버네이트에서 지원하는 콜백 리스트입니다. 해당 콜백 메소드들을 사용해 간단히 엔티티 이벤트를 캐치할 수 있습니다. 짧은 코드로 구현하자면 다음과 같은 형태가 되겠지요.@Entity@Data@NoArgsConstructor@AllArgsConstructor@EntityListeners(value = SoccerTeamListeners.class)@Table(name = \"soccer_team\")public class SoccerTeam { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String name;}@Componentpublic class SoccerTeamListeners{ @Autowired SoccerTeamRepository soccerTeamRepository; @PrePersist public void prePersist(SoccerTeam soccerTeam) { System.out.println(\"size: \" + soccerTeamRepository.findAll().size()); }}JPA EntityListeners Dependency Injection위의 예제 코드는 잘못된 코드입니다. EntityListeners로 지정한 클래스에서 의존성 주입이 필요해 SoccerTeamRepository를 주입하였지만, 정작 prePersist 메소드 수행시 soccerTeamRepository에는 null이 할당되어있을 것입니다. null이 할당되어 있으니 당연히 메소드 호출시 NullPointerException 이 발생합니다.에러가 발생하는 이유는 EntityListeners는 Spring IOC 에서 관리되는 클래스가 아니기 때문입니다. 정확히는 EntityListeners에 등록되는 클래스에 @Component 어노테이션을 붙이면 bean으로 등록되긴 하지만 JPA 관련 프로퍼티들이 bean으로 등록되고 난 후에 Spring 관련 빈들이 등록되기 때문입니다.따라서 Spring bean이 모두 등록된 후 EntityListeners로 지정한 클래스를 모두 찾아 의존성 주입을 하면 되겠네요. 인터넷을 검색해보면 많은 해결방법들이 있습니다. 저는 제 나름대로의 방법을 소개하고자 합니다.Example Code@Entity@Data@NoArgsConstructor@AllArgsConstructor@Builder@EntityListeners(value = SoccerPlayerListeners.class)@Table(name = \"soccer_player\")public class SoccerPlayer { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String name; @ManyToOne SoccerTeam soccerTeam; @Override public String toString() { return String.format(\"name = %s / soccerTeamName = %s\", name, soccerTeam != null ? soccerTeam.getName() : null); }}엔티티입니다. SoccerPlayerListeners.class를 EntityListeners로 등록하였습니다.public interface SimpleEventListeners {}구현할 메소드가 아무것도 없는 인터페이스입니다. 이 인터페이스는 추후 사용될 것입니다.@Componentpublic class SoccerPlayerListeners implements SimpleEventListeners { private static SoccerPlayerRepository soccerPlayerRepository; @PrePersist public void prePersist(SoccerPlayer soccerPlayer) { System.out.println(\"total soccer player count: \" + soccerPlayerRepository.findAll().size()); }}SoccerPlayerListeners 클래스입니다. 구현할 메소드가 없긴 하지만 위에서 정의한 인터페이스를 구현하는 클래스입니다. 모든 객체가 메모리를 공유하도록 하기 위해 static 키워드를 사용하여 변수를 선언하였습니다.@PrePersist 어노테이션을 붙여 새로운 엔티티가 flush되기 전에 축구선수의 총원을 콘솔에 찍는 메소드를 선언하였습니다.@Configurationpublic class SimpleEventListenerConfig { @Autowired private ApplicationContext applicationContext; @Autowired private List&lt;SimpleEventListeners&gt; simpleEventListenerList; @PostConstruct public void injectDependency() { simpleEventListenerList.forEach(el -&gt; { Arrays.stream(el.getClass().getDeclaredFields()) .filter(field -&gt; Modifier.isStatic(field.getModifiers())) .forEach(field -&gt; { field.setAccessible(true); try { field.set(field.getType(), applicationContext.getBean(field.getType())); } catch (Exception ex) { System.err.println(ex.getMessage()); } }); }); }}가장 중요한 config 파일입니다. @PostConstruct가 실행될때는 모든 Spring 관련 bean들이 등록된 후입니다.여기서 SimpleEventListeners 인터페이스를 사용한 이유가 나옵니다. SimpleEventListeners를 구현하는 모든 클래스들을 변수에 할당해야하기 때문입니다. 이 방식은 List&lt;SimpleEventListeners&gt; 객체를 루프 돌면서 static 변수만 필터링하고 리플렉션을 이용해 static 변수에 값을 할당하는 방식입니다.그리고 나서 새로운 엔티티가 flush되기 전에 정의한 prePersist 메소드가 수행되는지 테스트해보면 정상적으로 수행되는것을 확인하실 수 있을 것입니다. (java 17에서도 문제없이 동작합니다.)" }, { "title": "JPA & Hibernate Flush Mode", "url": "/posts/jpa-flush/", "categories": "JPA", "tags": "JPA, Hibenate", "date": "2022-03-05 20:12:00 +0900", "snippet": "Flush플러시는 영속성 컨텍스트의 변경 내용을 데이터베이스에 반영하는것을 의미합니다. 플러시는 다음과 같은 경우에 발생합니다. EntityManager의 flush 메소드를 호출했을 때 트랜잭션 커밋시 JPQL 쿼리 실행시영속성 컨텍스트에 있는 엔티티를 지우고 db에 저장하는 개념이 아니라 영속성 컨텍스트의 변경 내용을 db와 동기화하는 것이 플러시 임을 잊지 마세요.Flush Mode in JPA &amp; HibernateJPA는 AUTO, COMMIT 2개의 플러시 모드를 지원하고 Hibernate는 AUTO, COMMIT, ALWAYS, MANUAL 4개의 플러시 모드를 지원합니다.AUTO (JPA &amp; Hibernate)JPA 명세는 FlushModeType.AUTO 를 기본 타입으로 정의합니다. AUTO는 다음 2가지 상황에서 영속성 컨텍스트를 플러시 합니다. 트랜잭션이 커밋되기 이전 영속성 컨텍스트에 보류 중인 변경이 포함된 데이터베이스 테이블을 조회하는 쿼리를 실행하기 전모든 JPQL 또는 Criteria Query에 대해 하이버네이트는 SQL문을 생성합니다. 따라서 어떤 데이터베이스 테이블이 사용되는지 미리 알수 있습니다. 하이버네이트는 그정보를 영속성 컨텍스트의 모든 엔티티 객체에 대해 더티 체크를 수행할 때 사용할 수 있습니다. 만약 쿼리에 의해 참조되는 테이블 중 하나에 매핑된 더티 엔티티가 발견되면 조회하기 전에 변경사항을 플러시해야 합니다.@Entity@Data@NoArgsConstructor@AllArgsConstructor@Table(name = \"soccer_team\")public class SoccerTeam { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String name;}@Entity@Data@NoArgsConstructor@AllArgsConstructor@Table(name = \"soccer_team\")public class SoccerTeam { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String name;}@Testvoid flushTest() { EntityManager em = emf.createEntityManager(); em.getTransaction().begin(); var soccerPlayer = new SoccerPlayer(); soccerPlayer.setName(\"손흥민\"); em.persist(soccerPlayer); em.createQuery(\"SELECT t FROM SoccerTeam t\").getResultList(); Query q = em.createQuery(\"SELECT p FROM SoccerPlayer p WHERE p.name = :name\"); q.setParameter(\"name\", \"손흥민\"); q.getResultList(); em.getTransaction().commit(); em.close();}예제 코드입니다. SoccerTeam은 SoccerPlayer의 부모 엔티티 입니다. em.createQuery(\"SELECT t FROM SoccerTeam t\").getResultList(); 쿼리를 수행하여도 SoccerTeam 엔티티는 SoccerPlayer 엔티티를 참조하지 않기 떄문에 insert 쿼리가 날라가지 않습니다.마지막에서 세번째 q.getResultList(); 를 수행할때야 비로소 insert 쿼리가 데이터베이스로 날라가게 됩니다. 하이버네이트가 insert 구문의 수행 시간을 지연시킴으로서 성능상의 이점을 만들수도 있게되는 것입니다.만약 네이티브 SQL 쿼리를 사용한다면 조금 더 복잡해집니다. 하이버네이트는 JPQL이 아닌 네이티브 SQL 쿼리가 사용하는 테이블을 판별할 수 없기 때문에, 각 네이티브 쿼리의 대상을 만들어야 합니다. 그렇지 않으면 영속성 컨텍스트를 플러시해야 하는지 여부를 결정할 수 없게 됩니다.COMMIT (JPA &amp; Hibernate)플러시 타입 COMMIT은 트랜잭션을 커밋하기 전에 플러시가 필요하지만 쿼리를 실행하기 전에 수행해야할 작업을 정의하지 않습니다. 하이버네이트 5, 6 에서는 아무 쿼리나 수행하는 것만으로는 보류중인 수정사항을 플러시하지 않습니다. 간단히 말해 flush()를 직접 호출하거나 트랜잭션이 커밋되는 경우에만 플러시 됩니다.@Testvoid flushCOMMIT() { EntityManager em = emf.createEntityManager(); em.setFlushMode(FlushModeType.COMMIT); em.getTransaction().begin(); var soccerPlayer = new SoccerPlayer(); soccerPlayer.setName(\"손흥민\"); em.persist(soccerPlayer); em.createQuery(\"SELECT f FROM SoccerTeam f\").getResultList(); Query q = em.createQuery(\"SELECT p FROM SoccerPlayer p WHERE p.name = :name\"); q.setParameter(\"name\", \"손흥민\"); q.getResultList(); em.getTransaction().commit(); em.close();}예제 코드입니다. AUTO 때의 예제코드에 em.setFlushMode(FlushModeType.COMMIT); 한줄을 추가하여 플러시 모드를 COMMIT 으로 설정했습니다. 예제를 실행시켜보면 위 AUTO때와 다르게 em.getTransaction().commit(); 코드가 실행될때 insert 쿼리가 데이터베이스에 날라가게 됩니다.실제 프로덕션 레벨의 어플리케이션에서는 AUTO 보다는 COMMIT 모드를 권장합니다. 참고로 AUTO, COMMIT 모드의 경우 위의 예제처럼 ‘entityManager.setFlushMode(FlushModeType.COMMIT);’ 형식으로 모드를 지정할 수 있습니다.ALWAYS (Hibernate)ALWAYS 모드는 하이버네이트 명세에만 존재하기 때문에 만약 Spring Data JPA를 사용한다면 사용할수 없는 모드입니다. 이 모드는 하이버네이트에게 쿼리를 실행하기 전에 영속성 컨텍스트를 플러시하라고 지시합니다. 이 모드를 사용한다면, 하이버네이트는 플러시가 필요한지 여부를 확인하지 않고 모든 유형의 쿼리를 동일한 방식으로 처리합니다.@Testvoid flushALWAYS() { EntityManager em = emf.createEntityManager(); Session session = em.unwrap(Session.class); session.setHibernateFlushMode(FlushMode.ALWAYS); em.getTransaction().begin(); var soccerPlayer = new SoccerPlayer(); soccerPlayer.setName(\"손흥민\"); em.persist(soccerPlayer); Query q = em.createQuery(\"SELECT f FROM SoccerTeam f\"); q.getResultList(); var insertedPlayer = em.createQuery(\"SELECT p FROM SoccerPlayer p\").getResultList().stream().findFirst().orElse(null); Assert.notNull(insertedPlayer, \"One soccer player should be inquired.\"); em.getTransaction().commit(); em.close();}예제 코드입니다. SoccerTeam 조회 쿼리의 플러시 모드를 ALWAYS로 설정하였습니다. 예제를 실행시켜보면 SoccerTeam은 SoccerPlayer의 존재를 모름에도 불구하고 플러시 모드를 ALWAYS로 세팅하였기 때문에 select 쿼리가 날라가기 전에 insert 쿼리가 수행되는것을 확인할 수 있습니다. 테스트 또한 정상적으로 통과됩니다.MANUAL (Hibernate)마지막으로 MANUAL 모드입니다. MANUAL은 ALWAYS와 마찬가지로 하이버네이트 스펙에서만 지원하는 모드입니다. 이 모드는 모든 자동 플러시가 비활성화되고 개발자가 명시적으로 플러시 코드를 작성해야 플러시가 동작합니다.@Testvoid flushMANUAL() { EntityManager em = emf.createEntityManager(); Session session = em.unwrap(Session.class); session.setHibernateFlushMode(FlushMode.MANUAL); em.getTransaction().begin(); var soccerPlayer = new SoccerPlayer(); soccerPlayer.setName(\"손흥민\"); session.save(soccerPlayer); session.flush(); var insertedPlayer = em.createQuery(\"SELECT p FROM SoccerPlayer p\").getResultList().stream().findFirst().orElse(null); Assert.notNull(insertedPlayer, \"One soccer player should be inquired.\"); em.getTransaction().commit(); em.close();}예제 코드입니다. SoccerPlayer 엔티티를 저장하는 코드인데요, 한번 실행시켜 본 후에 session.flush(); 코드를 제거하고 다시 실행시켜 보세요. 플러시를 명시적으로 작성하지 않았기 때문에 플러시가 일어나지 않아 테스트는 실패하게 됩니다.결론오늘은 JPA &amp; Hibernate의 4가지 플러시 모드를 사펴보았습니다. 개인적으로는 Spring Data JPA가 MANUAL 모드를 지원하지 않아 조금 아쉽습니다. Spring Data JPA가 나오기 이전 Hibernate / Session 으로 이리저리 놀았던 때에는 비즈니스 로직에 따라 세세하게 설정할수 있었던 점이 좋았던것 같습니다.그렇다고 이제와서 Spring Data JPA를 쓰지 않기엔 너무 편리한것도 사실입니다. 그런날이 올지는 모르겠지만 JPA 명세에 하이버네이트의 스펙이 추가되었으면 좋겠네요." }, { "title": "홈서버 구축기", "url": "/posts/home-server/", "categories": "ETC", "tags": "ETC", "date": "2022-02-19 10:12:00 +0900", "snippet": "개요집에서 시놀로지 NAS를 굴린지 3년이 되어간다. 그당시 개인 저장소에 대한 막연한 환상이 있어 구매하였지만 생각보다 코어단을 건드리는 부분에 있어 제약사항이 꽤 많았던것 같다. 뭐 도커를 설치하고 어쩌구 저쩌구 하면 되지만 결국 OS는 DSM이다.심지어 야심차게 밖아넣은 1TB HDD는 20%도 사용하지 않고 먼지만 쌓여가고 있었다. 그래서 올해 중고로 팔아버렸다. 제 값을 못받은것 같긴 하지만 뭐 쓸데도 없으니 후회하진 않는다.NAS를 팔긴 했지만 개인 서버가 필요하긴 했다. 중요한 자료를 저장할 곳이 필요하고 실시간으로 자료를 업로드하고 다운받을 수 있는, 그러면서 용량은 그렇게 크지 않아도 되는 그런 작은 서버 말이다.처음 생각한건 AWS였다. 근데 EBS의 용량이 올라가면 올라갈수록 요금이 기하급수적으로 올라가는것을 보고 포기하게 되었다. 무엇보다 최근 너무 많은 일들(개인, 회사..) 을 클라우드 서비스로 처리하다 보니 조금 질린(?) 듯 하였다.그래서 결론적으로 선택한건 베어본 PC이다. 적당히 전력을 먹으면서 내가 원하는 사양으로 맞출 수 있을것 같았다. (무엇보다 사이즈가 작고 이뻐서 인테리어에 큰 영향을 주지 않는다.)하드웨어 구성1. PC내가 선택한 PC는 PN41-BBC035MV 요놈이다. 2코어 2스레드의 사양을 가진 셀러론 N4505 CPU를 탑재하였고 USB 3.0 지원, 블루투스 탑재, 0.7kg의 무게 등의 특징을 가지고 있다. 무엇보다 이쁘다.모니터암 아래 남는 공간에 배치할 수 있어서 좋다. (에너지 절약 스티커가 있어도 이쁘네)2. 저장장치베어본 CP라 저장장치가 포함되어 있지 않다. 프로그램을 돌리는 용도의 SSD 256GB 와 파일, 데이터 저장용 HHD 1TB로 구성했다. SSD와 HDD는 각각 M.2 2280 for PCIe, 2.5인치의 규격으로 맞춰야 한다.안타깝게도 집에 굴러다니는 SSD와 HDD는 모두 데스크탑 용이었기 때문에 눈물을 머금고 10만원을 지출했다.3. RAM램도 포함되어있지 않지만 마침 8GB 짜리 램이 있어 램은 그대로 사용하기로 했다. 램은 노트북용인 SO-DIMM 규격을 사용해야 한다.4. 공유기기존 NAS를 사용할때는 NAS 자체를 거실에 두고 사용했고 IP가 바뀌더라도 그를 탐지하는 프로그램을 통해 자동으로 내 휴대폰에서 접근할수 있게끔 사용해왔다. 하지만 나는 이 홈서버는 24시간 중단없이 돌릴 생각이다. 선택할 수 있는 방법은 2가지가 있었다. 월 요금 지불후 고정 IP 사용 공유기(ipTIME)의 DDNS 기능 사용어쨌거나 나는 내 방에 PC를 둘 생각이었고 내 방의 랜포트는 한개기 때문에 공유기가 필요했다. 공유기를 사는 김에 DDNS 기능을 사용해 보기로 했다. 선택한 공유기는 A2004NS-MU 이다. 그냥 쿠팡에서 로켓배송 지원하고 1등이길래 구매했다.서버 구성 및 구축1. OSOS는 Rocky Linux로 정했다. Rocky Linux도 언젠가는 CentOS 꼴이 날것이라 생각하는 사람도 있고 나 역시 그럴 가능성이 있어 Ubuntu와 끝까지 고민했지만, 나는 레드햇 계열의 리눅스에 더 끌린다. 어쩔수 없나 보다.집에서 굴러다니는 USB을 부팅디스크로 만들어 OS를 설치했다.2. 원격접속최우선으로 작업해야 하는것은 메인 데스크톱을 통해 SSH로 접속 가능하게 하는 것이다. 키보드, 모니터를 pc마다 옮겨가며 세팅할 수 없기 때문에 다음 사진과 같이 책상이 더러워진다.다행이 putty나 xshell등 원격접속 툴을 사용할 줄 아는 사람이라면 문제가 없을듯 하다. 두 PC의 사설 IP 대역은 동일하기 때문이다.위 이미지와 같이 iptime 자체의 IP는 192.168.0.1, Windows의 경우 192.168.0.2, Linux의 경우 192.168.0.3을 각각 할당받고 있다. 이것이 의미하는 것은 Windows의 원격접속 툴에서 192.168.0.3 / 22 를 호스트 / 포트로 설정후 접속하면 된다는 뜻이 된다.Rocky Linux의 경우 22번 포트가 기본으로 모든 ip에서 접근을 허용하기 때문에 그냥 접속될 것이다. 혹시 접속이 안된다면 리눅스에서 netstat -tnlp 명령어를 통해 22번 포트가 오픈되어 있는지 확인하고 열있지 않다면 방화벽 설정을 통하여 22번 포트를 오픈해보자.3. WOL 설정자세하게 다루지는 않겠지만 PC가 알수없는 이유로 꺼졌을 경우나 외부에서 PC를 켜야할 경우를 대비하여 원격 부팅을 설정했다. 이 글을 통해 공유기를 세팅하였고 이 글 을 참조해 Linux에서 설정하였다.주의해야 할점은 일반 커맨드로 설정한 경우 시스템이 종료되면 설정이 초기화되기 때문에 네트워크 스크립트에 추가해줘야 한다는 점이다.4. 모니터링 툴 선택모니터링 툴을 선정해야 한다. 새삼 AWS의 Cloud Watch 기능이 그리워졌었다. 모니터링은 서버 운용에 있어 가장 중요하다고 생각했기 때문에 모니터링 툴을 알아봤는데 최종적으로 Zabbix와 Prometheus중에 하나 선택하기로 했다.Prometheus는 시각화 도구인 Grafana와 세트라고 생각하는데, 이 Grafana가 Zabbix보다 UI / UX 면에서 이뻐보이고 해외 포럼도 Prometheus의 손을 들어주는것 같았기에 결론적으로 Prometheus를 선택하였다.아직 구동중인 어플리케이션이나 기타 DB는 없기 때문에 일단 Node Exporter를 통해 CPU, OS, Memory 등의 시스템 매트릭을 모니터링만 할수 있도록 구성하였다.5. Docker마지막으로 도커를 설치하였다. 각각의 컨테이너는 한개씩일 것이기 때문에 k8s는 오버스펙이라고 생각하여 설치하지 않았다. 모든 어플리케이션, DB 등은 이 도커위의 컨테이너로 올라갈 것이다.마치며오랬동안 함께했던 NAS를 팔아버리고 새로운 미니 PC를 영입했다. AWS로 편하게 인스턴스 띄우다 실제 물리적인 장비 위에 Linux를 너무 오랜만에 깔아봐서 그런지 삽질을 좀 한것 같다. 그래도 옛날에 가상머신에 Linux깔아서 포트포워딩으로 포트폴리오 오픈하고 했던 기억이 새록새록 떠올라 재밌었다.실제 서비스를 오픈하거나 외부에 오픈할 것도 아니고 그냥 혼자 아기자기 하게 쓸 용도로 만든 구축한 서버기 때문에 돌리고 싶었던거나 많이 돌려봐야 겠다. 무엇보다 전기세가 NAS를 돌릴때보다 적게 나올것 같아 좋다." }, { "title": "Hibernate Reactive", "url": "/posts/hibernate-reactive/", "categories": "Java", "tags": "Java, Hibernate, Webflux", "date": "2022-02-10 10:12:00 +0900", "snippet": "개요Hibernate Reactive가 드디어 정식 출시 되었습니다. (사실 현재기준 3달 지났습니다.)Webflux - R2DBC로 프로젝트를 진행할때 ORM에 대한 갈망을 느끼곤 했었는데, 확실히 갈증을 해소해줄 만한 물건이 나온것 같습니다.현재는 Quarkus, Panache와 사용했을때 더 효율성을 발휘하는 것으로 보입니다. 그래도 예전 Spring Data JPA가 나오기전 Hibernate를 사용할때를 떠올리며 내용을 정리해 보았습니다. 언젠가는 Spring Data Family에 포함되어 Spring Data Reactive JPA(?)가 나오길 희망해 봅니다.Hibernate ReactiveWebflux가 나온지 4년이 지났습니다. Spring Data R2DBC 를 통해 non-blocking 하게 데이터베이스에 접근할 수 있지만, 이렇다할 ORM 구현체는 없는 상황이었습니다.Hibernate Reactive는 Hibernate ORM 을 위한 리액티브 API로, 데이터베이스에 non-blocking하게 접근할 수 있도록 지원합니다. Hibernate Reactive는 데이터베이스와 non-blocking 방식으로 통신하도록 설계된 최초의 ORM 구현체 입니다.또한 많은 부분이 기존의 Hibernate ORM, JPA 2.2 과 동일하기 때문에, 만약 JPA에 대한 이해도가 깊은 분이라면 쉽게 Hibernate Reactive에 적응하실 것이라 생각합니다.소개 및 예제이어질 내용 부터는 간단한 CRUD 어플리케이션을 만들면서 Hibernate Reactive를 소개해 보겠습니다.1. 프로젝트 세팅Spring 진영에서 아직까지는 Hibernate Reactive를 받아들이지 않은 것으로 보입니다만, 다행히도 Spring과 Hibernate Reactive를 조합하는 것은 그리 어렵지 않은것 같습니다. 따라서 저는 Spring Webflux 기반의 프로젝트를 만들어 보겠습니다.1.1. 라이브러리 추가사용할 라이브러리들을 정의한 gradle 파일 입니다.dependencies { implementation 'org.springframework.boot:spring-boot-starter-webflux' implementation 'org.hibernate.reactive:hibernate-reactive-core:1.1.2.Final' implementation 'io.vertx:vertx-pg-client:4.2.4' implementation 'com.ongres.scram:client:2.1' implementation 'io.smallrye.reactive:mutiny-reactor:1.3.1' implementation 'org.projectlombok:lombok'} 나중에 com.ongres.scram 클래스가 없다는 에러가 발생시 패키지에 implementation 'org.hibernate.orm:hibernate-jpamodelgen:6.0.0.CR1' 를 추가해보세요.이 프로젝트는 PostgreSQL 을 기반으로 진행될 예정 이지만 현재 사용할 수 있는 데이터베이스와 드라이버는 다음과 같습니다. Database Driver dependency PostgreSQL or CockroachDB io.vertx:vertx-pg-client:{vertxVersion} MySQL or MariaDB io.vertx:vertx-mysql-client:{vertxVersion} DB2 io.vertx:vertx-db2-client:{vertxVersion} SQL Server io.vertx:vertx-mssql-client:${vertxVersion} 1.2. persistence.xml 정의Hibernate Reactive는 표준 JPA persistence.xml 문서를 통해 구성되며, 보통 /META-INF 디렉토리에 배치되어야 합니다.&lt;persistence xmlns=\"http://xmlns.jcp.org/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_2.xsd\" version=\"2.2\"&gt; &lt;persistence-unit name=\"pu\"&gt; &lt;provider&gt;org.hibernate.reactive.provider.ReactivePersistenceProvider&lt;/provider&gt; &lt;properties&gt; &lt;!-- PostgreSQL --&gt; &lt;property name=\"javax.persistence.jdbc.url\" value=\"jdbc:postgresql://localhost:5432/database\"/&gt; &lt;!-- Credentials --&gt; &lt;property name=\"javax.persistence.jdbc.user\" value=\"user\"/&gt; &lt;property name=\"javax.persistence.jdbc.password\" value=\"password\"/&gt; &lt;!-- The Vert.x SQL Client connection pool size --&gt; &lt;property name=\"hibernate.connection.pool_size\" value=\"10\"/&gt; &lt;!-- Automatic schema export --&gt; &lt;property name=\"javax.persistence.schema-generation.database.action\" value=\"update\"/&gt; &lt;!-- SQL statement logging --&gt; &lt;property name=\"hibernate.show_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.highlight_sql\" value=\"true\"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; Hibernate Reactive에 유일한 필수 조건은 요소이며, 이는 꼭 명시되어야 합니다. 구성 속성중 JDBC라고 명시되어 있지만 Hibernate Reactive는 JDBC가 없으며 과거 JPA 사양에서 정의한 레거시 속성 이름을 뿐입니다. 특히 Hibernate Reactive는 자체적으로 JDBC URL을 읽고 해석합니다. hibernate.dialect 를 지정할 필요는 없습니다. Hibernate Reactive는 자동으로 알맞은 dialect를 결정합니다. Hibernate는 구성 가능한 많은 것들을 가지고 있지만 이것은 레거시 코드와의 호환성을 위해 유지되고 있을 뿐 JDBC와 JTA와의 직접적인 관련은 없습니다.다음은 자동 스키마 구성에 대한 내용입니다. 프로퍼티명 설명 javax.persistence.schema-generation.database.action create: 스키마를 삭제하고 테이블, 시퀀스, 제약조건을 생성   create-only: 테이블, 시퀀스, 제약조건을 생성   create-drop: 스키마를 삭제하고 SessionFactory가 생성되는 시점에 재생성, 추가적으로 SessionFactory가 소멸되면 스키마를 삭제   drop: SessionFactory가 소멸되는 시점에 스키마를 삭제   validate: 변경없이 데이터베이스 스키마와 엔티티가 일치하는지 검증   update: 엔티티와 스키마를 비교하여 스키마에 존재하지 않는 엔티티(혹은 필드)가 존재 한다면 스키마를 업데이트 javax.persistence.create-database-schemas (선택사항) 만약 true 라면, 스키마와 카탈로그를 자동 생성 javax.persistence.schema-generation.create-source (선택사항) 값이metadata-then-script혹은 script-then-metadata 테이블, 시퀀스를 생성할 때 추가 SQL 스크립트를 실행 javax.persistence.schema-generation.create-script-source (선택사항) 바로 위에서 실행할 SQL 스크립트 이름 주의! Db2의 경우 validate 와 update를 지원하지 않습니다.1.3 SQL 로깅 프로퍼티명 설명 hibernate.format.sql true라면, 멀티라인, 들여쓰기 형식으로 로그 기록 hibernate.highlight_sql true라면, ANSI 이스케이프 코드를 통해 구문을 강조하는 형태로 로그 기록 위 두 옵션이 모두 true라면 아래와 같이 보이게 됩니다.2. Java code 작성본격적으로 자바 코드를 작성해 보겠습니다.@Entity@Table(name = \"post\")@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class Post { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; private String title; private String contents; @ManyToOne(targetEntity = Author.class) private Author author;}@Entity@Table(name = \"author\")@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class Author { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; private String name;}작성자와 글을 N:1의 연관관계로 표현하였습니다.@Beanpublic Mutiny.SessionFactory sessionFactory() { return Persistence .createEntityManagerFactory(\"pu\") .unwrap(Mutiny.SessionFactory.class);}SessionFactory 객체를 bean으로 등록합니다. 이때 createEntityManagerFactory에 들어갈 값은 persistence.xml 파일의 persistence-unit name 의 값과 동일해야 합니다.Mutiny 타입과 Stage 타입을 선택할 수 있는데요, 큰 차이는 없고 공식문서의 코드는 Mutiny를 선택해였기 때문에 여기서도 Mutiny를 사용하도록 하겠습니다.public interface KcReactiveRepository { &lt;E&gt; Uni&lt;E&gt; save(E entity); &lt;E&gt; Uni&lt;List&lt;E&gt;&gt; listAll(Class&lt;E&gt; clazz); &lt;E&gt; Uni&lt;E&gt; findById(Class&lt;E&gt; clazz, Object id); &lt;E&gt; Uni&lt;Integer&gt; deleteById(Class&lt;E&gt; clazz, Object id); &lt;E&gt; Uni&lt;Integer&gt; deleteAll(Class&lt;E&gt; clazz);}@Componentpublic record KcReactiveRepositoryImpl(Mutiny.SessionFactory sessionFactory) implements KcReactiveRepository { @Override public &lt;E&gt; Uni&lt;E&gt; save(E entity) { Assert.notNull(entity, \"entity must not be null!\"); return sessionFactory.withSession(session -&gt; session.persist(entity).chain(session::flush).replaceWith(entity)); } @Override public &lt;E&gt; Uni&lt;List&lt;E&gt;&gt; listAll(Class&lt;E&gt; clazz) { var query = this.buildQuery(clazz); query.from(clazz); return sessionFactory.withSession(session -&gt; session.createQuery(query).getResultList()); } @Override public &lt;E&gt; Uni&lt;E&gt; findById(Class&lt;E&gt; clazz, Object id) { Assert.notNull(id, \"id must not be null!\"); return sessionFactory.withSession(session -&gt; session.find(clazz, id)); } @Override public &lt;E&gt; Uni&lt;Integer&gt; deleteById(Class&lt;E&gt; clazz, Object id) { Assert.notNull(id, \"id must not be null!\"); var criteriaBuilder = this.sessionFactory.getCriteriaBuilder(); CriteriaDelete&lt;E&gt; delete = criteriaBuilder.createCriteriaDelete(clazz); var root = delete.from(clazz); var idField = Arrays.stream(clazz.getDeclaredFields()) .filter(field -&gt; field.getAnnotation(Id.class) != null) .findFirst().orElse(null); Assert.notNull(idField, \"id field of entity must not be null!\"); delete.where(criteriaBuilder.equal(root.get(idField.getName()), id)); return sessionFactory.withTransaction((session, tx) -&gt; session.createQuery(delete).executeUpdate()); } @Override public &lt;E&gt; Uni&lt;Integer&gt; deleteAll(Class&lt;E&gt; clazz) { Assert.notNull(clazz, \"class not not be null!\"); var criteriaBuilder = this.sessionFactory.getCriteriaBuilder(); CriteriaDelete&lt;E&gt; delete = criteriaBuilder.createCriteriaDelete(clazz); delete.from(clazz); return sessionFactory.withTransaction((session, tx) -&gt; session.createQuery(delete).executeUpdate()); } private &lt;E&gt; CriteriaQuery&lt;E&gt; buildQuery(Class&lt;E&gt; clazz) { return sessionFactory.getCriteriaBuilder().createQuery(clazz); }}대망의 리포지토리 코드입니다. 문서를 보며 만들다 보니 모듈화 할수있는 부분이 보여 모듈화하여 코드를 작성해 보았습니다. 간단한 CRUD 기능을 수행할 수 있는 리포지토리 입니다.중간에 deleteById의 경우 기본키가 하나일 경우만을 생각하고 작성하였습니다. 복합키는 고려대상이 아닙니다.SessionFactory를 의존성으로 주입받아야 하기 때문에 아래 bean을 등록해 줍니다.@Beanpublic KcReactiveRepository kcReactiveRepository() { return new KcReactiveRepositoryImpl( Persistence .createEntityManagerFactory(\"pu\") .unwrap(Mutiny.SessionFactory.class) );}마지막으로, 위에서 작성한 리포지토리를 사용해 각각의 도메인에서 CRUD 기능을 수행하는 클래스들을 생성하였습니다.@Repositorypublic class PostRepository { @Autowired KcReactiveRepository kcReactiveRepository; public Uni&lt;Post&gt; save(String title, String contents, Author author) { Post entity = Post.builder() .title(title) .contents(contents) .author(author) .build(); return kcReactiveRepository.save(entity); } public Uni&lt;List&lt;Post&gt;&gt; listAll() { return kcReactiveRepository.listAll(Post.class); } public Uni&lt;Post&gt; findById(Long id) { return kcReactiveRepository.findById(Post.class, id); } public Uni&lt;Integer&gt; deleteAll() { return kcReactiveRepository.deleteAll(Post.class); } public Uni&lt;Integer&gt; deleteById(Long id) { return kcReactiveRepository.deleteById(Post.class, id); }}@Repositorypublic class AuthorRepository { @Autowired KcReactiveRepository kcReactiveRepository; public Uni&lt;Author&gt; save(String name) { Author entity = Author.builder() .name(name) .build(); return kcReactiveRepository.save(entity); } public Uni&lt;List&lt;Author&gt;&gt; listAll() { return kcReactiveRepository.listAll(Author.class); } public Uni&lt;Author&gt; findById(Long id) { return kcReactiveRepository.findById(Author.class, id); } public Uni&lt;Integer&gt; deleteAll() { return kcReactiveRepository.deleteAll(Author.class); } public Uni&lt;Integer&gt; deleteById(Long id) { return kcReactiveRepository.deleteById(Author.class, id); }}3. 테스트코드 작성이 모두 끝났습니다. 기능이 잘 동작하는지 테스트를 수행해 보겠습니다.@Testvoid fullTest() { var simpleAuthor = authorRepository .save(\"홍길동\") .await() .indefinitely(); IntStream.range(1, 11).forEach(idx -&gt; { var uni = postRepository.save(\"title\" + idx, \"contents\"+ idx, simpleAuthor); uni.await().indefinitely(); }); //////////////////////////////////////////////////////////////////////////////////////////////// var authorList = authorRepository.listAll().await().indefinitely(); var postList = postRepository.listAll().await().indefinitely(); Assert.isTrue(authorList.size() == 1, \"size of author list must be 1\"); Assert.isTrue(postList.size() == 10, \"size of post list must be 10\"); long authorId = authorList.stream().findFirst().get().getId(); long postId = postList.stream().findAny().get().getId(); var deleteNotExistAuthor = authorRepository.deleteById(authorId + 1).await().indefinitely(); var deleteExistPost = postRepository.deleteById(postId).await().indefinitely(); Assert.isTrue(deleteNotExistAuthor == 0, \"query result of delete empty author must be 0\"); Assert.isTrue(deleteExistPost == 1, \"query result of delete post must be 1\");} author과 post를 insert 합니다. 모든 author 과 post를 불러와 잘 insert 되었는지 확인합니다. 삭제를 통해 쿼리 수행 결과가 잘 넘어오는지 확인합니다.만약 webflux 에서 Mono, Flux 객체를 리턴해야 한다면return ServerResponse.ok().body(postRepository.listAll().convert().with(toMono()));위와같이 mutiny-reactor 패키지에 있는 toMono() 를 사용하여 Uni를 Mono, Flux로 변환하여 반환하면 되겠습니다.마무리기존의 JPA를 그리워 하면서 Webflux - Spring Data R2DBC 를 사용하고 계셨던 분들에게는 희소식이지 않을까 싶습니다. 또한 기존 JPA 스펙과 크게 다르지 않은 것으로 보아하니 기존의 JPA와 non-blocking 코드에 익숙하신 분들이라면 쉽게 익힐수 있을 것이라 생각합니다.저도 ‘Webflux는 쓸만한 orm 라이브러리 나오면 제대로 파야지~’ 라고 생각하곤 했었는데 이제 제대로 한번 공부해봐야 겠습니다.참조 https://hibernate.org/reactive/documentation/1.1/reference/html_single/ https://itnext.io/integrating-hibernate-reactive-with-spring-5427440607fe?gif=true" }, { "title": "Github로 자바 라이브러리 배포하기", "url": "/posts/github-personal-repo/", "categories": "Java", "tags": "Java, Github, Maven", "date": "2022-01-22 20:12:00 +0900", "snippet": "개요사이드 프로젝트를 진행하다보면 최초 세팅시 공통된 코드가 사용되는 경우가 많습니다. 하지만 이를 모듈화하여 개인 nexus에 배포하자니 서버 구축이 어렵고, Maven Central에 배포하자니 거쳐야하는 단계가 많아 어려움을 겪었었습니다.이 포스팅 에서는 Github를 이용해 개인 라이브러리를 배포하는 몇가지 방법에 대해 알아보겠습니다. 라이브러리 배포에 있어서는 Maven이 Gradle 보다 용이하다고 생각되어 아래 내용은 모두 Maven을 기반으로 작성되었습니다. (저도 개인적으로는 Gradle을 선호합니다…)아래 예제는 제가 쓰려고 만든 라이브러리 를 배포하는 과정을 담고 있습니다.1. Github Repository를 Maven Repository 처럼 사용하기첫번째 방법은 Github repository를 maven repository처럼 사용하는 방법입니다.1.1. Local &amp; Github Repository 만들기1.1.1. github에 public repository 생성하기빈 repository를 하나 생성합니다. 이름은 임의로 지정하였습니다.1.1.2. 방금 생성한 repository의 이름으로 폴더 생성하기mkdir maven-test-repo1.1.3. 폴더를 git repository로 만들고 github에서 생성한 repository와 연결cd maven-test-repogit initgit remote add origin https://github.com/keencho/maven-test-repo.git1.1.4. releases, snapshots 폴더 생성mkdir releases, snapshots1.2. Local에 내 라이브러리 배포하기 &amp; Github Repository에 배포하기1.2.1. 배포할 라이브러리의 pom.xml 수정하기 ... &lt;groupId&gt;com.keencho.lib&lt;/groupId&gt; &lt;artifactId&gt;keencho-p6spy&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt; ... &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;release&lt;/id&gt; &lt;url&gt;https://github.com/keencho/maven-test-repo/raw/master/releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshot&lt;/id&gt; &lt;url&gt;https://github.com/keencho/maven-test-repo/raw/master/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt;이때 groupId, artifactId, version 태그는 추후 어플리케이션에 의존성 추가할때 쓰임을 인지하고 이름을 정해주세요.1.2.2. altDeploymentRepository 명령어로 라이브러리 로컬에 배포하기altDeploymentRepository 명령어로 라이브러리를 로컬에 배포합니다.mvn -D altDeploymentRepository=snapshot::default::file:../maven-test-repo/snapshots clean deploy파일 구조는 다음과 같습니다. └── D: └── workspace ├── maven-test-repo │ ├── releases │ └── snapshots (배포될 디렉토리) └── keencho-lib-custom-p6spy (배포할 라이브러리)1.2.3. 배포 확인후 github 에 push 하기local 폴더에 정상적으로 배포되었다면 commit - push를 진행합니다.1.3. 라이브러리를 사용할 어플리케이션에 의존성 추가1.3.1. 라이브러리를 사용할 어플리케이션에 repository 연결정보를 추가&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;release&lt;/id&gt; &lt;url&gt;https://github.com/keencho/maven-test-repo/raw/master/releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshot&lt;/id&gt; &lt;url&gt;https://github.com/keencho/maven-test-repo/raw/master/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt;1.3.2. 의존성 추가&lt;dependency&gt; &lt;groupId&gt;com.keencho.lib&lt;/groupId&gt; &lt;artifactId&gt;keencho-p6spy&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt;&lt;/dependency&gt;위에서 정의한 groupId, artifactId, version 을 동일하게 입력합니다.1.3.3. 확인하기명령어로 maven install 후 내가 만든 라이브러리가 추가되어있는지 확인합니다.mvn install2. Github Packages 사용하기두번째 방법은 Github Packages 를 사용하는 방법입니다. 주의! 이 방법은 GITHUB_TOKEN 을 사용하는 방법입니다. 배포할때 뿐만 아니라 의존성을 추가할때도 토큰이 필요합니다.이에대해 github 개발자들도 인지하고 있고 21년 말까지 public access를 허용하겠다고 하였으나 22년 초 현재까지 소식이 없습니다.2.1. Github Packages 에 인증하기2.1.1. Personal Access Token 발급받기첫번째로 github token을 발급 받아야 합니다. 토큰 토큰 발급 과정은 다음과 같습니다.github -&gt; 우측 상단 프로필 -&gt; Settings -&gt; 좌측 사이드바 최하단 Developer settings -&gt; 좌측 사이드바 Personal access tokens -&gt; Generate new token임의의 이름을 입력하고 만료일을 선택한후, 스코프를 지정합니다. 원래대로라면 엄격하게 지정해야 하지만 테스트후 바로 삭제할 것이므로 모든 스코프에 체크한후 토큰을 생성합니다.2.1.2. 발급받은 토큰으로 settings.xml 세팅하기발급받은 토큰을 기반으로 settings.xml 파일을 세팅하면 GitHub Packages with Apache Maven에 인증할 수 있습니다.&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;github&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;github&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;https://repo1.maven.org/maven2&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;github&lt;/id&gt; &lt;url&gt;https://maven.pkg.github.com/keencho/lib-custom-p6spy&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;github&lt;/id&gt; &lt;username&gt;keencho&lt;/username&gt; &lt;password&gt;{TOKEN}&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;&lt;/settings&gt;2.2. 라이브러리 배포하기기본적으로 Github는 패키지와 동일한 이름의 기존 레포지토리에 패키지를 배포합니다. 예를들어 com.example:test 패키지는 OWNER/test 라는 리포지토리에 배포될 것입니다.2.2.1. pom.xml 수정하기배포할 라이브러리의 pom.xml을 수정합니다. OWNER 은 사용자 계정의 이름이며 REPOSITORY는 배포할 라이브러리의 패키지명 입니다. (여기서는 artifactId)&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;github&lt;/id&gt; &lt;name&gt;keencho&lt;/name&gt; &lt;url&gt;https://maven.pkg.github.com/keencho/lib-custom-p6spy&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt;2.2.2. 배포하기mvn deploy위 명령어로 배포합니다.위 캡쳐화면과 같이 github maven 저장소에 업로드 되었다는 로그를 확인할 수 있습니다.2.2.3. Github에서 확인하기Github UI 화면에서도 확인할 수 있습니다. 리포지토리의 우측 사이드바를 확인해보세요.2.3. 라이브러리 설치하기Github Packages 에 인증되어 있어야 라이브러리 설치가 가능합니다. 여기까지 진행하셨다면 당연히 인증되어 있겠지만 혹시 인증되어 있지 않다면 2.1. 항목의 과정들이 선행 되어야 합니다.2.3.1. 의존성 추가 후 설치하기방금 배포한 라이브러리의 의존성을 추가합니다.&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.keencho.lib&lt;/groupId&gt; &lt;artifactId&gt;keencho-p6spy&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;의존성을 추가하였다면 아래 명령어를 통해 설치합니다.mvn install설치가 완료되었다면 라이브러리가 추가되었는지 확인합니다.3. Jitpack으로 배포하기마지막 세번째 방법은 Jitpack을 활용해 배포하는 방법입니다. Jitpack을 사용하면 별도 설정 없이 (물론 Jitpack 에 빌드되어 배포는 되어 있어야 합니다.) Github 저장소의 주소를 원격 저장소로 사용할 수 있습니다.3.1. Jitpack 살펴보기3.1.1. 로그인하기Jitpack에 접속해 우측 상단 버튼을 클릭해 Github로 로그인합니다.3.1.2. 리포지토리들 확인하기좌측 사이드바에 내 리포지토리들이 잘 보이는지 확인합니다.3.2. Github Release3.2.1 JDK 세팅jitpack은 빌드시 jdk8을 사용합니다. 만약 라이브러리의 타겟 jdk가 11이라면 기본 설정으로는 빌드되지 않습니다.원하는 jdk로 빌드 되도록 하기 위해 jitpack.yml 파일을 프로젝트 루트폴더에 생성하고 다음과 같이 사용할 jdk를 명시합니다.jdk: openjdk113.2.2. code push릴리즈를 생성하기 전에, 작업 결과물을 commit &amp; push 합니다.3.2.3. releaseGithub release 기능을 이용하여 릴리즈를 생성 합니다.위 캡쳐화면의 플로우로 릴리즈 생성 페이지에 들어와 tag를 지정하고 publish relase 버튼을 눌러 배포합니다.여기서 tag는 추후 maven의 version이 되기 때문에 필수이며 title과 describe는 필수 입력 항목이 아닙니다.3.2.4. release 확인릴리즈가 완료되었음을 확인합니다.3.3. Jitpack 배포3.3.1. Look upJitpack 배포를 수행하기 위한 첫번째 단계로 Look up 을 진행해야 합니다. Jitpack에 접속후 Owner/Repository 의 양식으로 인풋 박스에 입력후 Look up 버튼을 클릭합니다.그럼 위와같은 화면이 보이실 텐데요, 저는 이미 배포가 완료된 상태라 Get it 버튼이 초록색이지만 아직 배포되지 않은 경우 버튼의 색상이 하얀색으로 보이게 됩니다.하얀색 Get it 버튼을 눌러 배포를 진행합니다.3.3.2 Jitpack 배포 확인Log 버튼을 눌러 배포가 완료되었는지 확인합니다. 프로세스가 끝나지 않은 경우 파일이 열리지 않으니 파일이 열리지 않는 경우 조금 기다려 보세요.파일이 열렸다면 스크롤을 최하단으로 내려 아래와 같이 BUILD SUCCESS 로그가 찍혀있는지 확인해 보세요.[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 6.712 s[INFO] Finished at: 2022-02-08T04:59:37Z[INFO] ------------------------------------------------------------------------Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8 -Dhttps.protocols=TLSv1.2Found module: com.keencho.lib:keencho-p6spy:1.0.2Build tool exit code: 0Looking for artifacts...Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8 -Dhttps.protocols=TLSv1.2Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8 -Dhttps.protocols=TLSv1.2Looking for pom.xml in build directory and ~/.m2Found artifact: com.keencho.lib:keencho-p6spy:1.0.2Exit code: 0✅ Build artifacts:com.github.keencho:lib-custom-p6spy:1.0.2Files:com/github/keencho/lib-custom-p6spy/1.0.2com/github/keencho/lib-custom-p6spy/1.0.2/build.logcom/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2-sources.jarcom/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.jarcom/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.jar.md5com/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.jar.sha1com/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.pomcom/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.pom.md5com/github/keencho/lib-custom-p6spy/1.0.2/lib-custom-p6spy-1.0.2.pom.sha13.4. 최종 확인3.4.1. 의존성 추가 후 설치하기Jitpack에 배포가 완료되었다면 이제 의존성 추가 후 사용하는 일만 남았습니다.pom.xml에 아래와 같이 태그를 추가해 주세요. jitpack.io를 리포지토리로 사용하겠다는 태그도 추가해 줘야 합니다.&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;...&lt;dependency&gt; &lt;groupId&gt;com.github.OWNER&lt;/groupId&gt; &lt;artifactId&gt;REPOSITORY&lt;/artifactId&gt; &lt;version&gt;TAG&lt;/version&gt;&lt;/dependency&gt;이때 OWNER은 github 계정 이름, REPOSITORY는 리포지토리명, TAG는 github relase 생성시 입력한 tag 입니다. 저의 경우는 다음과 같습니다.&lt;dependency&gt; &lt;groupId&gt;com.github.keencho&lt;/groupId&gt; &lt;artifactId&gt;lib-custom-p6spy&lt;/artifactId&gt; &lt;version&gt;1.0.2&lt;/version&gt;&lt;/dependency&gt;세팅이 모두 완료되었다면 mvn install 명령어를 통해 설치후 라이브러리가 추가되어 있는지 확인해 보세요.mvn install참조 https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-apache-maven-registry https://jitpack.io/docs/BUILDING/" }, { "title": "리액트에서 아임포트 컨트롤하기", "url": "/posts/react-iamport-component/", "categories": "React", "tags": "React", "date": "2022-01-08 20:12:00 +0900", "snippet": "개요제가 운영중인 서비스에서는 아임포트를 결제 모듈로 사용하고 있습니다. 현재 홈페이지 내부에서 결제할 수 있는 페이지는 1개가 아닙니다. 따라서 각각의 컨테이너 혹은 컴포넌트에서 그때그때 아임포트 모듈을 호출해야 합니다.아임포트를 쉽게 컨트롤하고 코드 재사용성을 향상시키기 위해 recoil을 이용하여 컴포넌트를 만들었습니다.recoil현재 프로젝트에 recoil 패키지가 포함되어 있다는 가정하에 진행됩니다. 혹시 recoil에 대해서 알지 못하거나 설치되어있지 않다면 여기 를 참고하여 recoil을 설치해보세요.코드 작성하기IamportDataAtom첫번째로 아임포트에서 사용할 Atom (상태의 단위) 을 작성하겠습니다.IamportModel.tsexport default interface IamportModel { fire: boolean, data: any, method: string, callBack: any customData: any, amount: number, mobileRedirectUrl?: string notNeedBuyerInfo?: boolean}iamport.atom.tsimport {atom} from 'recoil';import IamportModel from 'models/iamport.model';const IamportDataAtom = atom&lt;IamportModel&gt;({ key: 'iamportData', default: { fire: false, data: undefined, callBack: undefined, method: 'card', customData: {}, amount: 10000, mobileRedirectUrl: undefined }});export default IamportDataAtom;결제에 사용될 정보들을 담을 인터페이스와 그 인터페이스를 활용한 atom 객체 입니다. 결제는 pc, mobile로 나뉘고 결제자 정보가 필요없는 경우도 있기 때문에 mobileRedirectUrl 변수와 notNeedBuyerInfo 변수는 옵셔널로 두었습니다.Iamport아임포트 Element를 작성하겠습니다. 이 Element는 최상위 index.tsx에 포함됩니다.ReactDOM.render( &lt;React.StrictMode&gt; &lt;RecoilRoot&gt; &lt;App /&gt; &lt;Iamport /&gt; &lt;/RecoilRoot&gt; &lt;/React.StrictMode&gt;, document.getElementById('root'));const Iamport = (): JSX.Element =&gt; { const iamportData = useRecoilValue(IamportDataAtom); const setIamportData = useSetRecoilState(IamportDataAtom); const defaultBuyerTel = '01011112222' const defaultBuyerName = '홍길동' const defaultBuyerEmail = 'korea@korea.co.kr' const fire = async() =&gt; { setIamportData({ ...iamportData, fire: false } as IamportModel) const { IMP }: any = window; const impCode = 'xxx'; IMP.init(impCode); let data: any = { pg : 'html5_inicis', pay_method : iamportData.method.toLowerCase(), merchant_uid : 'service_' + new Date().getTime(), amount : iamportData.amount, custom_data : iamportData.customData, name : '결제 테스트', buyer_tel : '010xxxxxxxx', buyer_email : 'buyer@korea.com', buyer_name : '김철수', } if (iamportData.mobileRedirectUrl !== undefined &amp;&amp; iamportData.mobileRedirectUrl.length !== 0) { data['m_redirect_url'] = iamportData.mobileRedirectUrl; } if (iamportData.notNeedBuyerInfo === true) { data['buyer_tel'] = defaultBuyerTel; data['buyer_email'] = ''; data['buyer_name'] = defaultBuyerName; } IMP.request_pay(data, iamportData.callBack); } useEffect(() =&gt; { const jquery = document.createElement('script'); jquery.src = 'https://code.jquery.com/jquery-1.12.4.min.js'; const iamport = document.createElement('script'); iamport.src = 'https://cdn.iamport.kr/js/iamport.payment-1.1.7.js'; document.head.appendChild(jquery); document.head.appendChild(iamport); return () =&gt; { document.head.removeChild(jquery); document.head.removeChild(iamport); } }, []) useEffect(() =&gt; { if (iamportData.fire === true) { fire(); } }, [iamportData]) return ( &lt;&gt;&lt;/&gt; )}export default Iamport 아임포트는 jquery를 필요로 하기 때문에 마운팅시 코드를 동적으로 생성하여 jquery를 사용할 수 있게 추가합니다. 반대로 언마운트시에는 제거합니다. useEffect를 통해 IamportDataAtom 상태를 구독합니다. 만약 fire이 true가 되면, fire() 함수를 호출하여 아임포트 결제 모듈을 호출합니다. 미리 정의된 IamportDataAtom 값을 바탕으로 아임포트 결제 모듈을 호출합니다.Business Logic이제 위에 만들어둔 IamportDataAtom 에 데이터를 입력하고 fire을 true로 하여 useSetRecoilState를 통해 상태를 업데이트하기만 해주면 끝입니다....const setIamportData = useSetRecoilState(IamportDataAtom);...const callIamport = async (params: object, method: string) =&gt; { const res: any = await OrderService.encryptAndBuildData(params); const iamportData: IamportModel = { fire: true, data: null, customData: res.data, method: method, amount: 10000, notNeedBuyerInfo: true or false callBack: async (data: any) =&gt; { if (data.success === false) { alert('결제 실패'); return; } await orderPay(data.imp_uid, data.merchant_uid) } } if (isMobile) { iamportData.mobileRedirectUrl = window.location.href; } setIamportData(iamportData)}const orderPay = async(impUid: string, merchantUid: string) =&gt; { // 데이터를 활용한 결제처리}모바일웹 혹은 앱에서의 결제는 PC와 달리 앱 스킴을 통해 사용자가 결제할 앱을 직접 호출하여 결제하는 방식이기 때문에 아임포트는 결제가 완료되면 mobileRedirectUrl 에 미리 정해둔 url에 imp_success, imp_uid등 결제 정보를 쿼리스트링에 담아 호출하게 됩니다.따라서 최초 마운트시 현재 url의 쿼리스트링을 체크하는 함수가 필요합니다.const checkMobileCallback = async() =&gt; { const getParamByName: string | undefined = (name: string) =&gt; { const params: { [p: string]: string } = Object.fromEntries(new URLSearchParams(window.location.search).entries()); return params[name]; } const impSuccess = getParamByName('imp_success'); const errorMsg = getParamByName('error_msg'); const impUid = getParamByName('imp_uid'); const merchantUid = getParamByName('merchant_uid'); if (StringUtils.hasText(impSuccess) === false || StringUtils.hasText(impUid) === false || StringUtils.hasText(merchantUid) === false) { return; } await orderPay(data.imp_uid, data.merchant_uid)}...useEffect(() =&gt; { if (isMobile) { checkMobileCallback(); }})모바일 전용 콜백 페이지를 따로 만들어 모바일에서 행해지는 모든 결제는 그곳에서 처리할 수도 있지만 각기 다른 결제 상황에서 콜백 함수의 내용은 다를 수 있기 때문에 모바일의 경우 위와같이 처리하였습니다." }, { "title": "Java 17", "url": "/posts/java-17/", "categories": "Java", "tags": "Java", "date": "2021-12-18 20:12:00 +0900", "snippet": "Java 172021년 9월 14일 Java LTS 인 JDK 17 GA가 릴리즈 되었습니다. JDK 17은 최대 2029년 9월까지 업데이트가 제공될 예정인데요, 2018년 9월에 JDK 11이 릴리즈된 이후 3년만에 출시된 LTS 버전의 JDK 입니다.코드의 변화뿐만 아니라 LTS 출시 주기가 3년에서 2년으로 변경되는 등 많은 변화가 있었는데요, 이 글에서는 Java 17의 새로운 기능들에 대해 정리해 보겠습니다.JEP List1. Restore Always-Strict Floating-Point Semantics (JEP 306)이것은 과학적인 목적으로 수치에 민감한 프로그램을 위한 JEP 입니다. 이는 다시 기본 부동 소수점 연산을 strict 또는 strictfp로 설정하여 모든 플랫폼에서 부동 소수점 계산을 할 경우 동일한 결과를 얻을 수 있게 합니다. (일관성 보장) Java 1.2 이전에는 모든 부동소수점 계산이 strict하여 x87 기반 하드웨어에 과열을 일으켰습니다. Java 1.2 이후에는 과열을 방지하기 위해 부동 소수점 계산을 위해 strictfp 키워드를 필요로 합니다. 오늘날에는 Intel과 AMD 모두 과열 없이 엄격한 부동소수점 연산을 지원할 수 있는 SIMD Extensions 2 (Streaming SIMD Extensions 2) 확장을 지원하므로, x87 기반 하드웨어에서 문제가 발생하지 않습니다. 따라서 Java 17에서는 Java 1.2 이전 버전의 strict 한 부동소수점 연산 방식을 기본값으로 복원합니다. 이는 strictfp 키워드는 선택사항임을 의미합니다.2. Enhanced Pseudo-Random Number Generators (JEP 356)유사난수 알고리즘을 구현하거나 사용하기 쉽게 하기 위해 RandomGenerator 라 불리는 새로운 인터페이스를 제공합니다.이로 인해 서로 다른 알고리즘을 자유롭게 사용할 수 있습니다. 또한 스트림 기반 프로그래밍을 보다 효율적으로 지원합니다.package java.util.random;public final class RandomGeneratorFactory&lt;T extends RandomGenerator&gt; { // ... }RandomGenerator을 구현한 모든 알고리즘을 사용하여 1 ~ 100 사이의 정수들의 집합에서 랜덤한 5개의 숫자를 불러오는 코드입니다.public class Main { public static void main(String[] args) { var randomGeneratorList = RandomGeneratorFactory.all() .map(RandomGeneratorFactory::name) .collect(Collectors.toList()); randomGeneratorList.forEach(generator -&gt; { RandomGenerator rg = RandomGeneratorFactory.of(generator).create(999); System.out.println(rg.getClass()); int counter = 0; while (counter &lt;= 5) { int result = rg.nextInt(100); System.out.println(result); counter ++; } System.out.println(\"=======================\"); }); }}다음의 코드를 통해 모든 알고리즘을 확인할 수도 있습니다.RandomGeneratorFactory.all() .map(fac -&gt; fac.group()+ \" : \" +fac.name()) .sorted() .forEach(System.out::println);LXM : L128X1024MixRandomLXM : L128X128MixRandomLXM : L128X256MixRandomLXM : L32X64MixRandomLXM : L64X1024MixRandomLXM : L64X128MixRandomLXM : L64X128StarStarRandomLXM : L64X256MixRandomLegacy : RandomLegacy : SecureRandomLegacy : SplittableRandomXoroshiro : Xoroshiro128PlusPlusXoshiro : Xoshiro256PlusPlusjava.util.Random 패키지에 있는 SplitRandom 클래스나 SecureRandom 같은 클래스같은 레거시 클래스들은 JDK 17에서는 RandomGenerator 인터페이스를 확장하여 구현되었습니다.3. New macOS Rendering Pipeline (JEP 382)이 JEP는 애플이 스윙 GUI 프로그래밍에서 내부적으로 사용되는 OpenGL API를 deprecate 처리하였기 때문에 새롭게 macOS용 자바 2D 내부 렌더링 파이프라인을 구현합니다. 새로운 구현체는 애플 메탈 API를 사용하며 내부 엔진 외 기존 API에 대한 변경 사항은 없습니다.4. macOS/AArch64 Port (JEP 391)애플은 자사의 컴퓨터 라인을 x64에서 AArch64로 전환하는 장기 계획을 발표하였습니다. 이 JEP는 JDK가 macOS의 AArch64에서 실행되도록 코드를 포팅합니다.5. Deprecate the Applet API for Removal (JEP 398)이미 많은 웹 브라우저들이 자바 플러그인에 대한 지원을 중지하였습니다. API가 무의미해짐에 따라 JDK 9부터 deprecate 처리되었고, Applet API는 향후 릴리즈 버전에서 제거될 예정입니다.@Deprecated(since = \"9\", forRemoval = true)@SuppressWarnings(\"removal\")public class Applet extends Panel { // ...}6. Strongly Encapsulate JDK Internals (JEP 403)많은 써드파티 라이브러리, 프레임워크 혹은 도구들이 JDK 내부의 API나 패키지로 접근하고 있습니다. Java 16은 JEP 396를 통해 기본적으로 강력한 캡슐화를 기본전략으로 설정합니다. 하지만 단순 캡슐화로의 전환을 지원하기 위해 --illegal-access 옵션을 제공했었습니다.JEP 403은 --illegal-access옵션을 제거하였습니다. 이제 JDK의 강력한 내부 캡슐화로 한걸음 더 다가갔습니다.추가로 reflection 을 통한 private field / method 접근이 불가능하게 됩니다.entityField.setAccessible(true);Object object = entityField.get(this.path);7. Pattern Matching for Switch (Preview / JEP 406)이 JEP는 switch 문과 표현식을 위한 패턴 매칭을 추가하였습니다. Preview 버전이기 때문에 --enable-preview 옵션을 사용하여 기능을 활성화 시켜야 합니다.7-1. if else chainJDK 16에서 instanceof 에 대한 패턴 매칭이 추가되었는데, 이를 통해 instanceof 관용구를 단순화할 수 있었습니다.private static String stringFormatter(Object obj) { String result = null; if (obj instanceof Integer i) { result = String.format(\"int %d\", i); } else if (obj instanceof Long l) { result = String.format(\"long %d\", l); } else if (obj instanceof String s) { result = String.format(\"String %s\", s); } return result;}JDK 17에서는 위의 코드를 switch 문을 이용한 코드로 변환할 수 있습니다.private static String stringFormatter(Object obj) { return switch (obj) { case Integer i -&gt; String.format(\"int %d\", i); case Long l -&gt; String.format(\"long %d\", l); case String s -&gt; String.format(\"string %s\", s); default -&gt; throw new IllegalStateException(\"Unexpected value: \" + obj); };}7-2. null check in switch이제 switch 문안에서 null 체크를 할 수 있습니다. 더이상 switch 이전에 null 체크를 하지 않아도 됩니다.private static void stringTester (String s) { // this is not necessary // if (s == null) { // System.out.println(\"string is null\"); // return; // } switch (s) { case null -&gt; System.out.println(\"string is null\"); case \"Java 11\", \"Java 17\" -&gt; System.out.println(\"java LTS\"); case \"Java 8\" -&gt; System.out.println(s); default -&gt; System.out.println(\"nothing matched\"); }}7-3 refined pattern in switch아래 코드를 살펴보면, Car 객체의 바퀴수를 테스트하기 위해선 switch문 안에서 if 조건절을 사용해야만 했습니다.abstract static class Transportation { }static class Car extends Transportation { int wheel; int countWheel() { return this.wheel; } public Car(int wheel) { this.wheel = wheel; }}private static void testTransportation (Transportation transportation) { switch (transportation) { case null: System.out.println(\"tp is null\"); break; case Car car: if (car.countWheel() == 4) { System.out.println(\"car with four wheel\"); } else { System.out.println(\"car\"); } break; default: System.out.println(\"Unknown\"); break; }}Java 17 은 소위 redefined pattern 혹은 guarded pattern 이라 불리는 패턴을 허용합니다.private static void testTransportation (Transportation transportation) { switch (transportation) { case null -&gt; System.out.println(\"tp is null\"); case Car car &amp;&amp; car.countWheel() == 4 -&gt; System.out.println(\"car with four wheel\"); case Car car -&gt; System.out.println(\"car\"); default -&gt; System.out.println(\"Unknown\"); }}8. Remove RMI Activation (JEP 407)RMI Activation과 java.rmi.activation 패키지를 제거하였습니다.9. Sealed Classes (JEP 409)Java 15 와 16 에서 preview 기능으로 소개된 봉인 클래스가 표준 기능으로 확정되었습니다. Java 16에서의 변경점은 없습니다.이 기능은 봉인된 구성 요소를 확장하거나 구현할 수 있는 다른 클래스 혹은 인터페이스를 제한합니다. JEP 406 (패턴 매칭)과 결합하면 다음과 같이 깔끔하게 타입, 캐스트 패턴을 통해 검사할수 있습니다.int getWheel(Transportation transportation) { return switch (transportation) { case Car c -&gt; c.countWheel(); case MotorCycle m -&gt; m.countWheel(); case Subway s -&gt; s.countWheel(); case Airplane a -&gt; a.countWheel(); };}참조10. Remove the Experimental AOT and JIT Compiler (JEP 410)AOT와 JIT는 JDK 9 와 JDK 10 에 각각 도입된 기능입니다. 이들은 유지 보수 비용이 많이 드는 기능이었습니다. 하지만 그리 많이 사용되지 않는 기능이기 때문에 JEP는 이 기능들을 삭제하기로 하였습니다.그와 별개로 개발자는 여전히 GraalVM을 사용하여 이 기능들을 활용할 수 있습니다.11. Deprecate the Security Manager for Removal (JEP 411)Security Manager은 Java 1.0 에서 클라이언트측 Java 코드를 보호하기 위해 소개된 기능입니다만 더이상 사용되지 않습니다. 추후 릴리즈 버전에서 제거하기 위해 deprecate 처리합니다.@Deprecated(since=\"17\", forRemoval=true)public class SecurityManager { // ...}12. Foreign Function and Memory API (Incubator) (JEP 412)자바 개발자들은 외부 함수 및 메모리 API를 통해 JVM 외부에서 코드에 접근하고 힙 외부의 메모리를 관리할 수 있게 되었습니다. 이 API의 목표는 JNI API를 대체하고 보안과 성능을 향상시키는 것입니다.13. Vector API (Second Incubator) (JEP 414)Java 16에서 JEP 414는 새로운 Vector API를 Incubating API로 소개하였습니다.이 JEP는 Vector API 의 성능 및 문자 지원 작업 등의 향상된 기능을 제공합니다.14. Context-Specific Deserialization Filters (JEP 415)Java 9에 처음 도입된 JEP 290 기능은 많은 보안 문제의 공통 원인은 신뢰할 수 없는 코드에서 들어오는 직렬 데이터를 검증할 수 있게 해주었습니다. 이러한 검증은 JVM 레벨에서 수행되기 때문에 더 높은 보안성과 견고성이 요구됩니다.JEP 415를 사용하면 애플리케이션은 JVM 수준에서 정의된 컨텍스트별 및 동적으로 선택된 역직렬화 필터를 구성할 수 있습니다. 각 역직렬화 작업은 이러한 필터를 호출합니다." }, { "title": "OSIV(Open-Session-In-View)", "url": "/posts/osiv/", "categories": "JPA", "tags": "Spring, JPA", "date": "2021-12-04 17:12:00 +0900", "snippet": "OSIVOSIV(Open-Session-In-View)는 영속성 컨텍스트를 뷰 영역까지 열어두는 기능입니다. 여기서 뷰 영역은 Controller 혹은 템플릿 렌더링 영역까지를 의미합니다.뷰 영역까지 영속성 컨텍스트가 살아있다면 뷰 영역에서도 지연 로딩 (Lazy Loading) 을 사용할 수 있습니다. 별다른 설정 없이도 지금까지 뷰 영역에서 지연 로딩을 사용하고 계시는 분들도 계실텐데요. 이유는 Spring Boot 2.0 부터는 해당 옵션이 기본적으로 사용되도록 설정되어 있기 때문입니다.WARN 6052 --- [ main] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning아무 설정도 하지 않고 어플리케이션을 실행시키면 위와 같은 경고 메시지를 볼 수 있는데요, 이는 OSIV가 기본적으로 사용되도록 설정되어 있다는 것을 의미합니다.OSIV Flow DiagramOSIV가 사용되도록 설정되어 있으며 Post 라는 엔티티가 있다고 가정해 보겠습니다. 아래는 Post엔티티를 가져오기 위한 처리 과정과 이를 도식화한 다이어그램 입니다. OpenSessionInViewFilter는 SessionFactory의 openSession메소드를 호출하여 새 세션을 시작합니다. Session은 TransactionSynchronizationManager에 바인딩 됩니다. OpenSessionInViewFilter는 javax.servlet.FilterChain의 doFilter를 호출하고 요청이 추가됩니다. DispatcherServlet에 의해 요청은 Controller로 전달됩니다. Controller는 Post 엔티티를 가져오기 위해 Service의 getPosts() 메소드를 호출합니다. Service는 새로운 트랜잭션을 열고, HibernateTransactionManager는 OpenSessionInViewFilter에 의해 시작된 Session 객체를 재사용합니다. Repository에서는 지연 로딩 없이 Post 엔티티를 가져옵니다. Service는 트랜잭션을 커밋하지만, Session 객체는 외부에서 초기화되었기 때문에 닫히지 않습니다. DispatcherServlet에 의해 뷰 UI가 렌더링되기 시작하면 지연 로딩될 객체를 탐색하고 초기화합니다. 뷰 렌더링까지 모두 끝났습니다. 이제 OpenSessionViewFilter는 Session을 닫을 수 있고, 데이터베이스 커넥션 또한 반환됩니다.실제로 OSIV가 설정되어 있으면 위와 같은 흐름으로 뷰 영역까지 영속성 컨텍스트가 살아있게 됩니다.장점 뷰 영역까지 영속성 컨텍스트가 유지됩니다. (DB 커넥션이 유지됨을 의미하기도 합니다.) 지연 로딩(Lazy Loading)을 적극적으로 활용할 수 있습니다.단점언뜻 보기에는 좋은 기능이라고 생각될수도 있겠지만, 데이터베이스 관점에서 보면 절대 좋은 기능이라고 할 수는 없습니다.서비스 레이어에서 트랜잭션을 열고 닫지만 이후에(뷰 렌더링시)는 트랜잭션이 열려있지 않은 상태입니다. 따라서 뷰 영역에서 생성된 모든 쿼리문은 auto-commit 모드로 실행됩니다. 이는 N개의 쿼리문은 N번 디스크로 트랜잭션 로그을 플러쉬 해야함을 의미합니다. 그러므로 데이터베이스의 입장에서는 많은 I/O 트래픽을 발생시키게 됩니다.또한 데이터베이스 연결은 뷰 영역까지 유지되므로 데이터베이스 커넥션 풀의 한계까지 도달하면 어플리케이션 전체 트랜잭션 처리가 제한됩니다. 연결이 많이, 길게 유지된다는 것은 다음 요청이 받아 먹어야 할 커넥션 풀의 수가 줄어든다는 것은 요청 - 응답까지의 시간이 지연됨을 의미하므로 트래픽으로 많아질수록 성능이 하락합니다.결론개인적으로 장점보다 단점이 더 크게 다가오는 기능인것 같습니다. 저도 몰랐을때는 아무것도 건드리지 않고 그냥 사용했었지만 OSIV 를 알고난 후에는 꼭 옵션을 끄고 개발하는 편입니다.물론 트래픽이 많지 않을것이라 예상되는곳 (사내 어플리케이션, 관리자 등..) 에는 그냥 켜고 사용합니다.미자막으로 spring boot 2.0 부터는 해당 옵션이 기본으로 설정되어 있습니다. 해당 옵션을 끄시려면 설정 파일에 다음과 같이 설정하시면 됩니다.spring.jpa.open-in-view=false" }, { "title": "JPA 캐시", "url": "/posts/JPA-cache/", "categories": "JPA", "tags": "Spring, JPA", "date": "2021-11-27 17:12:00 +0900", "snippet": "JPA 캐시데이터베이스에 접근하는 비용은 매우 비쌉니다. 실제 운영하는 서비스라면 DB가 같은 시스템 안에 존재하는 일이 없기 때문에 네트워크 비용까지 발생합니다. 결과값을 뻔히 알 수 있는 쿼리에 대해 이런 값비싼 작업을 수행한다면 어플리케이션의 성능은 낮아질 것입니다.하이버네이트를 포함한 JPA 구현체들은 캐시를 지원합니다. 하이버네이트의 캐시는 어플리케이션과 데이터베이스 사이에 중간다리 계층 역할을 하며, 데이터베이스를 직접 호출하는 대신 서버 메모리에서 결과값을 반환하므로 데이터를 얻는데 걸리는 시간을 줄여줄뿐 아니라 어플리케이션의 성능까지 높일 수 있습니다.1차 캐시1차캐시부터 살펴보겠습니다. 1차캐시는 영속성 컨텍스트 내부에 엔티티를 보관하는 장소를 의미합니다. 1차캐시는 하이버네이트에서 기본적으로 사용되도록 설정되어 있습니다. 1차캐시는 트랜잭션을 시작하고 종료할 때까지만 존재합니다.플로우를 간단히 도식화해보면 위와 같습니다. 요청이 오면 1차 캐시에서 요청과 일치하는 엔티티가 있는지 확인합니다. 만약 있다면 그대로 해당 엔티티를 반환하고 없다면 db에서 실제로 조회 후에 1차 캐시에 저장 후 해당 결과값을 반환합니다.예제간단한 예제를 통해 1차 캐시가 잘 동작하는지 확인해보겠습니다.테스트에 활용된 설정 정보는 다음과 같습니다.build.gradledependencies { implementation 'org.springframework.boot:spring-boot-starter-data-jpa' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'com.github.gavlyukovskiy:p6spy-spring-boot-starter:1.7.1' compileOnly 'org.projectlombok:lombok' runtimeOnly 'com.h2database:h2' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test'}application.ymlspring: datasource: driver-class-name: org.h2.Driver url: jdbc:h2:../test username: sa password: jpa: database-platform: org.hibernate.dialect.H2Dialect hibernate: ddl-auto: create-drop코드는 다음과 같습니다.TestModel.java@Entity@Data@NoArgsConstructor@Table(name = \"test_model\")public class TestModel { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String keyword;}TestService.java@Servicepublic class TestService { @Autowired TestRepository testRepository; @Transactional public void insertDummyData() { var r = Arrays.asList(\"banana\", \"monkey\", \"apple\", \"bear\"); Random rand = new Random(); for (var a = 0; a &lt; 10; a ++) { TestModel testModel = new TestModel(); testModel.setKeyword(r.get(rand.nextInt(r.size()))); testRepository.save(testModel); } }}Test.java@Test@Transactionalvoid test() { testService.insertDummyData(); var listAll = testRepo.findAll(); Assert.notNull(listAll, \"ENTITY LIST SHOULD NOT BE NULL\"); for (var i = 1L; i &lt;= 10L; i ++) { var entity = testRepo.findById(i).orElse(null); Assert.notNull(entity, \"ENTITY SHOULD NOT BE NULL\"); } var entity = testRepo.findById(12L).orElse(null); Assert.isNull(entity, \"ENTITY SHOULD BE NULL\");}10개의 엔티티를 저장하고 testRepo.findAll() 로 모든 엔티티를 불러와 1차캐시에 저장하였습니다. 만약 1차 캐시가 없다면 반복문 부분에서 1L부터 10L까지 10번 조회하는 쿼리가 날라갈 것입니다.하지만 위와 같이 select * from test_model절 이후에 반복문 내부의 findById() 로 생성되고 실행되어야 할 쿼리는 생성되지 않고 바로 testRepo.findById(12L).orElse(null)의 쿼리인 select * from test_model where id=12절이 실행된것을 확인할 수 있습니다.이로써 findAll() 로 조회된 id 1~10까지의 엔티티는 1차캐시에서 바로 반환되었고 1차캐시에 존재하지 않는 id 12의 경우 쿼리가 생성되어 실제 db에서 조회하여 결과값을 반환한 것을 확인할 수 있습니다.2차 캐시1차 캐시가 트랜잭션 단위의 캐시라면 2차 캐시는 어플리케이션 단위의 캐시입니다. 따라서 어플리케이션을 시작하고 종료할 때까지 캐시가 유지됩니다. 2차 캐시는 다음과 같은 특징을 가지고 있습니다. 동시성을 극대화하기 위해 엔티티를 직접 반환하지 않고 복사본(dto)을 만들어 반환한다. 2차 캐시는 영속성 유닛 범위의 캐시다. 2차 캐시는 데이터베이스의 기본키를 기준으로 캐싱하지만 영속성 컨텍스트가 다르면 객체 동일성 (a == b)를 보장하지 않는다.2차 캐시의 플로우를 도식화 하였습니다. 1차 캐시와 DB 사이에 2차 캐시 레이어가 추가되었습니다. 1차, 2차 캐시에 모두 원하는 요청값이 없는 경우 DB에서 조회후 2차캐시, 1차캐시에 순차적으로 저장 후 결과값을 반환합니다.Region Factory하이버네이트 2차 캐시는 실제로 어떤 캐시 프로바이더가 사용되는지 알 수 없게 설계되었습니다. 하이버네이트는 org.hibernate.cache.spi.RegionFactory 인터페이스를 구현한 구현체만을 필요로 합니다.위 인터페이스는 실제 캐시 프로바이더와 관련된 모든 정보를 캡슐화합니다. 또한 기본적으로 하이버네이트와 캐시 프로바이더 사이의 브릿지 역할을 합니다.아래 예제에서는 Ehcache를 사용합니다. RegionFactory 인터페이스가 구현되어있는 어떤 구현체를 사용해도 문제 없습니다.예제2차 캐시를 실제로 적용해보겠습니다.일단 gradle 디펜던시에 ehcache를 추가합니다.implementation 'org.hibernate:hibernate-ehcache:5.6.3.Final'그 후 application.yml 파일에 프로퍼티를 추가합니다.spring: jpa: properties: javax: persistence: sharedCache: mode: ALL hibernate: cache: use_second_level_cache: true region: factory_class: org.hibernate.cache.ehcache.EhCacheRegionFactoryuse_second_level_cache 프로퍼티를 통해 2차캐시를 활성화하고 factory_class 프로퍼티를 통해 EhCacheRegionFactory를 2차 캐시 프로바이더로 선택합니다. sharedCache.mode 프로퍼티를 통해 캐싱할 엔티티의 범위를 지정할 수 있는데 선택 가능한 모드는 다음과 같습니다. ALL: 모든 엔티티 및 엔티티 관련 상태와 데이터가 캐시됩니다.DISABLE_SELECTIVE: @Cacheable 어노테이션에 false 라고 명시된 엔티티를 제외한 모든 엔티티에 대해 캐싱을 사용할 수 있습니다.ENABLE_SELECTIVE: @Cacheable 어노테이션에 true 라고 명시된 모든 엔티티에 대해 캐싱을 사용할 수 있습니다.NONE: 영속성 유닛에서 캐싱을 사용할 수 없습니다.UNSPECIFIED: 캐싱 동작이 정의되지 않았습니다. 프로바이더별 기본값이 적용될 수 있습니다.더 자세한 정보는 이곳을 참조하세요.다음으로 캐싱할 엔티티에 캐시 기능을 사용하겠다고 어노테이션을 추가해줘야 합니다.@Entity@Data@NoArgsConstructor@Table(name = \"test_model\")@Cacheable@org.hibernate.annotations.Cache(usage = CacheConcurrencyStrategy.NONSTRICT_READ_WRITE)public class TestModel { @Id @GeneratedValue(strategy = GenerationType.AUTO) Long id; String keyword;}@Cacheable 어노테이션으로 이 엔티티가 캐싱될수 있음을 명시하였습니다. @Cache 어노테이션와 usage() 속성을 통해 캐시 전략을 명시하였습니다. 사용할수 있는 전략은 다음과 같습니다. NONE: 전략이 없습니다.READ_ONLY: 읽기 전용입니다. 불변값에 적합합니다. NONSTRICT_READ_WRITE: 엄격하지 않은 읽기/쓰기 전략입니다. 동시성 접근을 고려하지 않고 캐싱합니다. 수정이 자주 일어나지 않는 엔티티에 적합합니다.READ_WRITE: 엄격한 읽기/쓰기 전략입니다. 동시 접근을 고려하고 개발해야 합니다.TRANSACTIONAL: JTA 환경에서 사용되며 hibernate.transaction.manager_lookup_class 프로퍼티를 명시해야 합니다.이곳 에서 더 자세한 정보를 확인해보세요.설정이 모두 끝났습니다. 테스트에 사용될 코드는 다음과 같습니다.@RestControllerpublic class CacheTestController { @Autowired TestRepository testRepository; @Autowired TestService testService; @Autowired SessionFactory sessionFactory; @GetMapping(\"/insert\") public ResponseEntity&lt;?&gt; insert() { var r = Arrays.asList(\"banana\", \"monkey\", \"apple\", \"bear\"); Random rand = new Random(); for (var a = 0; a &lt; 10; a ++) { TestModel testModel = new TestModel(); testModel.setKeyword(r.get(rand.nextInt(r.size()))); testRepository.save(testModel); } return ResponseEntity.ok(\"SUCCESS\"); } @GetMapping(\"/getById\") public ResponseEntity&lt;?&gt; getById(@RequestParam Long id) { sessionFactory.getStatistics().setStatisticsEnabled(true); System.out.println(\"=========================\"); System.out.println(\"find entity start\"); var e = testRepository.findById(id).orElse(null); System.out.println(\"second level cache hit count: \" + sessionFactory.getStatistics().getSecondLevelCacheHitCount()); System.out.println(\"second level cache miss count: \" + sessionFactory.getStatistics().getSecondLevelCacheMissCount()); System.out.println(\"second level cache put count: \" + sessionFactory.getStatistics().getSecondLevelCachePutCount()); System.out.println(\"find entity end\"); return ResponseEntity.ok(e); }}편의를 위해 h2 메모리 db를 사용하였습니다. 테스트 시나리오는 어플리케이션 실행후 insert() 메소드를 통해 10개의 엔티티를 저장합니다. 그 후 getById(id) 메소드를 통해 id가 1인 엔티티를 두번 조회후 id가 2인 엔티티를 두번 조회해 보겠습니다.10:11:42.982 [INFO ][p6spy.logSQL:line60] - =========================find entity start10:12:03.542 [INFO ][p6spy.logSQL:line60] - select testmodel0_.id as id1_0_0_, testmodel0_.keyword as keyword2_0_0_ from test_model testmodel0_ where testmodel0_.id=1\tConnection ID: 4\tExecution Time: 0 ms\tCall Stack (number 1 is entry point): \t\t1. sycho.spring.controller.CacheTestController.getById(CacheTestController.java:50)----------------------------------------------------------------------------------------------------10:12:03.560 [INFO ][p6spy.logSQL:line60] - second level cache hit count: 0second level cache miss count: 1second level cache put count: 1find entity end=========================find entity start10:12:05.356 [INFO ][p6spy.logSQL:line60] - second level cache hit count: 1second level cache miss count: 1second level cache put count: 1find entity end=========================find entity start10:12:06.855 [INFO ][p6spy.logSQL:line60] - select testmodel0_.id as id1_0_0_, testmodel0_.keyword as keyword2_0_0_ from test_model testmodel0_ where testmodel0_.id=2\tConnection ID: 6\tExecution Time: 0 ms\tCall Stack (number 1 is entry point): \t\t1. sycho.spring.controller.CacheTestController.getById(CacheTestController.java:50)----------------------------------------------------------------------------------------------------10:12:06.858 [INFO ][p6spy.logSQL:line60] - second level cache hit count: 1second level cache miss count: 2second level cache put count: 2find entity end=========================find entity start10:12:07.940 [INFO ][p6spy.logSQL:line60] - second level cache hit count: 2second level cache miss count: 2second level cache put count: 2find entity end테스트 결과는 위와 같습니다. 최초 id가 1인 엔티티를 조회할때는 2차 캐시에 데이터가 존재하지 않기 때문에 실제 쿼리가 날라가고 miss count와 put count 가 1이 되었습니다. 그 후 다시한번 id가 1인 엔티티를 조회할 때는 2차 캐시에 엔티티가 존재하기 때문에 쿼리가 날라가지 않고 hit count가 1이 되었습니다.id를 2로하여 조회해봐도 1로 조회할때와 마찬가지로 최초 조회때는 실제 쿼리가 날라가고 2번째 조회때는 캐시된 엔티티를 반환하는 것을 확인할 수 있습니다.분산환경에서의 2차 캐시하지만 요즘에는 단일환경에서 어플리케이션을 잘 운영하지 않습니다. AWS를 사용한다고 가정하면 오토 스케일링 그룹의 EC2에 동일한 어플리케이션을 구동하여 분산환경에서 자유롭게 스케일링 in / out 할 수 있는 형태로 서버를 구성합니다.메모리 db가 어플리케이션 단위로 동작하는 경우 최종 db에 커밋시에 문제가 생길수 있습니다. db lock 에 의해 문제가 생기지 않을 수 있지만 뒤에 처리하는 비즈니스 로직에서 문제가 발생할 수 있습니다. 따라서 기본적으로 별도의 서버에 메모리 db 을 구축하여 사용할 수 있습니다. 물론 이때도 올바른 캐시 전략을 지정하는것은 중요합니다.단일 노드 환경에서는 READ_WRITE, TRANSACTIONL 전략을 사용할 수 있습니다. 그러나 분산 노드 환경에서는 db lock 에 의존할 수 없으므로 NONSTRICT_READ_WRITE 전략을 사용해야 합니다. 어떤 곳에서는 일관성을 위해 (quorum 알고리즘)[https://en.wikipedia.org/wiki/Quorum_(distributed_computing)] 을 사용하는 메모리 db 에서는 READ_WRITE 을 사용해도 된다고 하네요.개인적으로 외부로 메모리 db를 빼고 _WRITE 전략을 사용하는것은 세세한 설정이 필요하고 유지보수에 더 큰 리소스가 들어갈 수 있기 때문에 선호하지 않는 편입니다. 그렇다 하더라도 2차캐시 자체는 충분히 매력적인 기능이라 생각합니다." }, { "title": "AWS ALB에 람다 없이 고정 IP로 접근하기", "url": "/posts/aws-alb-static-ip/", "categories": "AWS", "tags": "AWS", "date": "2021-10-07 20:12:00 +0900", "snippet": "ALB에 고정 IP로 접근하기AWS ALB(Application Load Balancer)는 NLB(Network Load Balancer)에 비해 가지는 이점들이 있습니다. 예를 들어 ALB는 서브도메인이나 path ‘/app/**’ 같은 경로로 타겟 그룹에 요청을 전달할 수 있습니다. 하지만 단점 하나가 있는데요, 바로 고정 IP를 부여하지 못한다는 것입니다. 서비스를 운영하다 보면 고객사가 내부 망을 사용하여 그들의 방화벽에 고정 IP를 등록해야 하기 때문에 고정 IP를 요구하는 경우가 있습니다.기존에는 람다, AWS GA 등을 사용하여 ALB에 고정 IP로 접근하곤 하였습니다. 하지만 2021년 9월 AWS에서 NLB ALB 로의 직접 트래픽 전달을 지원하게 됨으로써 더 쉽게 고정IP를 부여할수 있게 되었습니다. 이 포스팅에서는 새로운 방법을 소개하도록 하겠습니다.기존 방법기존에 ALB에 고정 IP를 부여하는 방법으로는 람다, AWS Global Accelerator, Proxy EC2 Server 크게 3가지가 있는것 같습니다. 그중에서 제가 맡고있는 서비스에는 EC2에 nginx 만 설치해 Proxy Server 처럼 사용하는 방식을 적용하였습니다. 그리고 그 서버에는 아래와 같은 nginx config 파일을 작성 하였습니다.server { listen 443 ssl; server_name server_name; ssl_certificate /etc/nginx/ssl/ssl.crt; ssl_certificate_key /etc/nginx/ssl/ssl.key; ssl_protocols SSLv2 SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / { access_log /var/log/nginx/proxy.log timed_combined; error_log /var/log/nginx/proxy-error.log; resolver 172.31.0.2 valid=10s; set $proxy \"DNS NAME\"; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-FORWARDED-FOR $remote_addr; proxy_redirect off; proxy_read_timeout 120s; proxy_pass https://$proxy; }}아시다시피 ALB는 동적 IP이기 때문에 도메인을 주면 안되고 DNS 주소 자체를 proxy_pass 가 바라볼수 있도록 세팅해야 합니다. 만약 www.xxx.com 형태의 도메인을 proxy_pass가 바라보게 설정한다면 ALB의 동적IP가 바뀌는 경우 nginx 캐시에 의해 올바른 엔드포인트를 찾아 들어가지 못해 클라이언트는 504 에러코드를 수신하게 되는 장애 상황이 발생하게 됩니다.이것뿐만이 아니고 이 Proxy Server 에서 들어오는 요청을 받는 웹서버 입장에서는 검증되지 않은 요청이기 때문에 받는 웹서버 nginx의 server 블록과 location 블록에 아래와 같은 설정을 추가하기도 했습니다.add_header 'Access-Control-Allow-Origin' 'domain';add_header 'Access-Control-Allow-Credentials' 'true';add_header 'Access-Control-Allow-Headers' 'Authorization,Accept,Origin,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Content-Range,Range';add_header 'Access-Control-Allow-Methods' 'GET,POST,OPTIONS,PUT,DELETE,PATCH';...if ($request_method = 'OPTIONS') { add_header 'Access-Control-Allow-Origin' 'domain'; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Headers' 'Authorization,Accept,Origin,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Content-Range,Range'; add_header 'Access-Control-Allow-Methods' 'GET,POST,OPTIONS,PUT,DELETE,PATCH'; add_header 'Access-Control-Max-Age' 1728000; add_header 'Content-Type' 'text/plain charset=UTF-8'; add_header 'Content-Length' 0; return 204;}이렇듯 고정 IP 설정을 위해 예외처리하는 설정이 많았고, 한번에 관리하기 힘든 문제, SSL 변경시 각기 적용 문제등 많은 문제가 있었지만 별다른 방법이 없었기 때문에 눈물을 머금고 이 상태로 서비스를 제공하고 있는 상태였습니다.NLB 에서 ALB 바로 바라보게 하기 (새로운 방법)그러던 와중에 2021년 9월 27일에 AWS 사이트에 NLB에서 ALB로의 직접 트래픽 전달을 지원한다는 글이 올라왔습니다. (링크) 설정하기가 쉽고 여러가지 이점이 있기 때문에 때문에 Proxy Server를 없애고 NLB가 ALB를 바라볼수 있게끔 인프라 구성을 변경하기로 결정하였습니다.로드밸런서로 ALB를 사용하고 있다는 전제하에 새로운 방식을 소개해 드리도록 하겠습니다. 만약 사용하고 계시지 않다면 여기를 참고하셔서 ALB를 만들어 보세요.타겟그룹 생성하기첫번째로 타겟그룹을 생성하겠습니다. AWS 콘솔에 접속하셔서 Create target group을 눌러주세요.타겟그룹 생성화면에 들어왔다면 타겟 타입은 Application Load Balancer 를 선택하고 나머지 정보를 채워주세요. Health check 의 경우 기존 ALB가 바라보고 있는 대상 그룹의 health check 경로를 적어주시면 됩니다.다음단계로 넘어간다음 기존 ALB를 선택하고 Create target group 버튼을 클릭한다면 타겟그룹 생성은 끝입니다. Network Load Balancer 생성하기다음은 NLB를 만들어보겠습니다. EC2 콘솔을 통해 로드밸런서 페이지에 들어오신후 Load Balancer 생성 버튼과 Network Load Balancer 버튼을 선택해 생성화면까지 들어와 주세요.이름을 입력하고 vpc를 선택한 후 가용영역을 선택합니다. 이때 AWS가 자동으로 할당하는 IP가 아닌 탄력적 IP(Elastic IP)를 사용하고 싶으시다면 탄력적 IP를 먼저 할당 받고 NLB를 생성해야 합니다.그 후 아까 만든 타겟 그룹을 Foward to 에서 선택하고 하단의 Create load balancer를 눌러 NLB를 생성합니다.443 포트 설정SSL을 적용하지 않는다면 위 내용까지 설정하고 IP에 도메인을 연결하면 끝입니다. 하지만 요즘에는 SSL을 적용하지 않은 웹사이트는 보기 어렵죠. 다행히 설정자체는 어렵지 않습니다. 아래 이어질 내용에는 ALB 에 SSL 인증서가 적용되어 있다는 가정이 들어가 있습니다.타겟그룹을 하나더 만들겠습니다. 타겟그룹은 아쉽게도 포트 수정을 지원하지 않아 새롭게 만들어야 합니다. 이름은 적절한 이름으로 지어주시고 아까 80포트를 입력한 대신 이번에는 443 포트를 입력해 생성합니다.그 후 로드밸런서 설정 화면으로 돌아와 방금 만든 로드밸런서를 선택하고 리스너 항목을 선택합니다. 리스너 추가를 누르고 프로토콜 / 포트는 TCP / 443 을 지정하고 전달 대상은 바로 위에서 적절한 이름으로 만든 타겟그룹을 선택하고 리스너를 추가합니다.이제 모두 끝입니다. 로드밸런서가 새로운 설정을 반영할 잠시의 시간이 흐르고 도메인으로 접속해보면 https 프로토콜을 통해 연결되고 SSL 인증서도 올바른 인증서임을 확인할 수 있습니다.443 포트 설정시 주의사항NLB 는 ALB 와 다르게 4계층에서 동작하기 때문에 http로 들어온 요청을 https로 redirect 처리할수 없습니다. 그래서 만약 443 포트로 통신하게 만든 대상그룹을 NLB의 80포트에 매핑하게 된다면, 아래와 같은 화면을 보게 될겁니다.따라서 내부 웹서버나 내부 로드밸런서가 영구 이동방식으로 redirect 처리하게 되더라도 최초 진입 엔트리 포인트는 내부까지 요청을 전달할수 있게끔 설정할 필요가 있습니다. 현재(2021-10-07) 기준으로는 다른 방법이 없어 어쩔수 없이 타겟그룹을 2개 만들어 각각 80, 443 포트로 통신하게 만들고 NLB의 리스너는 각각 다른 대상그룹을 바라보게 해야 합니다.이 기능자체가 나온지 얼마되지 않았기 때문에 위와 같은 문제를 해결할 새로운 방법을 제시해 줄지는 지켜봐야 할것 같습니다.결론개인적으로 이 기능은 람다, GA, 프록시 서버를 사용하지 않고 깔끔한(?) 방식으로 ALB 에 고정 IP를 부여할수 있는 방법인것 같습니다. SSL 설정도 기존의 ALB를 따라가게 되구요. 경우에 따라 부과되는 요금이 조금더 비쌀수는 있겠지만 전체적인 관리의 편의성을 봤을때 앞으로 이 방법을 계속 사용할것 같습니다.참조https://aws.amazon.com/ko/about-aws/whats-new/2021/09/application-load-balancer-aws-privatelink-static-ip-addresses-network-load-balancer" }, { "title": "팩토리 메소드 패턴 (Factory Method Pattern)", "url": "/posts/factory-method-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-09-25 20:12:00 +0900", "snippet": "팩토리 메소드 패턴팩토리 메소드 패턴은 부모 클래스에 알려지지 않은 구체 클래스를 생성하는 패턴이며, 자식 클래스가 어떤 객체를 생성할지 결정하도록 하는 생성패턴 입니다. 간단히 요약하자면 부모 클래스 대신 자식 클래스에서 객체를 생성하는 패턴이라고 생각하시면 됩니다.구조 생성자와 그 하위클래스 객체를 생성하는데 공통적으로 사용될수 있는 Product 인터페이스를 선언합니다. Concreate Products는 Product 인터페이스의 구현체 입니다. Creator 클래스는 새로운 product 객체를 반환하는 팩토리 메소드를 선언합니다. 중요한 점은 이 메소드의 리턴 타입이 product 인터페이스와 일치해야 한다는 것입니다. Concreate Creators는 다른 타입의 product를 반환할 수 있도록 기본 팩토리 메소드를 재정의 합니다.적용 가능한 경우 코드가 동작해야 하는 객체의 유형과 종속성을 미리 알수 없는 경우 라이브러리 혹은 프레임워크 사용자에게 구성 요소를 확장하는 방법을 제공하려는 경우 기존 객체를 재구성하는 대신 기존 객체를 재사용하여 리소스를 절약하고자 하는 경우장단점장점 생성자와 구현 클래스의 결합을 피할 수 있습니다. 객체 생성 코드를 한 곳 (패키지, 클래스 등)으로 이동하여 코드를 유지보수하기 쉽게 할수 있으므로 단일 책임 원칙을 만족합니다. 기존 클라이언트 코드를 건드리지 않고 새로운 타입의 객체를 생성할수 있기 때문에 개방/폐쇄 원칙을 만족합니다.단점 패턴을 구현하기 위해 많은 서브 클래스를 만들어 구현해야 하기 때문에 코드의 복잡성이 증가할 수 있습니다.예제모형의 타입에 따라 어떤 모형을 생성할지 팩토리 클래스에서 정할수 있게 되는 간단한 예제를 만들어 보겠습니다.일단 모양 인터페이스와 그 구현체들 입니다.public interface Shape { void draw();}public class Square implements Shape{ @Override public void draw() { System.out.println(\"draw square\"); }}public class Rectangle implements Shape{ @Override public void draw() { System.out.println(\"draw rectangle\"); }}다음은 팩토리 메소드 패턴의 꽃 팩토리 클래스 입니다. getShape() 메소드는 모형의 타입에 따라 square 혹은 rectangle 인스턴스를 반환합니다.public class ShapeFactory { public Shape getShape(String type) { if (\"SQUARE\".equalsIgnoreCase(type)) { return new Square(); } if (\"RECTANGLE\".equalsIgnoreCase(type)) { return new Rectangle(); } return null; }}마지막으로 클라이언트 입니다. 팩토리 클래스에 의해 조건에 맞는 모형 인스턴스를 생성하고 draw() 메소드를 호출합니다.public class FactoryMethodClient { public static void main(String[] args) { ShapeFactory shapeFactory = new ShapeFactory(); Shape s1 = shapeFactory.getShape(\"SQUARE\"); s1.draw(); Shape s2 = shapeFactory.getShape(\"RECTANGLE\"); s2.draw(); }}새로운 모형이 생긴다 하더라도 shape 인터페이스를 상속하는 새로운 클래스를 만들고 팩토리 클래스의 getShape() 메소드만 수정해주면 되기 때문에 단일 책임 원칙, 개방/폐쇄 원칙을 만족하게 됩니다.결론팩토리 메소드 패턴을 사용하는 궁극적인 이유는 클래스간의 결합도를 느슨하게 가져가게 하기 위함 입니다. 팩토리 클래스를 통해 객체 생성 코드를 한곳에 모이게 함으로써 클라이언트에서 직접 객체를 생성하지 않아도 되므로 코드를 보다 효율적으로 관리할 수 있게 됩니다.하지만 설계에 따라 서브 클래스가 많이 생성될 수 있으므로 코드 복잡도는 올라갈 수 있습니다. 따라서 다른 패턴들과 마찬가지로 올바른 설계가 필요합니다.참조 https://refactoring.guru/design-patterns/factory-method" }, { "title": "전략 패턴 (Strategy Pattern)", "url": "/posts/strategy-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-09-18 20:12:00 +0900", "snippet": "전략 패턴전략 패턴은 실행 중에 알고리즘을 선택할 수 있게 하는 행위 디자인 패턴 입니다. 즉, 각각의 객체들이 할 수 있는 동작들일 미리 전략으로 만들어 놓고 동작을 동적으로 변경해야 한다면 해당 전략만 변경하여 객체의 동작이 바뀌도록 하는 패턴입니다.구조 컨텍스트는 전략 구현체중 하나와 참조를 유지하고 전략 인터페이스를 통해서만 객체와 통신합니다. 전략 인터페이스는 모든 전략 구현체에 대한 공용 인터페이스입니다. 컨텍스트가 전략을 실행하는데 사용하는 메소드를 선언합니다. 인터페이스의 구현체는 컨텍스트가 사용하는 알고리즘의 다른 버전을 구현합니다. 컨텍스트는 알고리즘을 실행해야 할 때마다 해당 알고리즘과 연결된 전략 객체의 메소드를 호출합니다. 컨텍스트는 어떤 전략이 동작하는지, 혹은 어떤 알고리즘이 실행되는지 알 필요는 없습니다. 클라이언트는 특정 전략 객체를 만들어 컨텍스트에 전달합니다.적용 가능한 경우 객체 내에서 서로 다른 알고리즘을 사용하고 런타임시 다른 알고리즘으로 전환하고자 하는 경우 일부 행위를 동작하는 방법만 다른 비슷한 클래스가 많은 경우 비즈니스 로직을 알고리즘의 세부 내용에서 분리하고자 하는 경우장단점장점 런타임에 객체 내부에서 사용되는 알고리즘을 바꿀수 있습니다. 알고리즘을 사용하는 코드에서 이를 구현하는 세부 정보를 분리할 수 있습니다. 상속을 구성으로 대체할 수 있습니다. 기존 컨텍스트를 변경하지 않고 새로운 전략을 도입함으로써 개방/폐쇄 원칙을 만족합니다.단점 알고리즘이 많지 않고 자주 변경되지 않는다면 새로운 클래스와 인터페이스를 만들어 프로그램을 복잡하게 만들 이유가 없습니다. 개발자는 적절한 전략을 선택하기 위해 전략 간의 차이점을 알고 있어야 합니다. 최신 프로그래밍 언어에는 클래스 및 인터페이스를 추가하지 않아도 전략 객체를 사용하는 것과 동일한 효과를 낼 수 있는 방법이 있습니다.예제결제를 할수 있는 환경이 PC / Mobile 로 나뉘어져 있고 각각의 환경은 하나의 결제 방식을 선택할수 있으며 이를 바꿀수 있는 예제를 만들어 보겠습니다.아래 코드는 결제 전략과 이를 구현한 세가지의 결제 방식(페이) 클래스 입니다.public interface PayStrategy { void pay(int amount);}public class NaverPay implements PayStrategy{ @Override public void pay(int amount) { System.out.println(\"네이버 페이로 결제 - 금액 : \" + amount); }}public class KakaoPay implements PayStrategy{ @Override public void pay(int amount) { System.out.println(\"카카오 페이로 결제 - 금액 : \" + amount); }}public class TossPay implements PayStrategy{ @Override public void pay(int amount) { System.out.println(\"토스 페이로 결제 - 금액 : \" + amount); }}아래 코드는 결제 전략을 변수로 갖고 setter() 메소드를 통해 결제 전략을 주입할 수 있는 클래스 입니다. 해당 클래스를 통해 결제 환경을 만들 수 있습니다.public class PayEnvironment { private PayStrategy payStrategy; public void setPayStrategy(PayStrategy payStrategy) { this.payStrategy = payStrategy; } public void pay(int amount) { if (payStrategy != null) { payStrategy.pay(amount); } }}public class PC extends PayEnvironment {}public class Mobile extends PayEnvironment {}마지막 클라이언트 코드 입니다. 결제 환경에 따라 결제 전략을 선택할 수 있고 특정 조건을 만족하면 결제 전략을 변경합니다.public static void main(String[] args) { PayEnvironment pc = new PC(); PayEnvironment mobile = new Mobile(); pc.setPayStrategy(new KakaoPay()); mobile.setPayStrategy(new NaverPay()); pc.pay(10); mobile.pay(30); // 조건을 만족하면 결제 전략을 변경한다. if (condition()) { mobile.setPayStrategy(new TossPay()); } mobile.pay(50);}public static boolean condition() { return true;}결론전략 패턴은 현업에서 알게 모르게 많이 사용하고 있는것 같습니다. 요즘 새로운 언어에서는 기본 기능으로도 간단하게 구현할 수 있으니 한번 찾아 보시길 추천 드리겠습니다.참조 https://refactoring.guru/design-patterns/strategy" }, { "title": "빌더 패턴 (Builder Pattern)", "url": "/posts/builder-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-09-04 20:12:00 +0900", "snippet": "빌더 패턴빌더 패턴은 복합 객체의 생성 과정과 표현 방법을 분리하여 동일한 생성 절차에서 서로 다른 표현 결과를 만들수 있게 하는 패턴입니다. 객체를 생성할때 흔하게 사용됩니다.구조 빌더 인터페이스는 모든 유형의 빌더에 사용되는 공통적인 단계를 선언합니다. 빌더 구현체는 생성 단계에서 서로 다른 구현체를 제공합니다. 이 구현체는 공통 인터페이스를 따르지 않는 다른 빌더를 만들수도 있습니다. Product1, Product2는 결과 객체입니다. 이들은 동일한 클래스 계층 구조나 인터페이스에 속할 필요가 없습니다. Director 클래스는 개발자가 특정 Product의 구성을 생성하고 재사용할 수 있도록 생성 단계를 호출하는 순서를 지정합니다. 클라이언트 클래스는 Director 클래스와 함께 빌더 객체중 하나와 연결되어야 합니다.적용 가능한 경우 점층적 생성자 를 제거하고자 하는 경우 빌더 패턴을 사용하세요. 점층적 생성자란 아래 예제 코드와 같이 비필수 매개변수가 없는 생성자부터 필수 매개변수를 전부 받는 생성자까지 생성자의 매개변수를 점층적으로 늘려가는 패턴을 뜻합니다.불필요한 배개변수까지 값을 지정해야 하며 생성자 수가 많아질수 있다는 단점을 가지고 있습니다.public class TelescopingConstructor { private String name; private String age; private String gender; private String job; private String birthday; private String address; public TelescopingConstructor(String name, String age) { this(name, age, \"unknown\"); } public TelescopingConstructor(String name, String age, String gender) { this(name, age, gender, \"unknown\"); } public TelescopingConstructor(String name, String age, String gender, String job) { this(name, age, gender, job, \"unknown\"); } public TelescopingConstructor(String name, String age, String gender, String job, String birthday) { this(name, age, gender, job, birthday, \"unknown\"); } public TelescopingConstructor(String name, String age, String gender, String job, String birthday, String address) { this.name = name; this.age = age; this.gender = gender; this.job = job; this.birthday = birthday; this.address = address; }}2. 당신의 코드를 통해 상품을 다른 표현 방식으로 만들고 싶을때 빌더 패턴을 사용해보세요.3. 복잡한 클래스나 서로다른 트리들을 결합하여 객체를 생성하고자 할때 빌더 패턴을 사용해보세요.장단점장점 객체를 단계별로 구성하거나 구성 단계를 지연하거나 재귀적으로 생성할수 있습니다. 제품을 다양한 방식으로 표현할때 동일한 생성 코드를 재사용할 수 있습니다. 복잡한 구성의 코드를 비즈니스 로직으로 분리할수 있음으로써 단일 책임 원칙을 만족합니다.단점 패턴을 적용시키고자 할때 N개의 새로운 클래스르 만들어야 하므로 코드의 복잡성이 증가합니다. 생성비용 자체는 크지 않지만 성능이 중요시되는 상황이 오면 문제가 될수 있습니다.vs Java Beans Pattern빈 객체 생성후 setter 메소드를 이용한 값의 주입을 자바 빈 패턴이라고 합니다. 이는 실무에서도 자주 볼수있는 패턴입니다.하지만 객체 생성 시점에 모든 값들을 주입하지 않아 어느 시점에 객체의 값이 변경될지 알수 없어 유지보수시 장애물이 될수 있습니다. 또 일일히 setter 메소드를 정의해줘야 한다는 단점도 있습니다. (롬복이라는 훌륭한 라이브러리가 있긴 하지요…)이런 저런 이유로 객체를 생성하는 시점에 모든 값을 주입하여 불변 객체를 생성하는 빌더 패턴이 더 객체지향적인 패턴이라고 할수 있겠습니다.예제사람을 빌더 패턴을 이용해 만드는 예제입니다. 필수 정보는 Builder() 생성자에 포함되어야 합니다.public class Person { private String name; private String age; private String gender; private String job; private String birthday; private String address; public static class Builder { // 필수 private String name; private String age; // 비필수수 private String gender; private String job; private String birthday; private String address; public Builder(String name, String age) { this.name = name; this.age = age; } public Builder gender(String gender) { this.gender = gender; return this; } public Builder job(String job) { this.job = job; return this; } public Builder birthday(String birthday) { this.birthday = birthday; return this; } public Builder address(String address) { this.address = address; return this; } public Person build() { return new Person(this); } } public Person(Builder builder) { this.name = builder.name; this.age = builder.age; this.gender = builder.gender; this.job = builder.gender; this.birthday = builder.birthday; this.address = builder.address; } @Override public String toString() { return \"Person{\" + \"name='\" + name + '\\'' + \", age='\" + age + '\\'' + \", gender='\" + gender + '\\'' + \", job='\" + job + '\\'' + \", birthday='\" + birthday + '\\'' + \", address='\" + address + '\\'' + '}'; }}클라이언트 코드와 결과입니다. 값을 지정하지 않은 필드는 null 로 출력되는것을 확인할 수 있습니다.public class BuilderClient { public static void main(String[] args) { Person person = new Person .Builder(\"keencho\", \"20\") .gender(\"man\") .job(\"developer\") .build(); System.out.println(person.toString()); }}Person{name='keencho', age='20', gender='man', job='man', birthday='null', address='null'}간단한 빌더 패턴 예제입니다. 여기서는 메소드를 하나하나 만들었지만 저는 주로 롬복 라이브러리의 @Builder 어노테이션을 많이 사용하는 편입니다.결론빌더 패턴은 생성 패턴중의 하나로 자바빈 패턴의 단점을 보완하는 패턴입니다. 많이 사용되는 패턴중 하나이지만 성능에 민감한 상황이 되면 문제가 될수 있고 클래스는 시간이 갈수록 커지는 경향이 있으므로 주의하여 사용해야 합니다.참조 https://refactoring.guru/design-patterns/builder" }, { "title": "리액트에서 폴더 구조대로 라우팅하기 (feat. Vite)", "url": "/posts/react-vite-routing/", "categories": "React", "tags": "React, Vite", "date": "2021-08-28 20:12:00 +0900", "snippet": "Next js 처럼 라우팅하기Next js는 리액트에서 SSR을 쉽게 구현할수 있게 도와주는 라이브러리 입니다. Next의 파일 시스템을 기반으로한 라우팅 을 기본으로 하고 있습니다. (참조)오늘은 Next 없이 Next와 같은 라우팅을 구현해보고자 합니다.CSR로 구현하기?리액트는 lazy 와 Suspense API로 동적 로딩을 지원하고 있기는 합니다. 이를 이용해 쉽게 구현할수도 있지요. 또 이런 라이브러리도 있습니다.하지만 결국 이는 클라이언트 사이드에서 앱이 시작될때 라우팅 규칙이 생성되기 때문에 뭔가 제 마음에 썩 들지는 않았습니다.Vite사실 이 포스팅의 목적은 폴더 구조대로 라우팅이 아닌 최근에 관심있게 지켜보고 있는 Vite 라는 빌드 툴에 대해 정리하고자 하는것이었습니다. 하지만 Vite의 docs를 읽어보는 도중 폴더 구조대로 라우팅하는 방법을 찾았고, 포스팅의 목적을 바꾸게 되었습니다.Vite란?일단 Vite에 대해 조금만 알아보고 넘어가도록 하겠습니다. Vite는 Vue js의 개발자인 Evan You가 개발한 빌드 툴입니다. Webpack 이나 Snowpack같은 툴이지요. (Snowpack에 더 가깝긴 합니다.) 2021년 8월 기준 깃헙 스타도 31.3k 정도로 핫한 빌드툴입니다.Vite는 모던 브라우저에서 지원하는 &lt;script module&gt; 을 이용해 개발환경 구동시 필요한 모듈만 불러와서 실행하고 실제 서비스 빌드시에는 Rollup으로 번들링해주는 툴입니다. 직접 적용해서 시작해보면 시작속도가 굉장히 빠른것을 확인할 수 있습니다.뿐만 아니라 수정시 수정한곳만 다시 리로드해서 보여주므로 리로딩 속도도 굉장히 빠른편입니다. 또한 SSR도 지원합니다.Webpack을 대체할수 있어? 라고 물으신다면 이미 Webpack을 기반으로한 여러 라이브러리들이 많고 Vite는 IE11을 지원하지 않는등 여러 문제가 있어 아직은 시기상조라고 생각되나 추후 이 라이브러리가 더 개발된다면충분히 대체할수 있다고 생각합니다.파일 시스템 기반 라우팅Vite에 대해 대충 훑어봤으니 실제로 Vite를 이욯애 리액트앱을 만들고 파일 시스템 기반 라우팅을 적용해보도록 하겠습니다. 최종적인 라우팅 목표는 다음과 같습니다. 인덱스 라우팅 src/pages/index.tsx -&gt; ‘’, ‘/’ src/pages/user/index.tsx -&gt; ‘/user’ 중첩 라우팅 src/pages/user/list.tsx -&gt; ‘user/list’ 동적 라우팅 src/pages/user/[id].tsx -&gt; ‘/user/12’, ‘user/34’ 시작하기다음 명령어로 Vite를 이용해 리액트 - 타입스크립트 앱을 생성합니다.npm init vite@latest react-app --template react-ts그리고 react-router-dom을 설치합니다.npm install react-router-dom @types/react-router-dom라우팅 규칙 정의하기Vite의 glob import API를 사용하여 glob 패턴으로 라우팅 규칙을 정의해보겠습니다. Vite의 glob은 fast-glob 과 일치하며glob에서 지원하는 규칙을 알고 싶으시면 여기 를 확인해보세요.import.meta.globEager 를 사용하여 동기방식으로 src/pages 폴더 내부에 있고 확장자가 .tsx인 컴포넌트들을 가져와 변수에 저장하겠습니다.const COMPONENTS = import.meta.globEager('/src/pages/**/[a-z[]*.tsx')위 패턴은 파일 이름이 소문자 a-z 로 시작하거나 중괄호 [ 로 시작하며 확장자는 tsx인 파일들을 모두 찾아옵니다..map 메소드를 사용해 COMPONENTS를 순회하며 경로(path) 와 컴포넌트를 값으로 가지는 components변수를 정의하겠습니다.const components = Object.keys(COMPONENTS).map((component) =&gt; { const path = component .replace(/\\/src\\/pages|index|\\.tsx$/g, '') .replace(/\\[\\.{3}.+\\]/, '*') .replace(/\\[(.+)\\]/, ':$1') return { path, component: COMPONENTS[component].default }})위 규칙으로 정의한 components 변수의 값은 다음과 같이 정의될 수 있습니다.components = [ { path: \"/\", component: f App() }, { path: \"/user\", component: f UserList() }, { path: \"/user/:id }, component: f UserDetail() }]그리고 마지막으로 components 변수를 순회하며 리액터 라우트를 정의하여 리턴하는 Routes() 컴포넌트를 만들겠습니다.export const Routes = () =&gt; { return ( &lt;App&gt; &lt;Switch&gt; {components.map(({ path, component: Component = Fragment }) =&gt; ( &lt;Route key={path} path={path} component={Component} exact={true} /&gt; ))} &lt;/Switch&gt; &lt;/App&gt; )}404 정의하기추가적으로 라우터에 등록된 컴포넌트가 아닐 경우 404 페이지를 리턴하도록 해보겠습니다. 위 components 변수와 같이 _app.tsx와 404.tsx의 경로를 저장하는 변수를 정의하겠습니다.const BASIC = import.meta.globEager('/src/pages/(_app|404).tsx')_app.tsx와 404.tsx의 컴포넌트를 저장하는 basics 변수를 정의하겠습니다.const basics = Object.keys(BASIC).reduce((basic, file) =&gt; { const key = file.replace(/\\/src\\/pages\\/|\\.tsx$/g, '') return { ...basic, [key]: BASIC[file].default }}, {})자, 이제 위에서만든 Routes() 컴포넌트를 살짝 변경하겠습니다.export const Routes = () =&gt; { const App = basics?.['_app'] || Fragment const NotFound = basics?.['404'] || Fragment return ( &lt;App&gt; &lt;Switch&gt; {components.map(({ path, component: Component = Fragment }) =&gt; ( &lt;Route key={path} path={path} component={Component} exact={true} /&gt; ))} &lt;Route path=\"*\" component={NotFound} /&gt; &lt;/Switch&gt; &lt;/App&gt; )}만약 basics 변수에 해당 컴포넌트가 없다면 리액트의 Fragment가 쓰이게 됩니다.최종 코드모두 완성되었습니다. 최종 코드는 다음과 같습니다.// src/routes.tsximport React, { Fragment } from 'react'import { Switch, Route } from 'react-router-dom'const BASIC: Record&lt;string, { [key: string]: any }&gt; = import.meta.globEager('/src/pages/(_app|404).tsx')const COMPONENTS: Record&lt;string, { [key: string]: any }&gt; = import.meta.globEager('/src/pages/**/[a-z[]*.tsx')const basics = Object.keys(BASIC).reduce((basic, file) =&gt; { const key = file.replace(/\\/src\\/pages\\/|\\.tsx$/g, '') return { ...basic, [key]: BASIC[file].default }}, {})const components = Object.keys(COMPONENTS).map((component) =&gt; { const path = component .replace(/\\/src\\/pages|index|\\.tsx$/g, '') .replace(/\\[\\.{3}.+\\]/, '*') .replace(/\\[(.+)\\]/, ':$1') return { path, component: COMPONENTS[component].default }})export const Routes = () =&gt; { const App = basics?.['_app'] || Fragment const NotFound = basics?.['404'] || Fragment return ( &lt;App&gt; &lt;Switch&gt; {components.map(({ path, component: Component = Fragment }) =&gt; ( &lt;Route key={path} path={path} component={Component} exact={true} /&gt; ))} &lt;Route path=\"*\" component={NotFound} /&gt; &lt;/Switch&gt; &lt;/App&gt; )}// main.tsximport React, { StrictMode } from 'react'import ReactDOM from 'react-dom'import { BrowserRouter } from 'react-router-dom'import { Routes } from './routes'ReactDOM.render( &lt;StrictMode&gt; &lt;BrowserRouter basename='/finance'&gt; &lt;Routes /&gt; &lt;/BrowserRouter&gt; &lt;/StrictMode&gt;, document.getElementById('root'))파일 구조는 다음과 같습니다.├── index.html├── index.css├── package.json├── package-lock.json├── tsconfig.json└── src ├── pages │ ├── 404.tsx │ ├── _app.tsx │ └── user │ ├── index.tsx │ └── [id].tsx ├── main.tsx ├── routes.tsx └── vite-env.d.tsx결론Vite 와 globEager 를 사용해 파일 시스템을 기반으로한 라우팅을 적용해보았습니다. 적용하고 나니 참 편리한것 같습니다. 이곳에서 Vite에 대해 자세히 다루지는 않았지만개인적으로 앞으로의 토이 프로젝트에는 Vite를 주로 사용할것 같습니다. 토이 프로젝트를 React나 Vue (Angular는 아직입니다…) 로 구성할 계획이 있으신 분들이면 한번 사용해 보시기 바랍니다." }, { "title": "어댑터 패턴 (Adapter Pattern)", "url": "/posts/adapter-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-08-21 20:12:00 +0900", "snippet": "어댑터 패턴어댑터 패턴은 클래스의 인터페이스를 사용자가 기대하는 다른 인터페이스로 변환하는 패턴으로, 호환성이 없는 인터페이스 때문에 함께 동작할 수 없는 클래스들이 함께 동작하도록 해주는 패턴입니다.구조객체 어댑터 클라이언트는 비즈니스 로직을 포함하고 있는 클래스입니다. 클라이언트 인터페이스는 다른 클래스가 클라이언트 코드와 협력할 수 있도록 하기 위해 따라야 하는 프로토콜을 설명합니다. 서비스는 유용한 클래스(대체적으로 써드파티 혹은 레거시 코드) 입니다. 클라이언트는 호환되지 않는 인터페이스를 가지고 있기 때문에 이 클래스를 직접 사용할 수 없습니다. 어댑트는 클라이언트와 서비스 모두에서 작동할 수 있는 클래스입니다. 이 클래스는 서비스 객체를 감싸는 동안 클라이언트 인터페이스를 구현합니다. 클라이언트 코드는 클라이언트 인터페이스를 통해 어댑터와 작동하기 때문에 어댑터 클래스와 결합되지 않습니다.클래스 어댑터 클래스 어댑터는 클라이언트와 서비스의 동작을 직접 상속하므로 객체를 감쌀 필요학 없습니다.적용 가능한 경우 레거시 코드를 사용하고 싶지만 새로운 인터페이스가 레거시 코드와 호환되지 않을 때 어댑터 클래스를 고려해볼 수 있습니다. 슈퍼클래스에 추가할 수 없는 일부 공통 기능이 없는 기존 서브 클래스를 재사용하려면 이 패턴을 사용해보세요.장단점장점 인터페이스 또는 데이터 변환 코드를 프로그램의 비즈니스 로직과 분리할 수 있기 때문에 단일 책임 원칙을 만족합니다. 기존 클라이언트의 코드를 건들지 않고 클라이언트 인터페이스를 통해 어댑터와 작동하기 때문에 계방/폐쇄 원칙을 만족합니다.단점 새로운 인터페이스와 클래스 세트를 구현해야 하기 땜누에 코드의 복잡성이 증가합니다. 나머지 코드와 잘 작동하도록 서비스 클래스를 직접 변경하는 것이 오히려 더 간단할수 있습니다.예제모니터와 TV를 컨트롤하는 프로그램을 만들어보겠습니다. 모니터는 on, off의 기능을 가지고있고 TV는 on, off, 볼륨 업, 볼륨 다운의 기능을 가지고 있습니다.public interface Monitor { void on(); void off();}public class LEDMonitor implements Monitor{ @Override public void on() { System.out.println(\"LED Monitor ON\"); } @Override public void off() { System.out.println(\"LED Monitor OFF\"); }}public class LCDMonitor implements Monitor{ @Override public void on() { System.out.println(\"LCD Monitor ON\"); } @Override public void off() { System.out.println(\"LCD Monitor OFF\"); }}public interface TV { void on(); void off(); void volumeUp(); void volumeDown();}public class LEDTV implements TV{ @Override public void on() { System.out.println(\"LED TV ON\"); } @Override public void off() { System.out.println(\"LED TV OFF\"); } @Override public void volumeUp() { System.out.println(\"LED TV VOLUME UP\"); } @Override public void volumeDown() { System.out.println(\"LED TV VOLUME DOWN\"); }}간단한 코드로 모니터와 TV를 만들었습니다. 근데 모니터로 TV를 볼수 있도록 해달라는 요청사항이 생겼습니다. 어댑터 클래스를 사용하여 모니터에서 TV의 기능을 수행할 수 있도록 해보겠습니다.public class MonitorAdapter implements TV{ Monitor monitor; public MonitorAdapter(Monitor monitor) { this.monitor = monitor; } @Override public void on() { monitor.on(); } @Override public void off() { monitor.off(); } @Override public void volumeUp() { throw new RuntimeException(\"지원하지 않는 기능입니다.\"); } @Override public void volumeDown() { throw new RuntimeException(\"지원하지 않는 기능입니다.\"); }}모니터로 TV의 on, off 기능을 사용할 수 있게 하였으나, 볼륨 조절 관련 부분은 모니터 하드웨어 상으로 지원하지 않기 때문에 예외를 던짐으로써 해당 메소드의 기능을 제한하였습니다.public class AdaptorClient { public static void main(String[] args) { LCDMonitor lcdMonitor = new LCDMonitor(); LEDMonitor ledMonitor = new LEDMonitor(); System.out.println(\"===========================\"); lcdMonitor.on(); lcdMonitor.off(); System.out.println(\"===========================\"); ledMonitor.on(); ledMonitor.off(); System.out.println(\"===========================\"); TV monitorTV = new MonitorAdapter(ledMonitor); ledMonitorTV.on(); ledMonitorTV.off(); ledMonitorTV.volumeUp(); ledMonitorTV.volumeDown(); System.out.println(\"===========================\"); }}===========================LCD Monitor ONLCD Monitor OFF===========================LED Monitor ONLED Monitor OFF===========================LED Monitor ONLED Monitor OFFException in thread \"main\" java.lang.RuntimeException: 지원하지 않는 기능입니다.\tat sycho.java.pattern.adapter.MonitorAdapter.volumeUp(MonitorAdapter.java:23)\tat sycho.java.pattern.adapter.AdaptorClient.main(AdaptorClient.java:23)클라이언트 코드입니다. 모니터 어댑터 클래스로 TV 기능을 하는 모니터를 만들었고, 볼륨 업 메소드를 호출하자 지원하지 않는 기능이라며 에러를 뱉는 모습까지 확인할 수 있습니다.결론어댑터 패턴은 레거시 코드에서 신규 코드로의 변환을 도모할때 도움이 되는 패턴입니다. 다만 어댑터 클래스의 수만큼 코드 복잡성이 증가하게 되는 꼴이므로 역시 올바른 설계가 필요합니다.참조 https://refactoring.guru/design-patterns/adapterhttps://www.geeksforgeeks.org/adapter-pattern/" }, { "title": "프로토타입 패턴 (Prototype Pattern)", "url": "/posts/prototype-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-08-05 20:12:00 +0900", "snippet": "프로토타입 패턴프로토타입 패턴은 원형이 되는 인스턴스를 사용해 새롭게 생성할 객체의 종류를 명시하여 새로운 객체가 생성될 시점에 인스턴스의 타입이 결정되도록 하는 패턴입니다.구조 임의의 인스턴스를 복제하는 메소드를 가진 인터페이스인 Prototype 인터페이스를 생성합니다. 대두분의 경우 이 인터페이스에는 clone() 메소드 하나만 선언되어 있습니다. Prototype 인터페이스를 구현하는 클래스를 생성합니다. 이 클래스는 원본 객체의 데이터를 복사하는것 말고도 연결된 객체의 복사와 관련된 작업이나 전의 의존성에서 벗어나게 하는 작업 등을 수행할 수 있습니다. 클라이언트는 Prototype 인터페이스를 따르는 모든 객체를 복사하여 인스턴스를 생성할 수 있습니다.적용 가능한 경우 코드가 복사해야 하는 구현 클래스에 의존하지 않아야 하는 경우 프로토타입 패턴을 사용할 수 있습니다. 이 경우는 코드가 인터페이스를 통해 써드파티 코드와 함께 작동할 경우 많이 발생합니다. 객체를 초기화 하는 방식만 다를뿐 서브클래스의 수를 줄이려는 경우 프로토타입 패넡을 사용할 수 있습니다.장단점장점 구현 클래스에 직접 연결하지 않고 객체를 복사할 수 있습니다. 프로토타입이 미리 정의되어 있기 때문에 중복되는 초기화 코드를 제거할 수 있습니다. 복잡한 오브젝트를 보다 편리하게 만들수 있습니다.단점 순환 참조가 있는 복잡한 객체를 복제하는 것은 매우 까다로울 수 있습니다.예제db에서 모든 자동차의 종류를 가져와 클라이언트에 보여줘야하는 프로그램이 있다고 가정해 보겠습니다. 아래는 간단한 Car 클래스 입니다.public class Car implements Cloneable { private List&lt;String&gt; carList; public Car() { this.carList = new ArrayList&lt;&gt;(); } public Car(List&lt;String&gt; carList) { this.carList = carList; } public List&lt;String&gt; getCarList() { return carList; } public void listAll() { // db에서 자동차의 종류를 가져오는 코드 this.carList.add(\"truck\"); this.carList.add(\"suv\"); this.carList.add(\"sedan\"); this.carList.add(\"sports car\"); } @Override protected Object clone() throws CloneNotSupportedException { return new Car(new ArrayList&lt;&gt;(this.carList)); }}Cloneable 인터페이스를 구현하여 Car 클래스를 생성하였고 clone() 메소드를 통해 깊은 복사 방식으로 인스턴스를 복제하도록 하였습니다.public class CarClient { public static void main(String[] args) throws CloneNotSupportedException { Car car = new Car(); car.listAll(); Car car2 = (Car) car.clone(); Car car3 = (Car) car.clone(); List&lt;String&gt; car2List = car2.getCarList(); List&lt;String&gt; car3List = car3.getCarList(); car2List.add(\"mini car\"); car3List.remove(\"truck\"); System.out.println(car.getCarList()); System.out.println(car2List); System.out.println(car3List); }}[truck, suv, sedan, sports car][truck, suv, sedan, sports car, mini car][suv, sedan, sports car]위 코드는 자동차 클라이언트와 클라이언트의 실행 결과 입니다. 깊은 복사 방식을 사용했으므로 각각의 객체 인스턴스가 모두 다른 결과를 도출해 내는 것을 확인할수 있습니다.결론프로토타입 패턴은 db에서 빈번하게 데이터를 가져올때, 그 데이터가 항상 똑같은 값을 반환하는 경우 유용하게 사용할 수 있습니다.db에서 그때그때 가져오는 것이 좋은것 아니야? 라고 생각하실수 있겠지만 db에 접근하는데 사용하는 자원, 비용이 객체를 복사하는 비용보다 훨씬 크다는것을 염두해 두셔야 합니다.참조 https://refactoring.guru/design-patterns/prototype" }, { "title": "Transaction silently rolled back because it has been marked as rollback-only", "url": "/posts/transaction-rollback/", "categories": "Spring", "tags": "Spring, Transaction", "date": "2021-07-28 20:12:00 +0900", "snippet": "이게 왜 저장이 안될까?서비스를 운영하다가 이 포스팅의 제목과 같이 Transaction silently rolled back because it has been marked as rollback-only 라는 에러 메시지를 받게 되었습니다.해당 에러의 원인은 금방 찾았으나, 한 서비스가 내부 서비스의 메소드를 try-catch 블록으로 감싸 호출하는 형태로 코드를 작성했기 때문에 예외가 발생하더라도 commit은 정상적으로 이루어질 것으로 예상했습니다.그러나 제 마음처럼 되지 않았고 예외 발생전 작성한 저장 / 수정 코드가 롤백 되어 위의 에러 메시지를 받게 되었습니다. 이 포스트에서는 왜 이런일이 발생한 것인지, 해결 방법은 있는 것인지 알아보도록 하겠습니다.상황 재현하기상용 서비스와 비슷한 환경을 만들어 상황을 재현해보겠습니다. 현재 서비스는 spring-boot 2.4.2, postgresql, spring data jpa 를 중심으로 이루어져 있습니다.@Service@Transactional@Slf4jpublic class AccountParentService { @Autowired AccountChildService accountChildService; public void saveAccountWithException() { try { accountChildService.saveAccountAndThrowException(); } catch (Exception ex) { log.error(\"내부 서비스에서 계정 생성중 에러 발생 - \" + ex.getMessage()); } log.info(\"계정 생성 완료\"); }}@Service@Transactionalpublic class AccountChildService { @Autowired AccountRepository accountRepository; public void saveAccountAndThrowException() { accountRepository.save( Account.builder() .name(\"이름\") .loginId(\"로그인 ID\") .age(30) .password(UUID.randomUUID().toString()) .build() ); throw new RuntimeException(\"예외 던지기\"); }}앞서 말씀드렸다시피 외부 트랙잭션 클래스의 메소드가 try-catch 블록으로 내부 트랜잭션 클래스의 메소드를 감싼 채로 호출하고 있습니다. 실제 throw new RuntimeException()를 던졌다 하더라도 accountRepository.save() 이후이고외부 메소드에서는 예외처리가 되어있기 때문에 계정이 생성된다고 생각할 수 있지만 실제로는 ‘계정 생성 완료’ 로그 이후에 아래와 같은 에러메시지가 출력되고 있었습니다.2021-08-24 10:49:31.131 ERROR 16632 --- [nio-9999-exec-2] s.s.basic.service.AccountParentService : 내부 서비스에서 계정 생성중 에러 발생 - 예외 던지기2021-08-24 10:49:31.131 INFO 16632 --- [nio-9999-exec-2] s.s.basic.service.AccountParentService : 계정 생성 완료2021-08-24 10:49:31.148 ERROR 16632 --- [nio-9999-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet] : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.transaction.UnexpectedRollbackException: Transaction silently rolled back because it has been marked as rollback-only] with root causeorg.springframework.transaction.UnexpectedRollbackException: Transaction silently rolled back because it has been marked as rollback-only\tat org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:752) ~[spring-tx-5.3.9.jar:5.3.9]\tat org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:711) ~[spring-tx-5.3.9.jar:5.3.9]\tat org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:654) ~[spring-tx-5.3.9.jar:5.3.9]\tat org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:407) ~[spring-tx-5.3.9.jar:5.3.9]\tat org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119) ~[spring-tx-5.3.9.jar:5.3.9]\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.9.jar:5.3.9]\tat org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:750) ~[spring-aop-5.3.9.jar:5.3.9]\tat org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:692) ~[spring-aop-5.3.9.jar:5.3.9]\tat sycho.spring.basic.service.AccountParentService$$EnhancerBySpringCGLIB$$b70115de.saveAccountWithException(&lt;generated&gt;) ~[main/:na]...&lt;생략&gt;원인 파악하기스택 트레이스를 참조하여 롤백이 진행되는 메소드를 찾아보니 TransactionAspectSupport 클래스의 completeTransactionAfterThrowing 메소드를 찾을수 있었습니다.protected void completeTransactionAfterThrowing(@Nullable TransactionInfo txInfo, Throwable ex) { if (txInfo != null &amp;&amp; txInfo.getTransactionStatus() != null) { if (logger.isTraceEnabled()) { logger.trace(\"Completing transaction for [\" + txInfo.getJoinpointIdentification() + \"] after exception: \" + ex); } if (txInfo.transactionAttribute != null &amp;&amp; txInfo.transactionAttribute.rollbackOn(ex)) { try { txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException | Error ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); throw ex2; } }...&lt;생략&gt;다시 txtInfo.transactionAttriute.rollbackOn(ex) 메소드를 찾아 들어가 보면 DefaultTransactionAttribute 클래스의 rollBackOn() 메소드를 만날수 있습니다.@Overridepublic boolean rollbackOn(Throwable ex) { return (ex instanceof RuntimeException || ex instanceof Error);}아까 던진 예외는 RuntimeException 이기 때문에 롤백이 발생하게 되는군요. 다시 롤백 프로세스를 살펴보겠습니다.if (status.hasTransaction()) { if (status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure()) { if (status.isDebug()) { logger.debug(\"Participating transaction failed - marking existing transaction as rollback-only\"); } doSetRollbackOnly(status); } else { if (status.isDebug()) { logger.debug(\"Participating transaction failed - letting transaction originator decide on rollback\"); } }}processRollback() 메소드를 살펴보면 status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure() 을 만족하면 rollbackOnly 를 세팅하게 되어 있습니다.여차저차 프로세스를 따라가보면 TransactionDriverControlImpl 의 rollbackOnly 필드를 true로 변경하는 것을 확인할 수 있습니다.근데 이 트랜잭션 에서는 rollbackOnly 필드만 변경할 뿐 내부 클래스의 트랜잭션에서는 UnexpectedRollbackException 를 바로 던지지는 않네요.AbstractPlatformTransactionManager.processRollback()if (unexpectedRollback) { // false throw new UnexpectedRollbackException( \"Transaction rolled back because it has been marked as rollback-only\");}내부 클래스의 트랜잭션이 완료되면 최초 트랜잭션 처리가 시작됩니다. 이번에는 status.isNewTransaction() 조건을 만족하고 트랙잭션의 상태가 MARKED_ROLLBACK 으로 마크되어있기 때문에 status.isGlobalRollbackOnly() 가 true를 return 함에 따라 unexpectedRollback 필드가 true가 됩니다.else if (status.isNewTransaction()) { if (status.isDebug()) { logger.debug(\"Initiating transaction commit\"); } unexpectedRollback = status.isGlobalRollbackOnly(); doCommit(status);}그렇다면 최종적으로 아래 코드에 의해 UnexpectedRollbackException이 발생하는 것을 확인할 수 있습니다.// Throw UnexpectedRollbackException if we have a global rollback-only// marker but still didn't get a corresponding exception from commit.if (unexpectedRollback) { throw new UnexpectedRollbackException( \"Transaction silently rolled back because it has been marked as rollback-only\");}그럼 왜?결론적으로 보면 status.isLocalRollbackOnly() || isGlobalRollbackOnParticipationFailure() 조건중 isGlobalRollbackOnParticipationFailure() 이 true 이면 전체 롤백이 되도록 되어있는것 같네요.해당 메소드에 들어가서 이것저것 살펴보고 있는데 마침 위에 setGlobalRollbackOnParticipationFailure() 메소드의 주석에 딱 제가 원하는 내용이 있었습니다./** * Set whether to globally mark an existing transaction as rollback-only * after a participating transaction failed. * &lt;p&gt;Default is \"true\": If a participating transaction (e.g. with * PROPAGATION_REQUIRED or PROPAGATION_SUPPORTS encountering an existing * transaction) fails, the transaction will be globally marked as rollback-only. * The only possible outcome of such a transaction is a rollback: The * transaction originator &lt;i&gt;cannot&lt;/i&gt; make the transaction commit anymore. * &lt;p&gt;Switch this to \"false\" to let the transaction originator make the rollback * decision. If a participating transaction fails with an exception, the caller * can still decide to continue with a different path within the transaction. * However, note that this will only work as long as all participating resources * are capable of continuing towards a transaction commit even after a data access * failure: This is generally not the case for a Hibernate Session, for example; * neither is it for a sequence of JDBC insert/update/delete operations. * &lt;p&gt;&lt;b&gt;Note:&lt;/b&gt;This flag only applies to an explicit rollback attempt for a * subtransaction, typically caused by an exception thrown by a data access operation * (where TransactionInterceptor will trigger a {@code PlatformTransactionManager.rollback()} * call according to a rollback rule). If the flag is off, the caller can handle the exception * and decide on a rollback, independent of the rollback rules of the subtransaction. * This flag does, however, &lt;i&gt;not&lt;/i&gt; apply to explicit {@code setRollbackOnly} * calls on a {@code TransactionStatus}, which will always cause an eventual * global rollback (as it might not throw an exception after the rollback-only call). * &lt;p&gt;The recommended solution for handling failure of a subtransaction * is a \"nested transaction\", where the global transaction can be rolled * back to a savepoint taken at the beginning of the subtransaction. * PROPAGATION_NESTED provides exactly those semantics; however, it will * only work when nested transaction support is available. This is the case * with DataSourceTransactionManager, but not with JtaTransactionManager. * @see #setNestedTransactionAllowed * @see org.springframework.transaction.jta.JtaTransactionManager */번역기의 힘을 빌려 대충 번역하여 다음과 같은 정보를 얻을수 있었습니다. 기본 트랜잭션을 rollback-only 로 마킹하는 것을 전역적으로 적용할지 말지 세팅 기본값은 true 만약 참여중인 트랜잭션의 타입이 PROPAGATION_REQUIRED 거나 PROPAGATION_SUPPORTS 이고 실패하면 트랜잭션은 전역적으로 rollback-only로 마킹된다. rollback-only로 마킹된 트랜잭션은 결국 롤백되게 된다.사실 왜 이렇게 세팅되어 있는지는 정확히는 잘 모르겠습니다만, 저는 롤백 조건중 excpetion instanceof RuntimeException 이라는 조건이 있는것으로 보았을때 런타임 시점에 Unchecked Exception 이 발생하면그것을 감싸고 있는 외부의 예외처리 블록이 있더라도 내부 메소드는 롤백을 원하기 때문에 커밋을 원하는 외부 메소드와 충돌(모순?) 이 일어나 결론적으로는 모두 롤백된다. 라고 마음속으로 결론을 내리겠습니다.해결 방법이 글을 작성하면서 드는 생각입니다만, 편하다고 해서 비즈니스 로직에 무턱대고 예외를 던지는 행위는 또다른 문제를 일으킬수 있을것 같다는 생각이 들었습니다. 예외 객체를 만드는것 자체가 다른 객체를 만드는 비용에 비해 비싸기도 하지요.물론 해결방법은 있습니다. 상황에 알맞은 방법으로 문제를 해결해 보시기 바랍니다.1. Checked ExceptionRuntimeException이 아닌 Exception 을 상속받아 CustomException을 만들고 예외를 던져야 할 때 이 CustomException을 던지면 롤백되지 않습니다.public class CustomException extends Exception { public CustomException(String msg) { super(msg); }}public void saveAccountAndThrowException() throws CustomException { accountRepository.save( Account.builder() .name(\"이름\") .loginId(\"로그인 ID\") .age(30) .password(UUID.randomUUID().toString()) .build() ); throw new CustomException(\"예외 던지기\");}이유는 Exception을 상속받아 만든 Exception은 Checked Exception 이기 때문입니다. 간단하게 Exception을 상속받아 만든 Exception은 Checked Exception 이고 RuntimeException을 상속받아 만든Exception은 Unchecked Exception 이라고 생각하면 될 것 같습니다.위에서 살펴봤다시피 롤백 조건중 excpetion instanceof RuntimeException 조건이 있었습니다. 반대로 말하면 ‘Checked Exception은 발생하더라도 롤백되지 않는다.’ 가 됩니다.Checked Exception은 복구가 가능하다는 전제조건을 가지고 있기 때문에 예외가 발생하더라도 복구가 가능할 때 사용하는 것이 좋습니다. 하지만 실질적으로 모든 경우에 대하여 복구 전략을 세우기 어렵기 때문에정말 확실한 경우에만 사용하는 편이 좋겠지요.2. noRollbackFor 속성 사용@Transactional 메소드에는 noRollbackFor 속성이 있습니다. (javax 패키지의 @Transactional 에는 dontRollbackOn 속성이 있습니다.) 이 속성을 이용하면 예외가 발생해도 롤백하지 않을 예외 클래스를 지정할 수 있습니다.단, 이 속성에 들어갈 클래스는 Throwable 의 서브클래스여야 합니다.@org.springframework.transaction.annotation.Transactional(noRollbackFor = RuntimeException.class)@javax.transaction.Transactional(dontRollbackOn = RuntimeException.class)public void saveAccountAndThrowException() { accountRepository.save( Account.builder() .name(\"이름\") .loginId(\"로그인 ID\") .age(30) .password(UUID.randomUUID().toString()) .build() ); throw new RuntimeException(\"예외 던지기\");}3. Propagation.REQUIRES_NEW일단 재현 코드를 바꿔보겠습니다.@Service@Slf4jpublic class AccountParentService { @Autowired AccountRepository accountRepository; @Autowired AccountChildService accountChildService; @Transactional public void saveAccountWithException() { try { accountChildService.saveAccountAndThrowException(); } catch (Exception ex) { log.error(\"내부 서비스에서 계정 생성중 에러 발생 - \" + ex.getMessage()); } accountRepository.save( Account.builder() .name(\"테스트 이름\") .loginId(\"테스트 ID\") .age(20) .password(UUID.randomUUID().toString()) .build() ); log.info(\"계정 생성 완료\"); }}@Servicepublic class AccountChildService { @Autowired AccountRepository accountRepository; @Transactional public void saveAccountAndThrowException() { accountRepository.save( Account.builder() .name(\"이름\") .loginId(\"로그인 ID\") .age(30) .password(UUID.randomUUID().toString()) .build() ); throw new RuntimeException(\"예외 던지기\"); }}내부 메소드에서 계정 생성 이후 외부 메소드로 다시 돌아와 또 다른 계정을 생성하는 코드를 작성하였습니다. 당연히 현재는 내부 트랜잭션이 전파되기 때문에 전역 롤백이 발생합니다.그렇다면 다음과 같이 내부 메소드의 @Transactional 전파옵션을 REQUIRES_NEW 로 변경해 보겠습니다.@Transactional(propagation = Propagation.REQUIRES_NEW)public void saveAccountAndThrowException() {...&lt;생략&gt;다시 작업을 수행해 보니 내부 메소드의 작업은 롤백되었지만 외부 메소드의 작업은 정상적으로 진행되어 계정 한개가 생성되었습니다.이 글에서 자세히 다루지는 않을 것이지만 내부 메소드에 진입시 새로운 트랜잭션을 시작하여 외부 트랜잭션과 다른 독립적인 트랙잭션으로 작동하기 때문에 내부에서 예외가 발생해도 외부로는 롤백이 전파되지 않는다. 라고 이해하시면 될 것 같습니다.단, 이 전략은 개발자가 원치않은 또다른 끔찍한 에러를 발생시킬 수 있기 때문에 트랜잭션의 동작 원리 / 방식에 대한 명확한 이해가 필요합니다.결론의도치 않은 롤백 상황이 일어나는 이유와 해결방법에 대해 알아보았습니다.막상 일이 일어났을때는 조금 당황했지만 이유를 알고보니 정말 멋지고 안전한 전략이라는 생각이 들었습니다.스프링의 동작방식에 대해 하나둘 알아가면 갈수록 참 재밌는 것 같습니다. 동시에 좌절감을 주기도 하지요..해결 방법에 대해서는 몇가지 적긴 했지만 저는 비즈니스 로직에 예외를 발생시켜야 하는 경우 해당 예외를 Exception을 상속받은 Checked Exception으로 만들고,확실한 복구 전략을 세워 명확한 예외처리를 하는 방법을 추천드립니다." }, { "title": "커맨드 패턴 (Command Pattern)", "url": "/posts/command-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-07-24 20:12:00 +0900", "snippet": "커맨드 패턴커맨드 패턴은 실행될 기능을 객체의 형태로 캡슐화 함으로써 여러 기능을 실행할 수 있는 재사용성이 높은 클래스를 설계하는 패턴 입니다. 이 패턴은 어떤 이벤트가 발생했을 때 실행될 기능이 다양하면서 여러가지 변경이 필요한 경우에이벤트를 발생시키는 클래스를 변경하지 않고 재사용 할 때 유용합니다.구조 호출자는 요청을 시작하는 역할을 합니다. 이 클래스에는 명령 객체를 참조하는 필드가 있어야 합니다. 호출자는 요청을 수신자에게 직접 보내는 대신 해당 명령을 트리거합니다. 커맨드 인터페이스는 대체로 명령을 실행하기 위한 단일 메소드만 선언합니다. 커맨드 터페이스의 구현 클래스는 다양한 종류의 요청들을 상속합니다. 구현 클래스는 자체적으로 작업을 수행하는 것이 아니라 비즈니스 로직 객체중 하나로 이러한 명령을 전달해야 합니다. 명령을 전달받는 수신 클래스에는 비즈니스 로직을 포함하고 있습니다. 거의 모든 객체가 수신자처럼 동작될 수 있습니다. 클라이언트 코드는 구체적인 명령 객체를 만들고 구성합니다. 클라이언트는 커맨드 인터페이스의 생성자에 수신 클래스의 인스턴스를 포함한 모든 요청 파라미터를 전달해야 합니다.적용 가능한 경우 어떤 작업과 함께 객체를 파라미터화 하려는 경우 커맨드 패턴을 사용할 수 있습니다. 커맨드 패턴은 특정 메소드 호출을 독립 실행형 객체로 변환할 수 있습니다. 큐에 작업을 넣거나 작업을 스케쥴링, 혹은 작업을 원격으로 실행하고자 한 경우 커맨드 패턴을 사용할 수 있습니다. 롤백 가능한 작업을 구현하고자 하는 경우 커맨트 패턴을 사용할 수 있습니다.장단점장점 어떤 작업을 수행하는 클래스에서 작업을 수행하는 부분만 분리할 수 있기 때문에 단일 책임 원칙을 만족합니다. 기존 클라이언트 코드를 건드지 않고 새로운 명령을 구현할 수 있기 때문에 개방/폐쇄 원칙을 만족합니다. 취소 / 다시 실행 기능을 구현할 수 있습니다. 지연된 작업을 실행 시킬 수 있도록 구현할 수 있습니다. 간단한 N개의 명령 세트를 복잡한 명령으로 조합할 수 있습니다.단점 발신하는 코드와 수신하는 코드 사이에 완전히 새로운 계층을 도입하기 때문에 전체적인 코드가 더 복잡해질 수 있습니다.예제현재 4가지의 기능을 하는 리모콘을 가지고 있습니다. 코드는 다음과 같습니다.Button.javapublic enum Button { UP, DOWN, LEFT, RIGHT;}RemoteControl.javapublic class RemoteControl { List&lt;Integer&gt; channelList = new ArrayList&lt;&gt;(Arrays.asList(1, 2, 7, 9, 11, 16, 20, 23, 27, 30)); int channel = 1; int volume = 5; public void volumeUp() { if (volume &lt; 10) { System.out.println(\"현재 볼륨은 \" + ++volume + \"입니다.\"); } else { System.out.println(\"볼륨을 더이상 높일 수 없습니다.\"); } } public void volumeDown() { if (volume &gt; 0) { System.out.println(\"현재 볼륨은 \" + --volume + \"입니다.\"); } else { System.out.println(\"볼륨을 더이상 낮출 수 없습니다.\"); } } public void channelUp() { int currentChannelIndex = channelList.indexOf(channel); if (currentChannelIndex == channelList.size() - 1) { channel = channelList.get(0); } else { channel = channelList.get(currentChannelIndex + 1); } System.out.println(\"현재 채널은 \" + channel + \"번 입니다.\"); } public void channelDown() { int currentChannelIndex = channelList.indexOf(channel); if (currentChannelIndex == 0) { channel = channelList.get(channelList.size() - 1); } else { channel = channelList.get(currentChannelIndex - 1); } System.out.println(\"현재 채널은 \" + channel + \"번 입니다.\"); }}RemoteControlCommander.javapublic class RemoteControlCommander { private RemoteControl remoteControl = new RemoteControl(); public void press(Button button) { if (button == Button.UP) { remoteControl.volumeUp(); } else if (button == Button.DOWN) { remoteControl.volumeDown(); } else if (button == Button.LEFT) { remoteControl.channelDown(); } else if (button == Button.RIGHT) { remoteControl.channelUp(); } }}Client.javapublic class Client { public static void main(String[] args) { RemoteControlCommander remoteControlCommander = new RemoteControlCommander(); remoteControlCommander.press(Button.UP); remoteControlCommander.press(Button.DOWN); remoteControlCommander.press(Button.LEFT); remoteControlCommander.press(Button.RIGHT); }}리모콘의 위 아래 버튼을 누르면 볼륨을 컨트롤하고, 좌 우 버튼을 누르면 채널을 컨트롤하는 간단한 리모콘입니다.지금까지는 문제가 없었으나 새로운 버튼을 20개 정도 추가하라는 지시를 받았습니다. 그래서 우리는 RemoteControlCommander클래스의 press() 메소드에 다른 Button이 들어올 경우를 대비하여 20 개의 if-else 문을 추가해야 합니다. (혹은 switch-case문)또한 RemoteControl클래스에 20개의 메소드를 추가해야 합니다. 이는 개방/폐쇄 원칙과 단일 책임 원칙을 충족하지 않습니다. 그래서 커맨드 패턴을 사용해 코드를 리팩토링 하기로 결정하였습니다.일단 Command 인터페이스 부터 생성하겠습니다. 인터페이스, 추상 클래스 둘중 본인의 스타일대로 생성하면 됩니다. 인터페이스는 Command 의 구현체에서 RemoteControl을 주입해야 합니다.추상 클래스는 Command 자체에서 RemoteControl을 주입해야 합니다.Command.javapublic interface Command { void execute();}다음은 기존의 RemoteControl 클래스를 변경하겠습니다. 이 클래스는 이제 어떤 행위를 하는 메소드는 없고 필드를 컨트롤 할 수 있는 getter / setter 메소드만 존재할 것입니다.public class RemoteControl { private List&lt;Integer&gt; channelList = new ArrayList&lt;&gt;(Arrays.asList(1, 2, 7, 9, 11, 16, 20, 23, 27, 30)); private int channel = 1; private int volume = 5; public List&lt;Integer&gt; getChannelList() { return channelList; } public int getChannel() { return channel; } public void setChannel(int channel) { this.channel = channel; } public int getVolume() { return volume; } public void setVolume(int volume) { this.volume = volume; }}다음은 커맨드 인터페이스를 구현하는 구현체 클래스를 만들겠습니다.VolumeUpCommand.javapublic class VolumeUpCommand implements Command{ private RemoteControl remoteControl; public VolumeUpCommand(RemoteControl remoteControl) { this.remoteControl = remoteControl; } @Override public void execute() { if (remoteControl.getVolume() &lt; 10) { remoteControl.setVolume(remoteControl.getVolume() + 1); System.out.println(\"현재 볼륨은 \" + remoteControl.getVolume() + \"입니다.\"); } else { System.out.println(\"볼륨을 더이상 높일 수 없습니다.\"); } }}VolumeDownCommand.javapublic class VolumeDownCommand implements Command{ private RemoteControl remoteControl; public VolumeDownCommand(RemoteControl remoteControl) { this.remoteControl = remoteControl; } @Override public void execute() { if (remoteControl.getVolume() &gt; 0) { remoteControl.setVolume(remoteControl.getVolume() - 1); System.out.println(\"현재 볼륨은 \" + remoteControl.getVolume() + \"입니다.\"); } else { System.out.println(\"볼륨을 더이상 낮출 수 없습니다.\"); } }}ChannelUpCommand.javapublic class ChannelUpCommand implements Command{ private RemoteControl remoteControl; public ChannelUpCommand(RemoteControl remoteControl) { this.remoteControl = remoteControl; } @Override public void execute() { int currentChannelIndex = remoteControl.getChannelList().indexOf(remoteControl.getChannel()); if (currentChannelIndex == remoteControl.getChannelList().size() - 1) { remoteControl.setChannel(remoteControl.getChannelList().get(0)); } else { remoteControl.setChannel(remoteControl.getChannelList().get(currentChannelIndex + 1)); } System.out.println(\"현재 채널은 \" + remoteControl.getChannel() + \"번 입니다.\"); }}ChannelDownCommand.javapublic class ChannelDownCommand implements Command{ private RemoteControl remoteControl; public ChannelDownCommand(RemoteControl remoteControl) { this.remoteControl = remoteControl; } @Override public void execute() { int currentChannelIndex = remoteControl.getChannelList().indexOf(remoteControl.getChannel()); if (currentChannelIndex == 0) { remoteControl.setChannel(remoteControl.getChannelList().get(remoteControl.getChannelList().size() - 1)); } else { remoteControl.setChannel(remoteControl.getChannelList().get(currentChannelIndex - 1)); } System.out.println(\"현재 채널은 \" + remoteControl.getChannel() + \"번 입니다.\"); }}커맨드 인터페이스를 구현하는 구현 클래스를 모두 생성하였습니다. 마지막으로 기존 RemoteControlCommander 클래스를 삭제하고 클라이언트 코드를 변경하겠습니다.Client.javapublic class Client { public static void main(String[] args) { RemoteControl remoteControl = new RemoteControl(); VolumeUpCommand volumeUpCommand = new VolumeUpCommand(remoteControl); VolumeDownCommand volumeDownCommand = new VolumeDownCommand(remoteControl); ChannelUpCommand channelUpCommand = new ChannelUpCommand(remoteControl); ChannelDownCommand channelDownCommand = new ChannelDownCommand(remoteControl); volumeUpCommand.execute(); volumeDownCommand.execute(); channelUpCommand.execute(); channelDownCommand.execute(); }}커맨드 패턴을 적용한 코드 리팩토링이 완료되었습니다. 이제 새로운 버튼이 생기더라도 커맨드 인터페이스를 상속한 구현 클래스를 새롭게 만들고 클라이언트만 수정하면 됩니다.즉, 다른 코드를 건들지 않아도 되므로 계방 폐쇄 원칙을 만족합니다. 물론 RemoteControl 클래스에는 필요한 필드들이 생길 수 있습니다. (ex. 외부입력 포트) 그렇다 하더라도기존의 한 클래스에 모든 메소드를 넣은 방식에 비해 클래스가 필수적으로 가지고 있어야 하는 필드, 메소드들만 가지고 있게 변경되었으므로 단일 책임 원칙을 만족하게 됩니다.결론커맨드 패턴은 주로 GUI 프로그래밍에서 많이 쓰이곤 합니다. 어떤 버튼을 눌렀을 때 해당 버튼이 실행할 동작을 구현하고자 할때 주로 사용하지요.또한 자바 프로그래밍을 하며 쉽게 접할 수 있는 Runnable 인터페이스의 모든 구현체도 커맨드 패턴이 적용된 대표적인 사례입니다.이렇듯 많은 곳에서 사용되지만 모든 패턴들이 그렇듯 커맨드 패턴 또한 잘못된 설계로 개발이 진행되었을 때 오히려 코드가 더 복잡해질 수 있으니 주의가 필요합니다.참조 https://refactoring.guru/design-patterns/commandhttps://sourcemaking.com/design_patterns/command/java/1" }, { "title": "반복자 패턴 (Iterator Pattern)", "url": "/posts/iterator-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-07-17 20:12:00 +0900", "snippet": "반복자 패턴반복자 패턴은 객체지향 프로그래밍에서 반복자를 사용하여 컨테이너를 가로지르며 컨테이너의 요소들에 접근하는 디자인 패턴입니다… 라고 위키에는 나와 있습니다만 한번에 이해되지는 않습니다.String[] stringArray = {\"월\", \"화\", \"수\", \"목\", \"금\", \"토\", \"일\"};for (int i = 0; i &lt; stringArray.length; i++) { System.out.println(stringArray[i]);}위와같은 코드가 있을 때 컨테이너는 stringArray 배열을 지칭하고 반복자는 변수 i를 지칭합니다. 여기서 중요한것은 변수 i 인데요, i는 다음과 같은 기능을 합니다. 현재 가로지르고 있는 배열의 위치를 나타낸다. 값을 증가시켜 컨테이너의 모든 요소들에 접근할수 있도록 해준다. 위 코드에서 i가 하고 있는 기능을 추상화 하여 일반화 한 것을 반복자 패턴이라고 합니다.더 쉽게 풀어쓰자면 각기 다른 자료구조(컬렉션)가 있다고 가정했을 때 이 각각의 자료구조에 접근할 수 있는 방법을 모두 알 필요 없이 반복기능만 통일하여 일반화 한 패턴 정도로 이해하시면 될 것 같습니다.반복자 패턴에서 중요하게 여겨지는 원칙은 SRP(단일 책임 원칙) 입니다. 단일 책임 원칙이란클래스는 하나의 책임만 가져야 하며, 클래스를 변경하는 이유도 한개여야 함을 의미합니다.구조 반복자 인터페이스는 컬렉션 순회에 필요한 적업(다음 요소 가져오기, 현재 위치 검색, 반복 재시작 등) 을 선언합니다. 반복자 인터페이스의 구현체는 컬렉션 순회를 위한 알고리즘을 구현합니다. 반복자 객체는 자체적으로 진행중인 순회를 추적해야 합니다. 컬렉션 인터페이스는 컬렉션과 호환되는 반복자를 얻기 위해 하나 이상의 메소드를 선언합니다. 컬렉션 인터페이스의 구현체는 클라이언트가 요청할 때마다 특정 구현체 클래스의 새로운 인스턴스를 반환합니다. 클라이언트는 인터페이스를 통해 반복자, 컬렉션을 사용할 수 있습니다. 이렇게 하면 클라이언트가 구현 클래스와 결합되지 않으므로 동일한 클라이언트 코드로 다양한 반복자와 컬렉션을 사용할 수 있습니다.적용 가능한 경우 컬렉션이 내부적으로 복잡한 데이터 가지고 있지만 그 복잡성을 클라이언트로부터 숨기고 싶을 때 반복자 패턴을 사용합니다. 반복자는 복잡한 데이터 작업에 대한 세부 정보를 캡슐화하여 클라이언트에 컬렉션 요소에 액세스하는 몇 가지 간단한 방법을 제공합니다. 이를 통해 클라이언트는 컬렉션의 복잡성을 알 수 없으므로 부주의하거나 악의작인 작업을 수행할 수 없습니다. 앱 전체에서 순회 코드의 중복을 줄이고자 할 때 반복자 패턴을 사용합니다. 클라이언트 코드가 다른 데이터 구조를 순회하기를 원하거나 이러한 구조의 유형을 미리 알 수 없는 경우 반복자 패턴을 사용합니다.장단점장점 크기가 큰 순회 알고리즘을 별도의 클래스로 추출하여 클라이언트 코드와 컬렉션을 정리함으로 인해 단일 책임 원칙을 만족할 수 있습니다. 새로운 유형의 컬렉션 및 반복자를 구현하고 기존의 코드는 수정하지 않음으로써 개방/폐쇄 원칙을 만족할 수 있습니다. 각각의 반복자 객체는 그들만의 고유한 반복 상태가 포함되어 있기 때문에 동일한 컬렉션을 병렬로 반복할 수 있습니다. 위와 같은 이유로 반복을 지연시킬 수 있고 필요할 때 계속할 수 있습니다.단점 앱이 간단한 컬렉션 만으로도 동작하는 경우 패턴을 적용하는 것이 오히려 지나칠 수 있습니다. 반복자를 사용하는 것은 일부 특수 컬렉션의 요소를 직접 탐색하는 것 보다 덜 효율적일 수 있습니다.예제사용자 컬렉션의 정보를 모두 가져와 콘솔창에 보여주는 프로그램을 하나 만들어 보겠습니다.우리가 확인할 유저 객체는 다음과 같습니다.public class User { private String name; private int age; private String loginId; public User(String name, int age, String loginId) { this.name = name; this.age = age; this.loginId = loginId; } public String getName() { return name; } public int getAge() { return age; } public String getLoginId() { return loginId; }}그 후 두개의 인터페이스를 만들겠습니다.public interface Iterator { boolean hasNext(); Object next();}public interface Container { Iterator getIterator();}다음은 각각의 인터페이스를 구현하는 구현체를 만들겠습니다.public class UserIterator implements Iterator { private List&lt;User&gt; userList; int currentPosition = 0; public UserIterator(List&lt;User&gt; userList) { this.userList = userList; } @Override public boolean hasNext() { return currentPosition &lt; userList.size(); } @Override public Object next() { if (hasNext()) { return userList.get(currentPosition ++); } return null; }}public class UserRepository implements Container { private List&lt;User&gt; userList; public UserRepository(List&lt;User&gt; userList) { this.userList = userList; } @Override public Iterator getIterator() { return new UserIterator(this.userList); }}마지막으로 클라이언트 코드입니다. 이 코드는 모든 사용자의 정보를 출력합니다.public class Client { public static void main(String[] args) { UserRepository userRepository = new UserRepository(getTestUserList()); for (Iterator it = userRepository.getIterator(); it.hasNext();) { System.out.println(\"===========================================\"); User user = (User) it.next(); System.out.println(user.getName()); System.out.println(user.getAge()); System.out.println(user.getLoginId()); System.out.println(\"===========================================\"); } } private static List&lt;User&gt; getTestUserList() { return new ArrayList&lt;&gt;( Arrays.asList( new User(\"홍길동\", 20, \"gildong\"), new User(\"이순신\", 50, \"sunshin\"), new User(\"김첨지\", 43, \"chumji\"), new User(\"신사임당\", 20, \"imgentleman\") ) ); }}모든 사용자의 정보가 콘솔에 출력되면 성공입니다.일반적인 반복문과의 차이만약 위 예제를 Iterator를 사용하지 않고 foreach loop를 사용하면 어떻게 될까요? public static void main(String[] args) { List&lt;User&gt; userList = getTestUserList(); for (User user : userList) { System.out.println(\"===========================================\"); System.out.println(user.getName()); System.out.println(user.getAge()); System.out.println(user.getLoginId()); System.out.println(\"===========================================\"); } }동작 자체는 Iterator를 사용하였을 때와 동일할 것이고 코드 양도 줄었습니다. 하지만 Iterator는 foreach loop가 할 수 없는 일들을 할 수 있습니다.예를들어 요소 제거 를 할 수 있습니다. 일반적인 foreach 문으로 요소를 제거할 경우 ConcurrentModificationException 예외가 발생하는데 반해, Iterator를 이용하여 요소를 제거할 경우 안전하게 요소를 제거할 수 있습니다.자바 8 이상의 버전을 사용한다면 새로운 메소드(removeIf), Stream API(filter) 를 사용해도 안전하게 요소를 제거할 수 있습니다. 이러한 일이 발생하는 이유는 일반 for, foreach 문의 경우 반복자가 hasNext(), next() 에 대해 반환할 내용을 모르기 때문입니다.List&lt;User&gt; userList = getTestUserList();// OKfor (Iterator&lt;User&gt; iterator = userList.iterator(); iterator.hasNext();) { User user = iterator.next(); if (\"gildong\".equals(user.getLoginId())) { iterator.remove(); }}// ConcurrentModificationException Errorfor (User user : userList) { if (\"gildong\".equals(user.getLoginId())) { userList.remove(user); }}// OKuserList.removeIf(user -&gt; \"gildong\".equals(user.getLoginId()));결론반복자 패턴을 사용하면 구현 클래스 내부에서 어떤 식으로 일이 처리되는지 알 필요 없이(자료구조와 관계없이) 컬렉션 내부의 모든 항목에 접근할 수 있습니다. 다만 Iterator를 사용하는 경우 for loop를 사용하는 것 보다 성능이 떨어질 수 있으니 주의가 필요합니다.참조 https://refactoring.guru/design-patterns/iteratorhttps://stackoverflow.com/a/22268270/13160032" }, { "title": "옵저버 패턴 (Observer Pattern)", "url": "/posts/observer-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-07-09 20:12:00 +0900", "snippet": "옵저버 패턴옵저버 패턴은 객체의 상태 변화를 관찰하는 관찰자들, 즉 옵저버의 목록을 객체에 등록하여 상태 변화가 있을 때마다 메스드 등을 통해 객체가 직접 목록의 각 옵저버에게 통지하도록 하는 디자인 패턴입니다.구조 게시자는 다른 객체들에게 관심 이벤트를 발행합니다. 이 이벤트난 게시자가 상태를 변경하거나 어떤 동작을 실행할 때 발생합니다.게시자에는 새 구독자가 가입하고 현재 구독자가 목록에서 나갈 수 있도록 하는 구독 인프라가 포함되어 있습니다. 새 이벤트가 발생하면 게시자는 구독 목록을 살펴보고 각 구독차 객체의 인터페이스에 선언된 알림 메소드를 호출합니다. 구독자는 구독 인터페이스를 선언합니다. 대부분의 경우 단일 update 방법으로 구성됩니다. 메소드에는 게시자가 업데이트와 함께 일부 이벤트 세부 정보를 전달할 수 있도록 하는 매개 변수가 있을 수 있습니다. 구독자는 게시자가 발행한 알림에 대해 여러가지 작업을 수앻합니다. 게시자가 구현클래스와 결합되지 않도록 모든 클래스는 동일한 인터페이스로 구현되어야 합니다. 대부분의 경우 구독자는 업데이트를 올바르게 처리하기 위해 전후 정보가 필요합니다. 이러한 이유로 게시자는 알림 메소드의 인수로 전후 정보를 전달하곤 합니다.게시자는 자기 자신을 인자로 전달하여 구독자가 필요한 데이터를 직접 가져올 수 있습니다. 클라이언트는 게시자 및 구독자 객체를 별도로 만든 후 게시자 업데이트를 위해 구독자를 등록합니다.적용 가능한 경우 한 객체의 상태를 변경하기 위해 다른 객체를 변경해야할 경우 혹은 실제 객체 집합을 미리 알 수 없거나 동적으로 변경되는 경우 옵저버 패턴을 사용합니다. 앱이 한정된 시간, 특정한 케이스에만 다른 객체를 관찰해야 하는 경우 옵저버 패턴을 사용합니다.장단점장점 게시자의 코드를 변경하지 않고 새로운 구독자 클래스를 만들어 작업할 수 있으므로 개방/폐쇄 원칙을 만족합니다. 런타임 시점에 객체와 관계를 맺을 수 있습니다.단점 구독자는 랜덤으로 알림을 받습니다. 우선순위를 지정하는 코드를 삽입하여 순위를 지정할수도 있겠지만, 복잡성과 결합성만 높아지기 때문에 추천하지는 않습니다. 예제기본적인 형태의 게시자부터 만들어 보겠습니다.public class EventManager { Map&lt;String, List&lt;EventListener&gt;&gt; listeners = new HashMap&lt;&gt;(); public EventManager(String... operations) { for (String operation : operations) { this.listeners.put(operation, new ArrayList&lt;&gt;()); } } public void subscribe(String eventType, EventListener listener) { List&lt;EventListener&gt; users = listeners.get(eventType); users.add(listener); } public void unsubscribe(String eventType, EventListener listener) { List&lt;EventListener&gt; users = listeners.get(eventType); users.remove(listener); } public void notify(String eventType, File file) { List&lt;EventListener&gt; users = listeners.get(eventType); for (EventListener listener : users) { listener.doAction(eventType, file); } }}위 코드는 게시자입니다. 구독자를 listners 변수로 받고 구독, 구독해지, 알림의 기능을 메소드로 가지고 있습니다.public class Editor { public EventManager events; private File file; public Editor() { this.events = new EventManager(\"open\", \"save\"); } public void openFile(String filePath) { this.file = new File(filePath); events.notify(\"open\", file); } public void saveFile() throws Exception { if (this.file != null) { events.notify(\"save\", file); } else { throw new Exception(\"파일을 먼저 열어주세요.\"); } }}위 코드는 클라이언트가 사용할 메소드를 모아놓은 게시자 클래스입니다. 파일을 열고 저장하는 기능을 내포하고 있습니다.public interface EventListener { void update(String eventType, File file);}public class LogOpenListener implements EventListener { private File log; public LogOpenListener(String fileName) { this.log = new File(fileName); } @Override public void update(String eventType, File file) { System.out.println(log + \" 파일을 열었고 로그에 추가하였습니다.\"); }}public class EmailNotificationListener implements EventListener { private String email; public EmailNotificationListener(String email) { this.email = email; } @Override public void update(String eventType, File file) { System.out.println(eventType + \" 으로 인해 \" + email + \" 로 이메일을 전송하였습니다.\"); }}위 코드는 구독자 코드들입니다. 옵저버 인터페이스가 있고 옵저버 인터페이스를 구현하는 클래스 2개가 있습니다.public class Client { public static void main(String[] args) { Editor editor = new Editor(); editor.events.subscribe(\"open\", new LogOpenListener(\"D:\\\\file.txt\")); editor.events.subscribe(\"save\", new EmailNotificationListener(\"admin@example.com\")); try { editor.openFile(\"test.txt\"); editor.saveFile(); } catch (Exception e) { e.printStackTrace(); } }}마지막으로 클라이언트 코드입니다. 에디터를 생성하고 파일을 open 할 경우 LogOpenListener을 통해 해당 파일이 열렸다는 정보를 로그에 저장합니다.파일을 save할 경우 파일이 저장되었다는 정보를 미리 지정해둔 이메일(admin@example.com) 에 전송합니다.결론옵저버 패턴은 주로 gui 프로그램에서 사용되곤 합니다. 중요한점은 한 객체가 변경되었음을 다른 객체에 알려야 하는 경우 게시자(Publisher), 구독자(Observer), 알림(Notify)의 형태로 구현한다는 점입니다.참조 https://refactoring.guru/design-patterns/observerhttps://refactoring.guru/design-patterns/observer/java/example" }, { "title": "싱글턴 패턴 (Singleton Pattern)", "url": "/posts/singleton-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-07-06 20:12:00 +0900", "snippet": "싱글턴 패턴싱글턴 패턴은 생성자가 여러 차례 호출되더라도 실제로 생성되는 객체는 하나이고 최초 생성 이후 호출된 생성자는최초의 생성자가 생성한 객체를 리턴하는 디자인 유형입니다.구조 싱글턴 클래스는 자체 클래스의 동일한 인스턴스를 반환하는 getInstance() 라는 정적 메소드를 선언했습니다. 싱글턴의 생성자는 클라이언트 코드에서는 확인할 수 없도록 숨겨야 합니다. getInstance() 메소드를 호출하는 것 만이 싱글턴 객체를 가져올 수 있는 유일한 방식이어야 합니다. 적용 가능한 경우 프로그램 클래스에 모든 클라이언트가 사용할 수 있는 한개의 인스턴스만 존재해야 하는 경우 싱글턴 패턴을 사용합니다. 예를들어 위에 언급했듯이 DBCP가 있습니다. 싱글턴 패턴은 특별한 방법을 제외하고 클래스의 객체를 생성하는 다른 모든 수단을 차단합니다. 이 메소드는 이미 생성된 객체가 있는 경우 기존 객체를 반환하고 그렇지 않은 경우에만 새 겍체를 생성합니다. 전역 변수를 더 엄격하게 컨트롤해야 하는 경우 싱글턴 패턴을 사용합니다. 전역 변수와 달리 싱글턴 패턴은 클래스의 인스턴스가 하나만 있음을 보장합니다. 싱글턴 클래스 자체를 제외하고 이미 캐시된 인스턴스를 대체할 수 있는 것은 없습니다. 장단점장점 클래스에 하나의 인스턴스만 존재함을 확실시 할 수 있습니다. 해당 인스턴스에 대한 전역 엑세스 포인트를 얻습니다. 싱글턴 객체는 첫 요청시에만 초기화 됩니다.단점 전역 엑세스 포인트를 얻기 때문에 단일 책임 원칙 (Single Responsibility Principle)을 위반합니다. 너무 많은 곳에서 사용할 경우 잘못된 디자인 형태가 될 수도 있습니다. 멀티 스레드 환경에서 특별한 처리가 필요하므로 멀티 스레드 환경에서 싱글턴 객체를 여러번 생성하지 않습니다. 많은 테스트 프레임워크가 Mock 객체를 생성할 때 상속에 의존하기 때문에 싱글턴의 클라이언트 코드를 테스트하기 어렵습니다.예제간단한 싱글턴을 구현해보겠습니다. 단지 할 일은 생성자를 숨기고 정적으로 생성할 메소드를 구현하면 됩니다.Singleton.java: 싱글턴 객체public class Singleton { private static Singleton instance; public String value; private Singleton(String value) { try { Thread.sleep(1000); } catch (InterruptedException ex) { ex.printStackTrace(); } this.value = value; } public static Singleton getInstance(String value) { if (instance == null) { instance = new Singleton(value); } return instance; }}SingleThreadSingleton.java: 클라이언트 코드public class SingleThread { public static void main(String[] args) { System.out.println(\"같은 값을 보게 된다면 싱글턴 객체가 재사용된 것입니다..!!\" + \"\\n\" + \"다른 값을 보게 된다면 두개의 싱글턴 객체가 생성된 것입니다..!!\"); Singleton singleton = Singleton.getInstance(\"x\"); Singleton anotherSingleton = Singleton.getInstance(\"y\"); System.out.println(singleton.value); System.out.println(anotherSingleton.value); }}위 코드를 실행해보면 다음과 같은 결과를 얻을 수 있습니다.같은 값을 보게 된다면 싱글턴 객체가 재사용된 것입니다..!!다른 값을 보게 된다면 두개의 싱글턴 객체가 생성된 것입니다..!!xx결과값이 같은 것으로 보아 싱글턴 객체가 재사용된 것을 확인할 수 있습니다.위 첫번째 예제는 싱글 스레드 환경에서 싱글턴 패턴을 사용한 경우입니다. 만약 멀티 스레드 환경에서 싱글턴 패턴을 구현하고자 한다면, 두개 이상의 스레드가 인스턴스를 획득하기 위해 getInstance() 메소드에 진입하여 경쟁하는 과정에 두개의 인스턴스가 생성되는일이 발생할 수 있습니다.그렇다면 멀티 스레드 환경에서 싱글턴 패턴을 사용하고자 할 때 어떤식으로 구현해야 할까요? 예제를 통해 알아보겠습니다.첫번째 방법은 DCL(Double Checked Locking) 입니다. 사실 그다지 권고되지 않는 방법입니다.public static Singleton getInstance(String value) { if (instance == null) { synchronized(Singleton.class) { Singleton inst = instance; if (inst == null) { synchronized(Singleton.class) { instance = new Singleton(value); } } } } return instance;}인스턴스가 null 일 경우 동기화 블럭에 진입해 인스턴스를 생성합니다. 언뜻보기 문제없어 보이는 코드이지만 다음과 같은 문제가 발생할 수 있습니다. 스레드 a, 스레드 b 가 존재한다. 스레드 a가 인스턴스를 생성하기 전에 메모리 공간에 할당이 가능하기 때문에 스레드 b가 인스턴스를 생성하기 위해 진입한다. 스레드 a는 스레드 b가 할당된 것을 보고 인스턴스를 사용하려고 하나 스레드 b의 인스턴스 생성과정이 끝난 상태가 아닐수 있기 떄문에 에러가 발생한다. 위와 같은 문제가 있어 동기화 블록의 사용을 추천하지는 않고 LazyHolder기법 사용을 추천합니다.public class Singleton { private static class LazyHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton() { } public static Singleton getInstance() { return LazyHolder.INSTANCE; }}위 예제는 getInstance() 가 호출될때만 싱글턴 클래스를 초기화하고 100% Thread-safe 합니다. 심지어 클래식한 방법이어서 자바 버전에 구애받지 않고 성능도 꽤 높습니다.위 코드가 동작하는 이유는 클래스 로더가 클래스의 정적 초기화를 처리하기 위한 자체 동기화 코드를 갖고 있기 때문입니다. 클래스가 사용되기 전에 정적 초기화가 완료되었음을 보장받을 수 있으며 위 코드에서 클래스는 getInstance() 메소드에서만 사용됩니다.이는 getInstance() 메소드를 호출할 때 클래스가 내부 클래스를 로드함을 의미합니다.결론싱글턴 패턴은 인스턴스가 한개만 생성되어야 하는 경우에 적합한 패턴입니다. 다만 클라이언트 코드에서 전역 엑세스 포인트를 얻어 너무 많은 곳에서 사용하면 클래스간의 결합도가 높아져 오히려 패턴을 사용 안하느니만 못하게 될 수도 있으므로 주의가 필요합니다.참조 https://refactoring.guru/design-patterns/singletonhttps://stackoverflow.com/a/11072311/13160032" }, { "title": "프록시 패턴 (Proxy Pattern)", "url": "/posts/proxy-pattern/", "categories": "Design Pattern", "tags": "Design Pattern, Java", "date": "2021-07-03 20:12:00 +0900", "snippet": "프록시 패턴프록시(Proxy) 는 대리자 라는 뜻입니다. 실생활서의 의미처럼 프로그램에서의 프록시도 누군가에게 어떤 일을 대신 시키는 것 이란 의미를 가지고 있습니다.때때로 우리는 객체에 대한 액세스를 제어하는 기능을 필요합니다. 예를 들어 무겁고 많은 자원을 필요로 하는 클래스의 한두가지 메소드만 사용해야 하더라도 생성자로 전체 클래스를 인스턴스화 합니다.이 시점에 프록시 패턴을 사용할 수 있습니다. 무거운 객체가 상속하고 있는 인터페이스를 상속하는 가벼운 객체를 하나 만드는 방식입니다.이때 가벼운 객체를 프록시 라고 하며 우리는 이 가벼운 객체를 인스턴스화하여 사용하고 실제 무거운 행위를 하는 메소드는 정말 그 메소드가 필요한 시점에 호출할 것입니다.적용 가능한 경우 지연 초기화(가상 프록시) - 가끔 필요하지만 항상 메모리에 적재되어 있는 무거운 서비스 객체가 있는 경우 서비스가 시작될 때 객체를 생성하는 대신에 객체 초기화가 실제로 필요한 시점에 초기화될수 있도록 지연할 수 있습니다. 액세스 제어(보호 프록시) - 특정 클라이언트만 서비스 객체에 접근 가능하도록 하려는 경우 프록시 객체를 통해 클라이언트의 자격 증명이 기준과 일치하는 경우에만 서비스 객체에 요청을 전달할 수 있습니다. 원격 서비스의 로컬 실행(리모트 프록시) - 서비스 객체가 원격 서버에 위치해 있는 경우 프록시 객체는 네트워크를 통해 클라이언트의 요청을 전달하여 네트워크와 관련된 불필요한 작업들을 처리하고 결과값만 반환합니다. 로깅 요청(로깅 프록시) - 서비스 객체에 대한 기록을 유지하려는 경우 프록시 객체는 서비스 객체에 요청을 전달하기 요청을 기록할 수 있습니다. 캐싱 요청 결과(캐싱 프록시) - 서비스 객체의 결과값이 큰 경우 프록시는 클라이언트 요청의 결과를 캐싱하고 이 캐시의 수명 주기를 관리해야 하는 경우 프록시는 항상 동일한 결과를 생성하는 반복 요청에 대해 캐싱을 구현할 수 있습니다. 또 클라이언트 요청의 매개변수를 캐시 키로 사용할 수 있습니다. 스마트 참조 - 무거운 객체를 사용하는 클라이언트가 없을 때 메모리에서 해제해야 하는 경우 프록시는 서비스 객체의 결과에 대한 참조를 얻은 클라이언트를 추저할 수 있습니다. 때때로 프록시는 클라이언트로 넘어와 아직 클라이언트가 활성 상태인지 확인합니다.만약 클라이언트의 리스트가 비어있을 경우 프록시는 서비스 객체를 종료하고 불필요한 시스템 자원을 메모리에서 해제할 수 있습니다. 장단점장점 클라이언트가 알지 못하는 상태에서 서비스 객체를 제어할 수 있습니다. 클라이언트가 신경쓰지 않을 때 서비스 객채의 생명 주기를 관리할 수 있습니다. 프록시는 서비스 객체가 준비되지 않았거나 사용할 수 없을때도 작동합니다. OCP(개방-폐쇄 원칙)에 따라 서비스나 클라이언트를 변경하지 않고 새 프록시를 생성할 수 있습니다.단점 많은 프록시 클래스를 생성해야 하므로 코드가 더 복잡해질 수 있습니다. 프록시 클래스 자체에 들어가는 자원이 많다면 서비스로부터의 응답이 오히려 늦어질 수 있습니다.예제이미지 뷰어 프로그램을 만들어보겠습니다. 이미지 뷰어는 다음과 같은 동작을 수행합니다. 이미지 뷰어는 고해상도의 이미지를 불러와 사용자에게 보여줘야 합니다.사용자가 목록에서 이미지를 선택하기 전까지 실제 이미지를 표시할 필요는 없습니다.다음 코드는 이미지 인터페이스 입니다. 이미지를 렌더링하기 위해 구현체가 구현해야 하는 추상메소드 showImage()가 있습니다.public interface Image { void showImage();}다음 코드는 우리가 현재 사용하고 있는 이미지 인터페이스의 구현체입니다. 디스크에서 고해상도 이미지를 불러와 메모리에 적재하고 showImage()가 호출되면 화면에 렌더링 합니다.public class HighResolutionImage implements Image { public HighResolutionImage(String path) { loadImage(path); } private void loadImage(String path) { // 이미지를 디스크에서 불러와 메모리에 적재 // 작업 자체가 무겁고 많은 자원을 필요로함 } @Override public void showImage() { // 이미지를 화면에 렌더링 }}다음 코드는 이미지 뷰어 메인 클래스입니다.public class ImageViewer { public static void main(String[] args) { Image highResolutionImage1 = new HighResolutionImage(\"highResolutionImage1\"); Image highResolutionImage2 = new HighResolutionImage(\"highResolutionImage2\"); Image highResolutionImage3 = new HighResolutionImage(\"highResolutionImage3\"); highResolutionImage2.showImage(); }}사용자가 3개의 이미지가 있는 폴더를 선택하였습니다. 사용자가 어떤 이미지를 선택할지 몰라 모든 이미지를 인스턴스화 하였습니다. 실제로는 highResolutionImage2 만 사용자에게 보여줘야 함에도 불구하고 HighResolutionImage 클래스의 생성자에 의해 모든 이미지는 loadImage() 메소드를 호출하게 됩니다.이 시점에서 불필요한 자원낭비가 발생하였습니다.자, 이제 이미지 인터페이스를 구현하는 가벼운 프록시 클래스를 만들어보겠습니다.public class ImageProxy implements Image { private String path; private Image proxyImage; public ImageProxy(String path) { this.path = path; } @Override public void showImage() { proxyImage = new HighResolutionImage(path); proxyImage.showImage(); }}이 클래스는 showImage() 메소드가 호출되는 시점에 HighResolutionImage 클래스를 인스턴스화 합니다.이미지 뷰어 메인 클래스의 코드를 변경하겠습니다.public class ImageViewer { public static void main(String[] args) { Image proxyHighResolutionImage1 = new ImageProxy(\"highResolutionImage1\"); Image proxyHighResolutionImage2 = new ImageProxy(\"highResolutionImage2\"); Image proxyHighResolutionImage3 = new ImageProxy(\"highResolutionImage3\"); proxyHighResolutionImage1.showImage(); }}객체의 타입은 Image 로 동일하지만 HighResolutionImage 클래스 대신 ImageProxy 객체로 인스턴스화 하였습니다.첫번째 메인 클래스의 경우와는 다르게 객체를 인스턴스화 하는 시점에 HighResolutionImage 클래스의 loadImage() 메소드를 호출하지 않습니다.proxyHighResolutionImage1.showImage()를 호출하는 시점에 loadImage() 메소드를 호출하게 됩니다. 실제 메소드를 호출하는 시점에 메모리 적재가 이루어지기 때문에 불필요한 자원낭비가 발생하지 않게 됩니다.결론프록시 패턴은 많은 곳에서 쓰이고 있습니다. 핵심은 클라이언트 -&gt; 프록시 -&gt; 실제 객체 로 구성된 구조라는 점입니다. 매우 간단한 패턴이지만 자칫 잘못하면 불필요한 코드로 오히려 복잡해 질수 있으니 적재적소에 활용하는 편일 좋을것 같습니다.참조 https://refactoring.guru/design-patterns/proxyhttps://www.oodesign.com/proxy-pattern.html" }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 4. 어플리케이션 구축 및 로드밸런서 적용", "url": "/posts/aws-cicd-4/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2021-06-27 20:12:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리어플리케이션 구축 및 로드밸런서 적용이번 포스팅에서는 간단한 Spring Boot 어플리케이션을 만들어 ec2 인스턴스에 배포하고 로드밸런서를 적용해보도록 하겠습니다. 이 시리즈의 서버 구성은 3-tier 가 아닌 presentation, logic 티어가 한 티어에 존재하는 일종의 2-tier 구성 입니다.3-tier 와의 차이점은 웹(nginx, apache 등)과 어플리케이션(react, spring boot 등)이 프록시 설정을 적용한 한개의 인스턴스에 존재한다는 점입니다.3-tier 로 구성하려면 presentation(web), logic(was) 티어 간의 통신에 NLB(Network Load Balancer)를 사용하면 되며 이 시리즈를 잘 이해하신다면 쉽게 적용하실 수 있을 것이라 생각됩니다.Spring Boot 어플리케이션 만들기현재 호스트ip와 접근ip를 리턴하는 메소드 한개를 가진 간단한 Spring Boot 어플리케이션을 만들었습니다.@RestController@SpringBootApplicationpublic class TestApplication { public static void main(String[] args) { SpringApplication.run(TestApplication.class, args); } @GetMapping(\"/ip\") public Map&lt;String, String&gt; ip(HttpServletRequest request) throws UnknownHostException { return new HashMap&lt;&gt;() {{ put(\"hostIp\", InetAddress.getLocalHost().getHostAddress()); put(\"accessIp\", request.getHeader(\"x-forwarded-for\")); }}; }}어플리케이션을 만들었다면 jar 파일(혹은 war 파일) 로 빌드 후 프라이빗 서브넷 내부 인스턴스에 옮겨둬야 합니다. 프라이빗 서브넷에 다이렉트로 접근하는 방법(SSH Tunneling 등)에 대해 모르고 계신다면 어쩔 수 없이Bastion Host에 한번 옮기고 각각의 인스턴스에 다시 파일을 전송해야 합니다. 저는 scp 명령어를 사용하여 파일을 전송하였습니다.scp -i key.pem test.jar -r private.ip:/home/ec2-user현재 프라이빗 서브넷 내부 인스턴스에는 아무 패키지도 설치되지 않은 상태입니다. 따라서 타임존을 세팅하고 필요 패키지를 설치하도록 하겠습니다. (Amazon Linux 2 기준)# 타임존 세팅rm /etc/localtimeln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime# nginx 설치amazon-linux-extras install nginx1 -ysystemctl enable nginxsystemctl start nginx# open jdk 11 설치amazon-linux-extras install java-openjdk11 -y타임존을 세팅하고 nginx, java를 설치하였습니다. 이제 옮긴 jar 파일을 실행하겠습니다.nohup java -jar -Dserver.port=8080 /home/ec2-user/test.jar 1&gt; /dev/null 2&gt;&amp;1 &amp;jar 파일이 정상적으로 실행되었는지 확인하기 위해 아까 만든 메소드를 curl 명령어를 이용해 호출해보겠습니다.curl -l localhost:8080/ip{\"accessIp\":\"127.0.0.1\",\"hostIp\":\"192.168.2.118\"}현재 인스턴스의 프라이빗 IPv4 주소가 hostIp에 찍혀있다면 성공입니다.Application Load Balancer 생성 / 적용하기프라이빗 인스턴스에 어플리케이션을 배포했지만 외부에서의 접근은 불가능한 상태입니다. 이제 로드밸런서를 생성하고 http 프로토콜로 접근 가능하도록 해보겠습니다.대상그룹 생성하기로드밸런서를 생성하기 전에 로드밸런서가 타겟으로 삼을 대상 그룹을 만들어야 합니다. aws 콘솔에서 대상그룹을 검색하여 로드밸런서 대시보드로 이동하겠습니다. 이동하였다면 우측 상단의 Create target group 을 클릭합니다.그 후 아래 설정처럼 설정 한 후 다음으로 넘어갑니다. Health check path 에 /health 라고 입력해둔것을 잘 기억해두세요.타겟에는 프라이빗 서브넷 내부 인스턴스를 선택하고 추가한 후, Create target group 을 클릭하여 타겟 그룹을 생성합니다.로드밸런서 생성하기aws 콘솔에서 로드밸런서를 검색하여 로드밸런서 대시보드로 이동하겠습니다. 이동하였다면 좌측 상단의 Load Balancer 생성 버튼을 클릭합니다.첫번째 Application Load Balancer의 생성 버튼을 클릭합니다. 리스너가 80포트에 연결되어있는지 확인하고 가용영역을 지정(2a, 2c)한 후 다음 단계로 넘어갑니다.두번째 보안 설정 구성 단계는 현재 https 프로토콜을 적용하지 않았기 때문에 넘어갑니다. (실제 배포될 프로덕션 어플리케이션에는 필수입니다.)세번째 보안 그룹 구성에는 80포트만 오픈한 보안그룹을 하나 만들고 적용합니다.네번째 라우팅 구성에서는 방금 만든 대상그룹을 선택하고 넘어갑니다.구성을 완료하였다면 로드밸런서를 생성합니다.실제 접근하기DNS 이름을 복사하고 브라우저 주소창에 붙여넣은후 접근해보도록 하겠습니다. 외부 도메인 등록 업체를 이용하는 경우 이 DNS를 해당 도메인에 연결하면 됩니다.관련해서는 업체마다 설정방법이 다르니 이용하고 있는 업체에 문의하시기 바랍니다.접근이 되지 않습니다. 로드밸런서를 생성하고 적용했지만 아직 몇가지 확인해야 할것이 남아있습니다. 대상그룹에 연결된 타겟들이 모두 healthy 상태인가? 로드밸런서가 프라이빗 서브넷에 접근할 수 있도록 프라이빗 서브넷 내부 인스턴스의 보안그룹을 수정하였는가? 웹서버와 was가 프록시 설정을 통해 연결되었는가?1번 항목부터 살펴보겠습니다. 대상 그룹은 만들때 설정한 Health check path 를 통해 해당 인스턴스가 유효한 상태인지 판단합니다. 우리는 위에서 /health 라고 설정하였기 때문에 인스턴스의 nginx 설정을 바꿔주도록 하겠습니다./etc/nginx/conf.d/test.confserver { listen 80; location /health { return 200; }}config 파일을 하나 만들고 /health 경로에 대해 200 상태값을 반환하도록 설정하였습니다. 저장후 nginx를 재시작해야 한다는 것을 잊지 마세요.다시 대상그룹 대시보드로 돌아와 방금 만든 대상그룹을 선택하고 등록한 타겟의 Health status가 healthy 인지 확인합니다. (가용존별 프라이빗 서브넷에 인스턴스를 생성하였다면 등록한 타겟은 N개여야 합니다.)다음은 보안그룹입니다. 프라이빗 서브넷 내부 인스턴스에 적용되어있는 보안그룹을 수정하여 로드밸런서가 80포트를 통해 인스턴스에 접근할 수 있도록 합니다.마지막으로 프록시 설정입니다. 아까 생성한 test.conf 파일을 수정하여 80포트로 들어오는 요청을 8080포트로 보내도록 하겠습니다.server { listen 80; location /health { return 200; } location / { proxy_pass http://127.0.0.1:8080; }}모든 설정이 완료되었다면 nginx 재시작 후 DNS로 접속해보도록 하겠습니다. 우리는 https 설정을 하지 않았기 때문에 http 프로토콜로 접속해야 함을 잊지 마세요.DNS 주소/ip 로 접근했을때 위와같이 accessIp에는 내 ip가, hostIp에는 프라이빗 인스턴스의 IPv4 주소가 찍혀 나온다면 성공입니다. 만약 가용존별 프라이빗 서브넷을 구성하였다면 새로고침을 눌렀을때 hostIP가 계속 바뀌는지 확인해보세요.마치며이번 포스팅까지 기본적인 서버 구축은 끝이 났습니다. 여기까지가 aws를 활용해 설계할 수 있는 가장 기본적인 형태의 아키텍처라고 생각되는데요, 다음 포스팅부터는 Jenkins와 Codedeploy를 활용하여 배포까지 자동화 해보도록 하겠습니다." }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 3. VPC와 기본 리소스 생성하기", "url": "/posts/aws-cicd-3/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2021-06-26 10:28:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리VPC와 기본 리소스 생성하기AWS 콘솔을 사용해 직접 VPC와 필요한 리소스들을 생성해 보겠습니다.VPC 생성하기첫번째로 VPC를 생성해보도록 하겠습니다.일단 AWS 콘솔에 접속한 후에, 검색창에 VPC를 검색해 VPC 대시보드로 들어갑니다.그 후 오른쪽 상단의 VPC 생성을 눌러 VPC 생성 화면으로 이동합니다.앞 포스팅에서 말씀드렸듯이 CIDR 블록에는 사설망 대역을 사용하는게 일반적인데요, 저는 3가지중 192.168.0.0/16 을 사용하겠습니다.IPv6 CIDR 블록도 사용할 수 있지만 여기서는 IPv4 CIDR 블록에만 초점을 맞추도록 하겠습니다.VPC 가 생성 되었습니다.여기까지의 구성도입니다.서브넷 생성하기AWS 콘솔 검색창에 서브넷을 검색하여 서브넷 대시보드로 들어갑니다. 그 후 오른쪽 상단의 서브넷 생성을 눌러 서브넷 생성 화면으로 이동합니다.먼저 방금전 만든 VPC를 선택해줍니다. 서브넷은 총 4개를 만들어야 합니다. 가용존 ap-northeast-2a, ap-northeast-2c 에 각각 서브넷 2개씩 만들어 줍니다.가용존 하나에 만들어도 되지만 장애 대응 측면에서 가용존을 나누어 만들도록 하겠습니다. (물론 더 많은 가용존에 만들어도 무방합니다.)우리가 만든 VPC의 IP 범위는 192.168.0.0 ~ 192.168.255.255 이기 때문에, 하나의 서브넷당 IP의 범위를 192.168.X.X 만큼 설정해 주도록 하겠습니다.서브넷 4개가 생성되었습니다.여기까지의 구성도입니다.인터넷 게이트웨이 생성하기앞서 만든 VPC는 인터넷과 연결된 통로가 없습니다. 그래서 이 상태에서 인스턴스를 만들어도 접속할 수 없는데요. 아마존에서는 인터넷 액세스를 활성화하려면 다음을 수행해야 한다고 설명하고 있습니다. 인터넷 게이트웨이를 생성하여 VPC에 연결합니다. 인터넷 바인딩된 트래픽을 인터넷 게이트웨이로 전달하는 라우팅을 서브넷의 라우팅 테이블에 추가합니다. 서브넷의 인스턴스에 전역적으로 고유한 IP 주소(퍼블릭 IPv4 주소, 탄력적 IP 주소 또는 IPv6 주소)가 있는지 확인합니다. 네트워크 액세스 제어 목록 및 보안 그룹 규칙에서 관련 트래픽이 인스턴스로, 그리고 인스턴스에서 흐르도록 허용되는지 확인합니다. AWS 콘솔 검색창에 인터넷 게이트웨이를 검색하여 대시보드로 들어갑니다. 그 후 오른쪽 상단의 인터넷 게이트웨이 생성을 눌러 생성 화면으로 이동하고 이름을 입력하여 인터넷 게이트웨이를 생성합니다.인터넷 게이트웨이는 생성후에 VPC와 연결해 주어야 합니다. 오른쪽 상단의 작업 탭을 선택후 VPC에 연결을 선택하여 VPC 연결 화면으로 이동합니다. 그후 처음에 만든 VPC를 선택하고 인터넷 게이트웨이 연결을 눌러 VPC와 인터넷 게이트웨이를 연결해줍니다.여기까지의 구성도입니다.라우팅 테이블 생성하기인터넷 게이트웨이를 생성하여 VPC와 연결했지만 아직 인스턴스에 연결할 수 없습니다. 아마존에서 설명한대로 서브넷의 라우팅 테이블이 방금 만든 인터넷 게이트웨이를 향하게 해야하는데요. 일단 라우팅 테이블을 검색하여 라우팅 테이블 대시보드로 진입합니다.그러면 기본으로 생성된 라우팅 테이블 한개를 볼 수 있습니다. 쉽게 구분하기 위해 일단 이름을 rtb-public으로 변경합니다. 그 후 하단의 라우팅을 눌러보면 VPC 내부연결을 위한 규칙만 존재하는것을 확인 할 수 있습니다. 라우팅 탭의 라우팅 편집 을 눌러 라우팅을 추가해봅시다.라우터(목적지)는 0.0.0.0/0 을 지정하고 방금 만든 인터넷 게이트웨이를 타겟과 연결합니다.라우팅 테이블의 라우팅(목적지)이 수정되었습니다.이제 라우팅 테이블과 서브넷을 연결해야 합니다. 서브넷 연결 탭을 눌러보면 명시적 연결이 없는 서브넷 이 보이는데요, 이중 subnet-public-1 과 subnet-public-2 서브넷을 연결하여 이름뿐이 아닌 진짜 퍼블릭 서브넷으로 만들어 보겠습니다.서브넷 연결 탭의 명시적 연결이 없는 서브넷 박스에서 서브넷 연결 편집 을 눌러 서브넷 연결을 편집 화면에 진입합니다.subnet-public-1 과 subnet-public-2 을 선택하고 하단의 연결 저장 버튼을 클릭합니다.이제 위에서 만든 4개의 서브넷중 2개의 서브넷이 퍼블릭 서브넷이 되었습니다. 실제로 연결되는지 한번 확인해봅시다.인스턴스를 생성하기 위해 인스턴스 대시보드로 들어가 오른쪽 상단의 인스턴스 시작을 눌러 인스턴스 마법사를 시작합니다. 인스턴스 마법사 설정을 간단하게 설명드리겠습니다. AMI 선택 - 여기서는 Amazon Linux 2 AMI를 선택하겠습니다. 인스턴스 유형 선택 - 자금의 압박이 있기 때문에 프리티어 사용 가능 유형인 t2.micro를 선택하겠습니다. 인스턴스 세부 정보 구성 - 이곳이 중요합니다. 네트워크는 우리가 만든 VPC를 선택해주고 서브넷은 subnet-public-1을 선택하도록 하겠습니다. 스토리지는 기본 8GB 스토리지를 사용하겠습니다 태그는 건너 뛰겠습니다. 보안그룹도 새로 생성되는 보안그룹을 선택하겠습니다. 기본은 0.0.0.0/0 에 대해 22번 포트가 열려있는 형태입니다. 인스턴스 시작을 클릭합니다. 그 후 새 키 페어 생성을 선택하고 키 페어를 다운받아 잘 저장해 둡니다. (주의! 다시 다운로드 할 수 없습니다.)생성한 인스턴스의 정보입니다. VPC 와 퍼블릭 서브넷이 올바르게 매핑되었는지 확인합니다.그 후 탄력적 IP를 한개 발급받아 인스턴스와 연결하고, SSH Client를 사용해 원격 접속되는지 확인합니다. 탄력적 IP는 대시보드 좌측의 네트워크 및 보안 - 탄력적 IP 탭에서 할당받고 연결시킬 수 있습니다.SSH Client의 사용법을 모르신다면 이곳 에서 확인해보세요.퍼블릭 서브넷으로의 전환까지 완료한 구성도입니다.앞 포스팅에서 설명드렸듯이 인터넷 게이트웨이로 향하는 라우팅이 없는 라우팅 테이블과 연결된 서브넷을 프라이빗 서브넷 이라 합니다.프라이빗 서브넷을 만들기 위해 rtb-private 라는 이름을 가진 라우팅 테이블을 만들고 subnet-private-1 과 subnet-private-2 서브넷을 연결해 두겠습니다.여기까지 라우팅 테이블을 만들고 퍼블릭 서브넷, 프라이빗 서브넷에 연결하여 구성 완료한 구성도입니다.Bastion Host 생성하기퍼블릭 서브넷에 프론트 &amp; 백엔드 어플리케이션을 올리지는 않습니다. (물론 가능은 합니다.) 보통 프라이빗 서브넷에 어플리케이션을 올리고 외부에서의 접근은 로드밸런서를 통해 80, 443 포트만 프라이빗 서브넷의 인스턴스에 접근할 수 있게 하지요.그렇다면 프라이빗 서브넷 내부 인스턴스에 문제가 생겨 내 PC 에서 직접 인스턴스에 접속하려면 어떻게 해야 할까요? 정답은 Bastion Host 입니다. 퍼블릭 서브넷에 안스턴스를 생성하고 생성한 인스턴스에서프라이빗 IPv4 주소를 통해 재접속하는 방식입니다. 일종의 프록시 서버이죠.시작하기 전에 프라이빗 서브넷에 인스턴스 한개를 만들어 두도록 하겠습니다. 아까 인스턴스 생성 과정의 3. 인스턴스 세부 정보 구성 에서 서브넷만 subnet-private-1 으로 바꾸고 다른 옵션은 유지한채 인스턴스를 시작합니다.퍼블릭 IPv4 주소가 없고 서브넷 ID가 subnet-private-1 인 인스턴스 한개를 생성하였습니다. 사설 IP 대역이기 때문에 당연히 현재 PC 에서 이 인스턴스로 접근할 수 없습니다.이제 퍼블릭 서브넷에 Bastion Host 용도의 인스턴스 한개를 만들어 보겠습니다. Bastion Host는 보통 Linux로 구성합니다.하지만 자신이 터미널 환경의 운영체제에 익숙하지 않거나 터미널 환경의 DB 조작에 익숙하지 않다면 Windows 환경으로 구성하는것도 무방합니다.Windows 환경으로 구축하였을 경우 원격 데스크톱으로 해당 인스턴스에 접근해야 하는데요, 관련해서는 많은 글과 예제가 있으니 검색해 보시기 바랍니다.퍼블릭 서브넷에 인스턴스 한개를 생성하고 탄력적 IP를 연결하였습니다. 이 인스턴스가 Bastion Host 용도의 인스턴스가 되는 것입니다.이제 SSH Client를 이용해 방금 만든 Bastion Host에 접속해 보겠습니다.ifconfig 명령어를 통해 현재 ip를 확인해 보니 192.168.0.247로 subnet-public-1에 있는 컴퓨터인것을 확인하였습니다.그 후 프라이빗 서브넷에 있는 인스턴스를 생성할때 만든 .pem 파일을 Bastion Host로 옮깁니다. 옮긴 후에 ssh -i key.pem ec2-user:192.168.X.X 명령어를 사용하여 인스턴스에 접근해 보겠습니다.음? 접속이 되지 않습니다. 어떻게 된 일일까요?정답은 보안그룹입니다. 프라이빗 서브넷의 인스턴스를 생성하실때 보안그룹을 잘 확인하지 않고 그냥 기본 생성되는 보안그룹을 적용하셨다면 문제없이 접근하셨을 수도 있습니다.그러나 22번 포트에 대해 인바운드 규칙을 없애버리셨다면 접근 할 수 없는것이 당연합니다.보안그룹 탭에 들어가 현재 프라이빗 서브넷에 적용되어 있는 보안그룹을 변경하겠습니다. 유형에 SSH를 선택하시고 소스에는 Bastion Host의 EIP CIDR 블록을 직접 입력하는 것이 아닌 Bastion Host에 적용되어 있는 보안그룹을 선택해주세요. (여기서는 편의상 public, private sg 라 지칭해 두었습니다.)보안그룹이 적용되었다면 다시한번 Bastion Host에 접속해서 프라이빗 서브넷의 인스턴스에 접속해 보겠습니다.프라이빗 서브넷 내부 인스턴스에 접속 성공하였고 ifconfig 명령어를 통해 subnet-private-1에 있는 컴퓨터인것을 확인하였습니다.혹시 아래와 같은 에러 메시지를 보게 된다면 전송한 key.pem 파일의 권한을 chmod 600 key.pem 명령어를 통해 바꿔주세요.Bastion Host 생성을 완료하였고 여기까지의 구성도는 다음과 같습니다. 귀찮게 뭐하러 한단계 거쳐서 접근해? 내 컴퓨터에서 바로 접근하는 방법은 없어?라고 생각하시는 분들이 계실텐데요 (저도 그랬습니다…)이 부분은 SSH Tunneling 혹은 SSH Reverse Tunneling 으로 해결할 수 있으니 궁금하신 분들은 검색해 보시기 바랍니다.NAT Gateway 생성하기Bastion Host를 통해 프라이빗 서브넷에 있는 인스턴스에 접근까지는 성공하였습니다. 하지만 프라이빗 서브넷은 현재 인터넷과 관련된 동작은 할 수 없습니다. 이유는 나가는 라우팅(목적지)이 없기 때문입니다.일례로 sudo yum update 명령어를 쳐도 아무것도 동작하지 않습니다. 오히려 Connection time out 이라는 타임아웃 에러메시지를 확인 할 수 있습니다. 이때 필요한 것이 NAT Gateway 입니다.AWS 콘솔에서 NAT 게이트웨이를 검색하여 대시보드로 이동한 후에 우측 상단의 NAT 게이트웨이 생성을 눌러 생성화면으로 이동해보겠습니다. 이름을 지정하고 서브넷은 subnet-public-1 을 지정해 2a 가용영역의 퍼블릭 서브넷에 NAT Gateway가 생성될 수 있도록 합니다.마지막으로 탄력적 IP를 하나 할당 받아 적용하고 NAT Gateway를 생성합니다.NAT Gateway를 생성하였지만 아직 프라이빗 서브넷의 인스턴스에서는 인터넷에 접속할 수 없습니다. 이유는 프라이빗 서브넷의 라우터(목적지)를 수정하지 않았기 때문인데요.이를 수정하기 위해 라우팅 테이블 대시보드로 이동하여 현재 프라이빗 서브넷에 적용되어 있는 라우팅 테이블을 선택후 라우팅 편집을 클릭하여 편집화면으로 이동하도록 하겠습니다.라우팅 편집화면으로 이동하였다면 0.0.0.0/0 에 대한 라우팅을 추가하고 대상을 방금 만든 NAT Gateway로 지정해주도록 하겠습니다.편집이 모두 완료되었다면 다시 프라이빗 서브넷의 인스턴스로 접속하여 sudo yum update 명령어를 쳐보겠습니다.인스턴스가 NAT Gateway를 통해 인터넷과 연결되어 패키지가 업그레이드 되는것을 확인할 수 있습니다.만약 외부 업체의 api에 접근해야 하는데 보안 정책상 기본 접근은 막아두고 접근 허용할 IP를 알려달라고 하면 방금 만든 NAT Gateway의 EIP를 알려주면 되겠죠?또 지금은 2a 가용영역에만 NAT Gateway를 만들었지만 보통 가용존의 수만큼 만든다고 하니 알맞은 수의 NAT Gateway를 만들어 보시기 바랍니다.NAT Gateway 까지 생성 완료한 현재까지의 구성도입니다." }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 2. VPC와 기본 리소스", "url": "/posts/aws-cicd-2/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2021-06-22 20:12:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리VPC와 기본 리소스서버 구축의 첫번째 단계로 AWS의 VPC (Virtual Private Cloud) 와 기본 리소스들의 개념을 알아보도록 하겠습니다.리전 &amp; 가용영역VPC에 대해 알아보기 전에, 리전 &amp; 가용영역 에 대해 알아보겠습니다.AWS에는 리전이라는 개념이 있습니다. AWS는 전 세계에 데이터 센터를 두고 있는데요, 이 데이터 센터를 클리스터링하는 물리적 위치를 리전이라고 합니다.각 리전은 지리 영역 내에서 격리되고 물리적으로 분리된 여러 개의 가용영역(Availability Zone) 으로 구성됩니다. 사용자는 리전을 선택하여 VPC 를 구성할 수 있습니다.가용영역(Availability Zone)은 AWS 리전의 중복 전력, 네트워킹 및 연결이 제공되는 하나 이상의 개별 데이터 센터로 구성됩니다. N개의 가용영을 사용하면1개의 가용영역을 사용하는 것보다 높은 가용성, 내결함성 및 확장성을 갖춘 어플리케이션과 데이터베이스를 운영할 수 있습니다. 쉽게 말해 물리적으로 영역이 분리되어 있기 때문에 한 영역에 장애가 발생해도다른 영역에 트래픽이 몰리게 되어 컴퓨터의 자원 사용률은 높아지겠지만 시스템이 먹통이 되는 현상은 피할 수 있게 됩니다. 물론 컴퓨터 자원에 따라 물리적인 컴퓨터 수를 조절하는오토 스케일링 그룹을 사용하면 자원 사용률은 상관 없는 이야기이긴 합니다.VPCVPC 는 Virtual Private Cloud의 약자로, 아마존은 공식적으로 다음과 같이 설명하고 있습니다. Amazon Virtual Private Cloud(VPC)를 사용하면 AWS 클라우드에서 논리적으로 격리된 공간을 프로비저닝하여 고객이 정의하는 가상 네트워크에서 AWS 리소스를 시작할 수 있습니다.IP 주소 범위 선택, 서브넷 생성, 라우팅 테이블 및 네트워크 게이트웨이 구성 등 가상 네트워킹 환경을 완벽하게 제어할 수 있습니다.VPC에서 IPv4와 IPv6를 모두 사용하여 리소스와 애플리케이션에 안전하고 쉽게 액세스할 수 있습니다.즉, “AWS는 논리적으로 격리된 공간을 프로비저닝 하여 독립된 하나의 가상 블록을 제공하며, VPC는 AWS가 제공한 가상 블록을 기반으로 한 사용자의 계정 전용 가상 네트워크” 라고 생각하면 쉬울것 같습니다.이 가상 블록은 내부 리소스들을 모두 public 으로 공개할 수 있지만, 대부분의 사용자들이 내부 리소스들을 모두 private 으로 두고 그 private 리소스들에 접근할 수 있는 하나의 Public EC2 인스턴스(Bastion Host) 를 두고 사용하는 편입니다. (Bastion Host 에 대해서 는 추후에 다시 다루도록 하겠습니다.)VPC 는 2019년부터 강제되었기 때문에 AWS에서 제공하는 대부분의 서비스는 VPC 없이는 사용할수가 없게 되었습니다.하나의 VPC를 만들면 기본적으로 생성되는 리소스들이 있습니다. 개인적으로 AWS를 이용해 서버 구축을 하려면 필수로 알아야 한다고 생각되는 개념들인데요. 하나하나 짚어보도록 하겠습니다.CIDRAWS 콘솔을 이용해 VPC를 생성하려고 하면 CIDR 블록 을 지정해야 한다는 문구를 만날 수 있습니다.결론부터 말하자면 CIDR은 “IP주소 할당 기법” 입니다.CIDR 블록은 IP 주소와 슬래쉬가 뒤에 따라오는 넷마스크 형태의 숫자로 구성되어 있습니다. 슬래쉬 뒤에 오는 숫자는 IP의 범위를 나타냅니다.예를 들어 IP 172.31.0.0/32 는 정확히 172.31.0.0 를 가리킵니다.IP의 범위는 지정된 IP부터 2^(32-n)개 입니다. 만약 숫자가 16이면 2^(32-16) = 65,536 개의 IP 주소를 의미합니다.VPC 내부의 자원들은 각각의 프라이빗 ip를 가지는데요, 이때 CIDR 블록의 범위 안에서 IP를 할당받게 됩니다.VPC 범위 내에서 할당 가능한 IP가 모두 할당되면 더이상 리소스를 만들수 없습니다. 한 VPC 의 최대 크기는 16입니다. 즉, 65,356개의 IP를 사용할 수 있으며, 이보다 큰 VPC는 생성할 수 없습니다. (물론 그럴일은 잘 없겠지요.)CIDR 블록을 만들때 주의해야할 사항이 있습니다. 만약 사설망 대역이 아닌 인터넷과 연결되어 있는 경우 문제가 될 수 있습니다. 예를 들어 3.51.0.0/16를 블록으로 지정한 경우 이 VPC 에서 3.51.0.0/16로 접속하는 트래픽은 VPC 내부로 라우팅됩니다.그런데 3.51.0.0/16 범위는 외부 인터넷에서 사용할 수 있는 범위의 IP 입니다. 따라서 이 VPC 내부에서는 3.51.0.0/16 에 속한 인터넷 IP에 접근하는 것이 불가능 합니다. 외부로의 연결이 필요한 경우라면 무조건 사설망 대역을 사용해야 하며, 그렇지 않더라도사설망 대역을 사용하는 것이 권장되는 편입니다. 사설 IP 대역에는 아래 3가지 대역이 있습니다. Class A : 10.0.0.0 ~ 10.255.255.255 Class B : 172.16.0.0 ~ 172.31.255.255 Class C : 192.168.0.0 ~ 192.168.255.255서브넷VPC를 리전에 생성한것 만으로는 아무것도 할 수 없습니다. VPC는 내부적으로 다시 CIDR 블록으로 나뉘어지고, 서브넷은 실제 VPC 내부에서 리소스가 생성되는 물리적 공간인 가용영역(Availability Zone) 과 연결됩니다.VPC가 논리적인 범위라면, 서브넷은 VPC 내부에서 실제로 리소스가 생성될 수 있는 가상 네트워크라고 생각하면 될 것 같습니다.AWS 콘솔에서 어떤 리소스(ex. EC2, RDS)를 생성할 때, VPC만 지정하는 경우는 없습니다. 서브넷도 함께 지정하는데요, 보통 서브넷을 지정하면 VPC는 자동으로 지정되는 형태를 띕니다.하나의 VPC는 N개의 서브넷을 가질 수 있습니다. 서브넷의 최대 크기는 VPC의 크기와 같습니다. (넷마스크 16 ~ 28의 범위) 보통 한 리전 내의 사용 가능한가용영역을 고려하여 적절한 갯수의 서브넷을 가용영역 내부에 생성합니다. 이렇게 각각 가용영역 내부에 동일한 형태의 별도의 서브넷을 가지면 한개의 가용영역에 장애가 생겨도 트래픽 분산 측면에서 쉽게 장애에 대응할 수 있겠지요.장애 대응 측면에서 서브넷을 가용영역만큼 나누는 경우 사용할 리전에서 사용가능한 가용영역의 갯수를 미리 파악 할 필요가 있습니다. (서울 리전의 경우 4개)모든 가용영역 사용할 필요는 없지만 보통 2개 이상의 가용영역을 구성하는게 일반적입니다.퍼블릭 서브넷 &amp; 프라이빗 서브넷서브넷도 어떻게 설정하느냐에 따라 두가지로 나뉠 수 있습니다. 먼저 퍼블릭 서브넷은 VPC 내부에 있더라도 외부에서 퍼블릭 IP (탄력적 IP) 를 통해 들어올 수 있는 서브넷을 의미합니다.보통 퍼블릿 서브넷에 Bastion Host를 두어 같은 VPC 내부에 있는 프라이빗 서브넷, RDS 등에 접근 하곤 합니다. Bastion Host에 대해선 추후 서버 구축시 자세히 다루도록 하겠습니다.프라이빗 서브넷은 외부에서 직접 들어올수도, 나갈 수도 없는 VPC 내부에 단절된 네트워크를 사용하는 서브넷입니다.프라이빗 서브넷에 들어오려면 퍼블릭 자원(Load Balancer 등) 을 통해 들어와야 하며 프라이빗 서브넷에서 외부 인터넷에 연결하려면 “퍼블릭 서브넷에 있으며 탄력적 IP에 연결된 NAT Gateway”를 통해 나가야 합니다.NAT GatewayNAT Gateway는 프라이빗 서브넷이 인터넷과 통신할때 필요로 하는 아웃바운드 리소스 입니다. 프라이빗 서브넷에아웃바운드 트래픽이 필요한 경우가 있습니다. (ex. 외부 연동 API 서버 호출, OS 패키지 업데이트 등)이때, 퍼블릭 서브넷에 존재하는 NAT Gateway는 프라이빗 서브넷의 아웃바운드 요청을 받아 인터넷 게이트웨이로 연결하는 역할을 합니다.인터넷 게이트웨이VPC는 독립된 네트워크 환경입니다. 따라서 아무 설정을 하지 않는다면 내부 서브넷 설정이 어떻든 간에 인터넷을 사용할 수 없습니다.이때 인터넷에 연결하기 위해 필요한 것이 인터넷 게이트웨이 입니다. 외부 인터넷과 VPC를 연결해 주는 관문이 되는 것이지요.라우터 &amp; 라우팅 테이블한 서브넷 안에서 네트워크 요청이 발생하면 미리 정의된 라우팅 테이블에 따라 라우터로 향하게 됩니다. 라우터는 최종 목적지이고, 라우팅 테이블은 목적지(라우터)에 도달하기 위한 네트워크 노선 이라고 이해하면 쉽습니다.위에서 장황하게 설명하긴 했지만 A서브넷에 연결된 라우팅 테이블의 0.0.0.0/0 즉, 기본 경로가 인터넷 게이트웨이와 직접 연결되어 있으면 퍼블릭 서브넷이고인터넷 게이트웨이와 직접 연결되어 있지 않거나 NAT Gateway와 연결되어 있으면 프라이빗 서브넷이 되는 것입니다.보안그룹 (Security Group)위의 인터넷 게이트웨이 &amp; 라우터 &amp; 라우팅 테이블이 VPC 와 서브넷간의 트래픽을 제어한다면, 보안그룹은 EC2 인스턴스의 인바운드, 아웃바운드를 포트 번호별로 제어하기 위한 설정입니다.퍼블릭 서브넷에 존재하더라도 관련된 보안그룹의 포트가 오픈되어 있지 않다면 직접 인스턴스로 접근할 수 없습니다. 개인적으로 최종 방화벽(?) 같은 느낌이라고 생각합니다.마치며…VPC와 그를 사용하고자 할 때 필요한 여러 리소스들에 대해 알아보았습니다. 사실 저도 글로만 쓰면서 헷갈리는 것들이 있었는데요, 다음 포스팅에는 하나하나 직접 만들면서 자세히 알아보도록 하겠습니다." }, { "title": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 - 1. 개요", "url": "/posts/aws-cicd-1/", "categories": "AWS", "tags": "AWS, DevOps", "date": "2021-06-21 19:32:00 +0900", "snippet": "AWS를 사용해 무중단 배포 자동화 환경 구축하기 시리즈 개요 VPC와 기본 리소스 VPC와 기본 리소스 생성하기 어플리케이션 구축 및 로드밸런서 적용 AWS 리소스 세팅 CodeDeploy 연동 / 마무리개요AWS를 이용하여 가장 기본적인 형태의 인프라를 구축하고 GithubAction - CodeDeploy 를 이용하여 CI/CD 서버를 구축해보도록 하겠습니다. AWS의 기본적인 개념은 숙지하고 있다는 가정하에 진행됩니다. 블루 - 그린 배포 방식을 사용합니다.서버 구성최종적으로 구축할 웹서버 아키텍처는 다음과 같습니다. (직접 그린거라 삐뚤삐뚤합니다 ㅠㅠ…) 중간 Private Subnet 영역의 Auto Scailing group 에 포함되지 않은 인스턴스는 많은 트래픽을 필요로 하지 않는 어플리케이션 입니다. (ex. 단순 조회용 사이트) Auto Scailing group 에 포함된 인스턴스는 많은 트래픽이 발생합니다. (ex. 대고객 사이트, 외부 연동 api 서버) Auto Scailing group 내부 인스턴스의 내부 구조는 웹서버로 Nginx 를 두고 location 설정을 바탕으로 Spring-Boot Applicaton 에 접근합니다. (ex. /api - api 서버로)배포 구성자동화 배포에 사용할 배포 구조는 다음과 같습니다. Github 에 코드를 commit &amp; push 합니다. 미리 정의해둔 GithubAction Workflow에 의해 job이 실행됩니다. GithubAction은 어플리케이션을 빌드하고 S3 Bucket에 전송합니다. GithubAction은 CodeDeploy 에게 배포 명령을 내립니다. CodeDeploy는 블루-그린 배포방식을 바탕으로 새로운 인스턴스 그룹을 생성합니다. 미리 정해둔 템플릿을 바탕으로 시작된 인스턴스는 S3 Bucket 에 저장된 빌드 파일(번들)을 다운받습니다. 빌드파일에 포함된 AppSpec.yml 파일을 바탕으로 배포를 진행합니다. 모든 배포가 성공적으로 끝나면 CodeDeploy는 Application Load Balancer의 트래픽을 새로운 타겟 그룹으로 변경하고, 기존의 그룹을 종료합니다." } ]
